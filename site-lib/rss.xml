<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Anteckningar]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>site-lib/media/favicon.png</url><title>Anteckningar</title><link/></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Mon, 29 Dec 2025 16:57:26 GMT</lastBuildDate><atom:link href="site-lib/rss.xml" rel="self" type="application/rss+xml"/><pubDate>Mon, 29 Dec 2025 16:57:16 GMT</pubDate><ttl>60</ttl><dc:creator/><item><title><![CDATA[Optimization]]></title><description><![CDATA[
<a data-href="The optimization problem" href="matematik/optimization/the-optimization-problem.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">The optimization problem</a><a data-href="The optimization problem" href="matematik/optimization/the-optimization-problem.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">The optimization problem</a>
<br><a data-href="Equivalences of optimization problems" href="matematik/optimization/equivalences-of-optimization-problems.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Equivalences of optimization problems</a><a data-href="Equivalences of optimization problems" href="matematik/optimization/equivalences-of-optimization-problems.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Equivalences of optimization problems</a>
<br><a data-href="Minimizers" href="matematik/optimization/minimizers.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Minimizers</a><a data-href="Minimizers" href="matematik/optimization/minimizers.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Minimizers</a>
<br><a data-href="Feasible directions and descent directions" href="matematik/optimization/feasible-directions-and-descent-directions.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Feasible directions and descent directions</a><a data-href="Feasible directions and descent directions" href="matematik/optimization/feasible-directions-and-descent-directions.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Feasible directions and descent directions</a> <br><a data-href="The linear optimization problem" href="matematik/optimization/linear-optimization/the-linear-optimization-problem.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">The linear optimization problem</a><a data-href="The linear optimization problem" href="matematik/optimization/linear-optimization/the-linear-optimization-problem.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">The linear optimization problem</a>
<br><a data-href="Standard form" href="matematik/optimization/linear-optimization/standard-form.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Standard form</a><a data-href="Standard form" href="matematik/optimization/linear-optimization/standard-form.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Standard form</a>
<br><a data-href="Basic feasible solutions" href="matematik/optimization/linear-optimization/basic-feasible-solutions.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Basic feasible solutions</a><a data-href="Basic feasible solutions" href="matematik/optimization/linear-optimization/basic-feasible-solutions.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Basic feasible solutions</a>
<br><a data-href="The Simplex method" href="matematik/optimization/linear-optimization/the-simplex-method.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">The Simplex method</a><a data-href="The Simplex method" href="matematik/optimization/linear-optimization/the-simplex-method.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">The Simplex method</a>
<br><a data-href="Duality" href="matematik/optimization/linear-optimization/duality.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Duality</a><a data-href="Duality" href="matematik/optimization/linear-optimization/duality.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Duality</a>
<br><a data-href="Duality for other forms" href="matematik/optimization/linear-optimization/duality-for-other-forms.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Duality for other forms</a><a data-href="Duality for other forms" href="matematik/optimization/linear-optimization/duality-for-other-forms.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Duality for other forms</a> <br><a data-href="Convexity" href="matematik/optimization/convex-optimization/convexity.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Convexity</a><a data-href="Convexity" href="matematik/optimization/convex-optimization/convexity.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Convexity</a>
<br><a data-href="The convex optimization problem" href="matematik/optimization/convex-optimization/the-convex-optimization-problem.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">The convex optimization problem</a><a data-href="The convex optimization problem" href="matematik/optimization/convex-optimization/the-convex-optimization-problem.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">The convex optimization problem</a> <br><a data-href="The quadratic optimization problem" href="matematik/optimization/quadratic-optimization/the-quadratic-optimization-problem.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">The quadratic optimization problem</a><a data-href="The quadratic optimization problem" href="matematik/optimization/quadratic-optimization/the-quadratic-optimization-problem.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">The quadratic optimization problem</a>
<br><a data-href="Convex quadratic functions" href="matematik/optimization/quadratic-optimization/convex-quadratic-functions.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Convex quadratic functions</a><a data-href="Convex quadratic functions" href="matematik/optimization/quadratic-optimization/convex-quadratic-functions.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Convex quadratic functions</a>
]]></description><link>matematik/optimization.html</link><guid isPermaLink="false">Matematik/Optimization.md</guid><pubDate>Mon, 29 Dec 2025 16:55:05 GMT</pubDate></item><item><title><![CDATA[Convex quadratic functions]]></title><description><![CDATA[Theorem
Let be a quadratic function given by is convex if and only if is positive semidefinite. is strictly convex if and only if is positive definite. Proof: Since is the Hessian of , this follows immediately from a <a data-tooltip-position="top" aria-label="Convexity > ^07607e" data-href="Convexity#^07607e" href="matematik/optimization/convex-optimization/convexity.html#^07607e" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">previous theorem</a><a data-tooltip-position="top" aria-label="Convexity > ^07607e" data-href="Convexity#^07607e" href="matematik/optimization/convex-optimization/convexity.html#^07607e" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">previous theorem</a>.
From the same theorem it follows that if is positive definite then is strictly convex. Now suppose is strictly convex. From 1. we know is positive semidefinite. Suppose it is not positive definite, i.e. there exists some with . Then meaning is linear in the -direction, contradicting strict convexity. ]]></description><link>matematik/optimization/quadratic-optimization/convex-quadratic-functions.html</link><guid isPermaLink="false">Matematik/Optimization/Quadratic optimization/Convex quadratic functions.md</guid><pubDate>Mon, 29 Dec 2025 16:52:02 GMT</pubDate></item><item><title><![CDATA[Convexity]]></title><description><![CDATA[Definition: Convex set
Let . Then is convex if the all line segments between any two points in are contained in , that is, if for every and we have Definition: Convex function
Let be convex and . Then is convex if for every and we have Furthermore, is strictly convex if it also holds that whenever and , the inequality above is strict.
Theorem
Let be convex and let be continuously differentiable. Then is convex if and only if for all .
Theorem
Let be convex with a nonempty interior and let be twice continuously differentiable. is convex if and only if is positive semidefinite for all , where is the <a data-tooltip-position="top" aria-label="Minimizers" data-href="Minimizers" href="matematik/optimization/minimizers.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Hessian</a><a data-tooltip-position="top" aria-label="Minimizers" data-href="Minimizers" href="matematik/optimization/minimizers.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Hessian</a> of at .
If is positive definite for all , then is strictly convex. ]]></description><link>matematik/optimization/convex-optimization/convexity.html</link><guid isPermaLink="false">Matematik/Optimization/Convex optimization/Convexity.md</guid><pubDate>Mon, 29 Dec 2025 16:33:31 GMT</pubDate></item><item><title><![CDATA[The quadratic optimization problem]]></title><description><![CDATA[Definition: Quadratic function
A function is called quadratic whenever it is of the form for some symmetric matrix , some and some .
Quadratic optimization deals with optimizing a quadratic function but with linear constraints. The constant in a quadratic function is of course redundant in optimization problems, so we assume it is zero.Definition: Quadratic optimization problem
An <a data-tooltip-position="top" aria-label="The optimization problem" data-href="The optimization problem" href="matematik/optimization/the-optimization-problem.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">optimization problem</a><a data-tooltip-position="top" aria-label="The optimization problem" data-href="The optimization problem" href="matematik/optimization/the-optimization-problem.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">optimization problem</a> is called quadratic whenever it is of the form for some symmetric matrix and vector , matrices , , some vectors , and some vector .
Theorem
Let be a quadratic function given by Then, for every , and , Proof:
We have Corollary
For any we have Remark is the Hessian of .
]]></description><link>matematik/optimization/quadratic-optimization/the-quadratic-optimization-problem.html</link><guid isPermaLink="false">Matematik/Optimization/Quadratic optimization/The quadratic optimization problem.md</guid><pubDate>Mon, 29 Dec 2025 16:24:51 GMT</pubDate></item><item><title><![CDATA[Minimizers]]></title><description><![CDATA[Remark
Here, we consider a general optimization problem where and is any function.
Definition: Global minimizer, local minimizer
A point is a global minimizer of if for all .
A point is a local minimizer of if there exists a such that for all where .
It follows immediately that all local minimizers are global; we can choose any .Definition: Hessian
Suppose is twice continuously differentiable. The Hessian of at a point is Theorem
Suppose is differentiable. If is an interior point of and local minimizer of , then If furthermore is twice continuously differentiable, then the Hessian is positive semidefinite.
Proof:
Let with . Define by for some such that is a global minimum in its neighbourhood of radius . Then is a global minimum of . We get since by minimality, . But also, so for the derivative to exist we must have .
Now suppose . Since is twice continuously differentiable, is continuous and there thus exists a such that on . By Taylor expansion, for every there exists a with and since we get , contradicting the minimality of . Thus .
Since all directional derivatives are zero we have . We also know that where is the Hessian, meaning so is indeed positive semidefinite.
]]></description><link>matematik/optimization/minimizers.html</link><guid isPermaLink="false">Matematik/Optimization/Minimizers.md</guid><pubDate>Mon, 29 Dec 2025 15:51:57 GMT</pubDate></item><item><title><![CDATA[The convex optimization problem]]></title><description><![CDATA[Definition: Convex optimization problem
An <a data-tooltip-position="top" aria-label="The optimization problem" data-href="The optimization problem" href="matematik/optimization/the-optimization-problem.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">optimization problem</a><a data-tooltip-position="top" aria-label="The optimization problem" data-href="The optimization problem" href="matematik/optimization/the-optimization-problem.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">optimization problem</a> is called convex whenever it is of the form where is convex and is convex.
Theorem
Let be the set of all optimal solutions to . Then is convex.
Proof:
Let be optimal solutions with optimal value . Then but from optimality we also know , so we have equality.
Corollary is either empty, contains one point, or infinitely many points.
Proof:
Whenever contains two distinct points, the line segment between them is contained and consists of infinitely many points.
Theorem
Suppose in is strictly convex. Then there is at most one optimal solution.
Proof:
Suppose are distinct optimal solutions with optimal value . Then, for every , which contradicts strict convexity.
<br>Theorem
Suppose is a <a data-tooltip-position="top" aria-label="Minimizers" data-href="Minimizers" href="matematik/optimization/minimizers.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">local minimizer</a><a data-tooltip-position="top" aria-label="Minimizers" data-href="Minimizers" href="matematik/optimization/minimizers.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">local minimizer</a> of in . Then is a global minimizer.
Proof:
Suppose there exists a with . Let be a neighbourhood of in which is a global minimizer. Then there exists a such that But we have since , contradicting the local minimality of .
Theorem
Suppose in is continuously differentiable. Then an interior point is a global minimizer if and only if .
Proof:<br>
Suppose there is some with . Then for all . Taking the limit as we get contradicting that . Thus is a global minimizer. If is a global minimizer it is a local minimizer, from which it follows from a <a data-tooltip-position="top" aria-label="Minimizers > ^7cc7be" data-href="Minimizers#^7cc7be" href="matematik/optimization/minimizers.html#^7cc7be" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">previous theorem</a><a data-tooltip-position="top" aria-label="Minimizers > ^7cc7be" data-href="Minimizers#^7cc7be" href="matematik/optimization/minimizers.html#^7cc7be" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">previous theorem</a> that .
<br>Theorem
A point is optimal to if and only if there does not exist a <a data-tooltip-position="top" aria-label="Feasible directions and descent directions" data-href="Feasible directions and descent directions" href="matematik/optimization/feasible-directions-and-descent-directions.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">feasible descent direction</a><a data-tooltip-position="top" aria-label="Feasible directions and descent directions" data-href="Feasible directions and descent directions" href="matematik/optimization/feasible-directions-and-descent-directions.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">feasible descent direction</a> for at .
Proof:
If there exists a feasible descent direction it is clear that is not optimal. Now suppose is not optimal, and let such that . Then for every , so choosing we get as a feasible descent direction.
]]></description><link>matematik/optimization/convex-optimization/the-convex-optimization-problem.html</link><guid isPermaLink="false">Matematik/Optimization/Convex optimization/The convex optimization problem.md</guid><pubDate>Mon, 29 Dec 2025 15:50:28 GMT</pubDate></item><item><title><![CDATA[Feasible directions and descent directions]]></title><description><![CDATA[Remark
Here, we consider a general optimization problem where and is any function.
Definition: Feasible direction, descent direction A is a feasible direction at if there exists an such that for every . is a descent direction at if there exists an such that for every .
If is both a feasible direction and a descent direction it is called a feasible descent direction for at .
]]></description><link>matematik/optimization/feasible-directions-and-descent-directions.html</link><guid isPermaLink="false">Matematik/Optimization/Feasible directions and descent directions.md</guid><pubDate>Mon, 29 Dec 2025 15:38:44 GMT</pubDate></item><item><title><![CDATA[Anteckningar]]></title><description><![CDATA[
<a data-href="Linjär algebra" href="matematik/linjär-algebra.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Linjär algebra</a><a data-href="Linjär algebra" href="matematik/linjär-algebra.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Linjär algebra</a>
<br><a data-href="Discrete mathematics" href="matematik/discrete-mathematics.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Discrete mathematics</a><a data-href="Discrete mathematics" href="matematik/discrete-mathematics.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Discrete mathematics</a>
<br><a data-href="Groups and rings, Lectures" href="matematik/groups-and-rings,-lectures.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Groups and rings, Lectures</a><a data-href="Groups and rings, Lectures" href="matematik/groups-and-rings,-lectures.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Groups and rings, Lectures</a>
<br><a data-href="Optimization" href="matematik/optimization.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Optimization</a><a data-href="Optimization" href="matematik/optimization.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Optimization</a>
]]></description><link>anteckningar.html</link><guid isPermaLink="false">Anteckningar.md</guid><pubDate>Mon, 29 Dec 2025 15:32:28 GMT</pubDate></item><item><title><![CDATA[Duality]]></title><description><![CDATA[Remark
Here, we consider a <a data-tooltip-position="top" aria-label="The linear optimization problem" data-href="The linear optimization problem" href="matematik/optimization/linear-optimization/the-linear-optimization-problem.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">linear optimization problem</a><a data-tooltip-position="top" aria-label="The linear optimization problem" data-href="The linear optimization problem" href="matematik/optimization/linear-optimization/the-linear-optimization-problem.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">linear optimization problem</a> in standard form, where and we assume .
Given a linear optimization problem , one can instead consider its dual problem. The idea is that minimizing the cost is the same thing as maximizing a lower bound for the cost. We notice that given a we have This means that whenever we get as a lower bound for the cost . All such 's give us a valid lower bound, and we will discover that the largest such lower bound is actually the optimal value, i.e., it gives a perfect lower bound.Definition: Dual problem
The dual problem to is Theorem: Weak duality
If and are feasible solutions to and , respectively, thenProof:
Using and we get Corollary
If and are feasible solutions to and , respectively, and if , then and are optimal.
<br>Using the derivations in the <a data-tooltip-position="top" aria-label="The Simplex method" data-href="The Simplex method" href="matematik/optimization/linear-optimization/the-simplex-method.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Simplex method</a><a data-tooltip-position="top" aria-label="The Simplex method" data-href="The Simplex method" href="matematik/optimization/linear-optimization/the-simplex-method.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Simplex method</a> we can also prove strong duality. Theorem: Strong duality
If and are optimal solutions to and , respectively, then Proof:
By weak duality it is sufficient to show that given an optimal solution to there exists a feasible solution to with .
Given the existence of an optimal solution, we can assume is a basic optimal solution given by the Simplex method. Recall then that the reduced costs are nonnegative, , where , meaning This means is a feasible solution to . We also have that ]]></description><link>matematik/optimization/linear-optimization/duality.html</link><guid isPermaLink="false">Matematik/Optimization/Linear optimization/Duality.md</guid><pubDate>Mon, 29 Dec 2025 10:50:18 GMT</pubDate></item><item><title><![CDATA[Duality for other forms]]></title><description><![CDATA[In <a data-href="Duality" href="matematik/optimization/linear-optimization/duality.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Duality</a><a data-href="Duality" href="matematik/optimization/linear-optimization/duality.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Duality</a> we only define dual problems for problems of standard form where is of full row rank. Since this captures every linear optimization problem up to equivalence, we can deduce the dual problems of other forms.First, we remove the constraint of being of full row rank. It turns out that the dual is identical to the case when is of full row rank.<br>Theorem
Consider a <a data-tooltip-position="top" aria-label="The linear optimization problem" data-href="The linear optimization problem" href="matematik/optimization/linear-optimization/the-linear-optimization-problem.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">linear optimization problem</a><a data-tooltip-position="top" aria-label="The linear optimization problem" data-href="The linear optimization problem" href="matematik/optimization/linear-optimization/the-linear-optimization-problem.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">linear optimization problem</a> in standard form, where is not necessarily of full row rank. Assuming there are solutions to , the dual problem of is for which weak and strong duality still hold.
Proof:
Let be distinct indices such that the corresponding rows of form a basis for the row space of , and let be the remaining indices. Let denote with rows selected. Since the rows of are linear combinations of rows of we have for some matrix , and since has solutions we must also have .
Then is equivalent to whose dual by definition is For we have and so by introducing we see that is free in , so reduces to which is exactly the dual we derived since . Since the cost function has not changed, weak and strong duality must still hold.
Now consider instead a problem of canonical form, i.e. Our initial idea of summing equalities now transfers nicely to summing inequalities: with the added constraint that to preserve the inequality direction. It then still holds that whenever i.e. , we get as a lower bound for the cost.Theorem
Consider a linear optimization problem in canonical form, The dual problem of is for which weak and strong duality still hold.
Proof:
By adding slack variables for each constraint in we get a problem in standard form: where and . By the previous theorem its dual is Notice that the last constraints are simply . This is thus equivalent to which is precisely . By the previous theorem and since we haven't changed the cost function, weak and strong duality still hold.
Theorem: The complimentary theorem
Consider a linear optimization problem in canonical form, and its dual . For every and we let Then and are feasible for and , respectively, if and only if Furthermore, and are optimal if and only if it also holds that or equivalently, for all suitable .
Proof:
The first equivalence is clear: and . For the second equivalence, notice that By strong duality we know are optimal if and only if i.e. Since this is equivalent to . The final equivalence also follows since .
Remark
In other words, are optimal iff for every , either or , and either or .
]]></description><link>matematik/optimization/linear-optimization/duality-for-other-forms.html</link><guid isPermaLink="false">Matematik/Optimization/Linear optimization/Duality for other forms.md</guid><pubDate>Mon, 29 Dec 2025 10:43:22 GMT</pubDate></item><item><title><![CDATA[Standard form]]></title><description><![CDATA[Definition: Standard form
An <a data-tooltip-position="top" aria-label="The optimization problem" data-href="The optimization problem" href="matematik/optimization/the-optimization-problem.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">optimization problem</a><a data-tooltip-position="top" aria-label="The optimization problem" data-href="The optimization problem" href="matematik/optimization/the-optimization-problem.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">optimization problem</a> is in standard form if it is of the form for some matrix , vector and vector .
<br>An immediate observation is that problems of standard form are <a data-tooltip-position="top" aria-label="The linear optimization problem" data-href="The linear optimization problem" href="matematik/optimization/linear-optimization/the-linear-optimization-problem.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">linear</a><a data-tooltip-position="top" aria-label="The linear optimization problem" data-href="The linear optimization problem" href="matematik/optimization/linear-optimization/the-linear-optimization-problem.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">linear</a>; we simply choose the inequality matrix as , the inequality vector as , the equality matrix as and the equality vector as . However, the implication goes both ways.The idea is that free variables can be represented as differences between nonnegative variables, and that inequality constraints can be replaced by equality constraints where we add a so-called slack variable, "gapping" the distance in the inequality.<br>Theorem: Standard form
Any <a data-tooltip-position="top" aria-label="The linear optimization problem" data-href="The linear optimization problem" href="matematik/optimization/linear-optimization/the-linear-optimization-problem.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">linear optimization problem</a><a data-tooltip-position="top" aria-label="The linear optimization problem" data-href="The linear optimization problem" href="matematik/optimization/linear-optimization/the-linear-optimization-problem.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">linear optimization problem</a> is equivalent to a problem in standard form.
Proof:
Consider a linear optimization problem For each variable , add a corresponding variable . Whenever is not a constraint, replace with two variables and replace all occurences of with . This is equivalent to having a free variable since the difference is free.
For each inequality constraint, add a so called slack variable and rewrite the inequality constraint as . This is equivalent to the inequality constraint since .
Remark
Note that we do not restrict ourselves by further assuming that . Since we only consider equality constraints, whenever some constraint can be written as a linear combination of other constraints, the constraint is always implied (or causes a contradiction).
Remark
It follows directly that every linear optimization problem is equivalent to a problem of the form This form is sometimes called the canonical form.
]]></description><link>matematik/optimization/linear-optimization/standard-form.html</link><guid isPermaLink="false">Matematik/Optimization/Linear optimization/Standard form.md</guid><pubDate>Sun, 28 Dec 2025 20:57:18 GMT</pubDate></item><item><title><![CDATA[The Simplex method]]></title><description><![CDATA[Remark
Here, we consider a linear optimization problem in standard form, where and we assume .
The <a data-tooltip-position="top" aria-label="Basic feasible solutions" data-href="Basic feasible solutions" href="matematik/optimization/linear-optimization/basic-feasible-solutions.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">fundamental theorem of linear programming</a><a data-tooltip-position="top" aria-label="Basic feasible solutions" data-href="Basic feasible solutions" href="matematik/optimization/linear-optimization/basic-feasible-solutions.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">fundamental theorem of linear programming</a> gives us an algorithm for quickly finding an optimal solution. One way is to check all possible choices of basic indices, but this quickly becomes slow. The idea with the Simplex method is to repeatedly walk from a basic solution to a neighbouring one with lower cost, until this cannot be done.We assume that we have a basic feasible solution and we wish to move to one with lower cost. "Moving" away from the basic feasible solution means increasing a non-basic variable from zero. Let be the current solution and be the current cost. We must at all times have Definition: Given a basic index tuple we define We then have For the cost, we have
Definition: Reduced costs, simplex multipliers
Given a basic index tuple , the reduced costs are and the simplex multiplicators are This gives us and Since the non-basic variables must be increased from zero, it is possible to decrease the cost if and only if there is some negative component of .If , then we want to swap some element of with some element of . In other words, we want to increase one component of from precisely until one component of becomes . The component of to increase is just any component with , since this gives a reduced cost. Recall that we have In particular, when we choose to increase and leave the rest of zero we have This means that if at least one element of is positive, some component in will become zero when we increase . Otherwise, we can increase indefinitely and the objective value can thus become arbitrarily small, so then there is no finite optimum. From now on, we assume there is a finite optimum. Then we of course want to increase as little as possible, i.e. only such that one component in is zeroed. We thus choose With the Simplex method we always want to move to the corner that gives the maximally reduced cost, i.e. we choose such that is maximized. This can however be computationally inefficient since we need to compute each time, so instead we sometimes greedily choose the minimum component of .]]></description><link>matematik/optimization/linear-optimization/the-simplex-method.html</link><guid isPermaLink="false">Matematik/Optimization/Linear optimization/The Simplex method.md</guid><pubDate>Sun, 28 Dec 2025 19:28:38 GMT</pubDate></item><item><title><![CDATA[Basic feasible solutions]]></title><description><![CDATA[Remark
Here, we consider a <a data-tooltip-position="top" aria-label="The linear optimization problem" data-href="The linear optimization problem" href="matematik/optimization/linear-optimization/the-linear-optimization-problem.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">linear optimization problem</a><a data-tooltip-position="top" aria-label="The linear optimization problem" data-href="The linear optimization problem" href="matematik/optimization/linear-optimization/the-linear-optimization-problem.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">linear optimization problem</a> in standard form, where and we assume .
As noted about linear optimization problems, their feasible regions are simply intersections of half-planes, i.e. convex polygons. Note that since we know that moving in the opposite direction of decreases the cost. This means that the optimal solution is the point of in the feasible region that furthest in the -direction. It is thus obvious that the optimal solution cannot be strictly inside the feasible region. More importantly, whenever we consider a non-corner surface point, we see that at least one of the corners connected to the surface must have the same or lower cost; moving in the opposite direction reverses the cost change, so some direction must be in a nonnegative cost direction. It follows that an optimal solution must be a corner.<br>When considering the linear optimization in <a data-tooltip-position="top" aria-label="Standard form" data-href="Standard form" href="matematik/optimization/linear-optimization/standard-form.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">standard form</a><a data-tooltip-position="top" aria-label="Standard form" data-href="Standard form" href="matematik/optimization/linear-optimization/standard-form.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">standard form</a>, the feasible region is an intersection of a solution to a linear equation, i.e. an affine subspaces, with the region given by . Corners can thus only happen when sufficiently many coordinates are zero, forcing the rest of the coordinates to a unique solution. The idea is to consider sets of indices representing the only nonzero coordinates.Definition: Basic indices, basic variables
Let be a selection of distinct coordinates of , and let be the remaining distinct coordinates. and is a basic (resp. non-basic) index tuple consisting of basic (resp. non-basic) indices. and is the basic (resp. non-basic) variable vector consisting of basic (resp. non-basic) variables, corresponding to . and is the basic (resp. non-basic) matrix, corresponding to .
Definition: Basic solution, basic feasible solution
Let be a basic index tuple.
A solution to is a basic solution whenever .
A solution is a basic feasible solution whenever it is basic and feasible.
A solution is called degenerate whenever some coordinate of is zero, otherwise non-degenerate.
We can now formalize and prove our initial intuition.Theorem: The fundamental theorem of linear programming If there exists a feasible solution, then there exists a basic feasible solution.
If there exists an optimal solution, then there exists an optimal basic feasible solution. Proof: Let be a feasible solution. Let be the distinct indices of the nonzero coordinates of . If we are done. Suppose . Then cannot be injective. Take some nonzero and WLOG, assume some coordinate of is negative (otherwise, we could negate to get such a vector). Then there is some smallest such that has some zero coordinate. Since we choose the smallest such , the rest of the coordinates of remain nonnegative. Thus is another feasible solution with one more zero coordinate than . Repeat until .
Starting from an optimal solution , the exact same algorithm works. Notice that if , then , but then there exists a with a feasible solution and meaning contradicting the optimality of . Thus we must always have and so at no step of the algorithm does the cost change. ]]></description><link>matematik/optimization/linear-optimization/basic-feasible-solutions.html</link><guid isPermaLink="false">Matematik/Optimization/Linear optimization/Basic feasible solutions.md</guid><pubDate>Sun, 28 Dec 2025 19:03:55 GMT</pubDate></item><item><title><![CDATA[The linear optimization problem]]></title><description><![CDATA[An easy kind of optimization problem is where everything is linear.Definition: Inequalities between vectors of The expression where is componentwise, that is, Definition: Linear optimization problem
An <a data-tooltip-position="top" aria-label="The optimization problem" data-href="The optimization problem" href="matematik/optimization/the-optimization-problem.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">optimization problem</a><a data-tooltip-position="top" aria-label="The optimization problem" data-href="The optimization problem" href="matematik/optimization/the-optimization-problem.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">optimization problem</a> is called linear whenever it is of the form for some matrices , , some vectors , and some vector .
Remark
Equivalently, the problem is linear whenever the feasible region is and the cost function is Remark
Each equality constraint can be captured by two inequalities, so each linear optimization problem is equivalent to one of the form ]]></description><link>matematik/optimization/linear-optimization/the-linear-optimization-problem.html</link><guid isPermaLink="false">Matematik/Optimization/Linear optimization/The linear optimization problem.md</guid><pubDate>Sat, 27 Dec 2025 10:16:13 GMT</pubDate></item><item><title><![CDATA[Equivalences of optimization problems]]></title><description><![CDATA[Defining what it means for two <a data-tooltip-position="top" aria-label="The optimization problem" data-href="The optimization problem" href="matematik/optimization/the-optimization-problem.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">optimization problems</a><a data-tooltip-position="top" aria-label="The optimization problem" data-href="The optimization problem" href="matematik/optimization/the-optimization-problem.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">optimization problems</a> to be "equivalent" is hard, and is not usually done rigorously.Loose definition: Equivalent optimization problems
Two optimization problems are called equivalent whenever there are "simple" (as in "easy to compute") functions and such that The difficulty in defining this equivalence arises from optimization theory almost always being applied computationally. If we simply defined two problems being equivalent as the existence of arbitrary such functions as in the definition above, then all problems that have an optimal solution would be equivalent; we can simply let and map everything to an optimal solution.]]></description><link>matematik/optimization/equivalences-of-optimization-problems.html</link><guid isPermaLink="false">Matematik/Optimization/Equivalences of optimization problems.md</guid><pubDate>Thu, 25 Dec 2025 17:21:40 GMT</pubDate></item><item><title><![CDATA[The optimization problem]]></title><description><![CDATA[Optimization theory regards methods of solving problems that are of the general following form:Definition: Optimization problem, feasible region, cost function
An optimization problem is a problem of the following form: where is a set called the feasible region, and where is a real-valued function called the cost function or objective function.
Remark
Unless otherwise stated, is assumed to be a subset of .
Definition: Optimal value, feasible solution, optimal solution
The optimal value of an optimization problem is .
A vector is called a feasible solution whenever .
A vector is called an optimal solution whenever for all .
Remark
If an optimal solution exists, then is the optimal value. This is because is a minimum of and is thus the infimum.
]]></description><link>matematik/optimization/the-optimization-problem.html</link><guid isPermaLink="false">Matematik/Optimization/The optimization problem.md</guid><pubDate>Thu, 25 Dec 2025 10:37:33 GMT</pubDate></item><item><title><![CDATA[JSXGraph test]]></title><link>jsxgraph-test.html</link><guid isPermaLink="false">JSXGraph test.md</guid><pubDate>Fri, 17 Oct 2025 16:58:11 GMT</pubDate></item><item><title><![CDATA[Kladd]]></title><link>kladd.html</link><guid isPermaLink="false">Kladd.md</guid><pubDate>Fri, 17 Oct 2025 16:51:00 GMT</pubDate></item><item><title><![CDATA[Kladd katex 4]]></title><link>kladd-katex-4.html</link><guid isPermaLink="false">Kladd katex 4.md</guid><pubDate>Fri, 17 Oct 2025 15:25:16 GMT</pubDate></item><item><title><![CDATA[jsxgraph test]]></title><link>jsxgraph-test.html</link><guid isPermaLink="false">jsxgraph test.html</guid><pubDate>Fri, 17 Oct 2025 14:05:49 GMT</pubDate></item><item><title><![CDATA[Kladd katex 3]]></title><description><![CDATA[12345−1−2−3−4−5123−1−2−30,0–o+←↓↑→k = 1.00f(x) = e^{\int x}AB]]></description><link>kladd-katex-3.html</link><guid isPermaLink="false">Kladd katex 3.md</guid><pubDate>Fri, 17 Oct 2025 14:07:23 GMT</pubDate></item><item><title><![CDATA[Kladd katex 1]]></title><description><![CDATA[12345−1−2−3−4−5123−1−2−30,00,00,0–o+←↓↑→k=1.00k = 1.00k=1.00f(x)=sin⁡(tx)f(x) = \sin(tx)f(x)=sin(tx)AAABBB]]></description><link>kladd-katex-1.html</link><guid isPermaLink="false">Kladd katex 1.md</guid><pubDate>Fri, 17 Oct 2025 11:45:55 GMT</pubDate></item><item><title><![CDATA[Kladd katex 2]]></title><description><![CDATA[12345−1−2−3−4−5123−1−2−30,00,00,0–o+←↓↑→k=1.00k = 1.00k=1.00f(x)=e∫xf(x) = e^{\int x}f(x)=e∫xAAABBB]]></description><link>kladd-katex-2.html</link><guid isPermaLink="false">Kladd katex 2.md</guid><pubDate>Fri, 17 Oct 2025 11:34:15 GMT</pubDate></item><item><title><![CDATA[Inför tenta]]></title><description><![CDATA[
Euler-diskretisering bakåt: ersätt derivata med för något tidsintervall Vid rotort: kom ihåg att hitta skärningspunkter med imaginära axeln för att hitta stabila värden på .
Vid rotort: titta även på för vilka rötterna är på reella axeln. Abrupta hopp därifrån sker ofta ut i det komplexa planet.
Vid rotort: kom ihåg att den ska vara symmetrisk kring reella axeln!
Använd graf för att hitta passande för fasavancering, använd för att få fasavanceringen vid . Förstärkningen av lead länken vid är , så använd detta för att få rätt förstärkning. För att uppnå rätt reglerfel med lag-länken kan alltid (?) väljas till och sedan bestämmas så stort som möjligt med slutvärdessatsen för , d.v.s sätt .
Styrbarhet = finns input så att tillståndet nås?
Observerbarhet = påverkar tillståndet output?
Vid kancellation av pol/nollställe: om polen sker innan nollstället så kan tillståndet där polen verkar nås, alltså är den styrbar, men nollstället motverkar sedan denna verkan och polen är alltså inte observerbar. Om polen istället sker efter nollstället så ser nollstället till att tillståndet där polen verkar aldrig nås, alltså är polen ej styrbar. Om den däremot skulle vara i tillståndet där polen verkar så skulle det såklart påverka output, så polen är då observerbar.
Observatör: att simulera inre tillstånd när dessa inte är mätbara, med felkorrigering m.h.a och (som antas vara mätbart). Sätter då och väljer som kolumnvektor för att få önskad felkorrigering. Skattningsfelet ges då av Tustins approximationsformel: Approximera där För att lösa ut , ersätt med och addera ekvationen som fås med samma ekvation fast är ersatt med . Då fås vänsterled som ovan som kan ersättas med högerleden.
Kom ihåg vid Laplaceinvers.
]]></description><link>fysik/reglerteknik/inför-tenta.html</link><guid isPermaLink="false">Fysik/Reglerteknik/Inför tenta.md</guid><pubDate>Sun, 12 Oct 2025 09:35:46 GMT</pubDate></item><item><title><![CDATA[8.9 - Styrbarhet och observerbarhet]]></title><description><![CDATA[Systemet ges av Definition: Styrbart tillstånd
Ett tillstånd kallas styrbart om det kan nås från origo på ändlig tid. Systemet är styrbart om alla tillstånd är styrbara.
Definition: Observerbart tillstånd
Ett tillstånd kallas icke-observerbart om utsignalen är identiskt noll då initialvärdet är och insignalen är identiskt noll, d.v.s, påverkar inte utsignalen. Systemet är observerbart om det saknar icke-observerbara tillstånd.
Sats: Styrbarhet
De styrbara tillstånden är kolumnrummet till Sats: Observerbarhet
De icke-observerbara tillstånden är nollrummet till ]]></description><link>fysik/reglerteknik/8.9-styrbarhet-och-observerbarhet.html</link><guid isPermaLink="false">Fysik/Reglerteknik/8.9 - Styrbarhet och observerbarhet.md</guid><pubDate>Wed, 01 Oct 2025 11:11:34 GMT</pubDate></item><item><title><![CDATA[Kladd]]></title><description><![CDATA[Help me analyze the root locus when and .]]></description><link>fysik/reglerteknik/kladd.html</link><guid isPermaLink="false">Fysik/Reglerteknik/Kladd.md</guid><pubDate>Tue, 30 Sep 2025 11:36:51 GMT</pubDate></item><item><title><![CDATA[5.1-5.2 - Kretsförstärkning och stabilitet]]></title><description><![CDATA[Ett återkopplat system beskrivs av där känslighetsfunktionen och det slutna systemet ges av kallas kretsförstärkning och kompenseringslänk.Vi undersöker nu stabiliteten för det återkopplade systemet baserat på Bodediagram för . Vi förutsätter att ej har poler i höger halvplan. Detta har i själva verket redan gjorts med <a data-href="Nyquistkriteriet" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Nyquistkriteriet</a>. Nu översätter vi bara det till Bodediagram.Vi antar först att ett visst Nyquistdiagram inte omsluter , samt att kurvan skär enhetscirkeln i precis en punkt. Vi låter beteckna vinkeln mellan den negativa reella axeln och denna punkt. Att inte omsluts betyder alltså att . Vi låter även beteckna det inversa avståndet till punkten där kurvan skär den negativa reella axeln, som vi även antar sker på endast en punkt. Att inte omsluts betyder då alltså att .Att Nyquistkurvan skär enhetscirkeln betyder att , och vi kallar den vinkelnfrekvensen för skärfrekvensen. blir ett mått på hur mycket faskurvan kan förskjutas utan att systemet blir instabilt, och kallas därför fasmarginal. kan avläsas i ett Bodediagram som argumentkurvans avstånd till vid .På samma sätt blir ett mått på hur mycket amplitudkurvan kan höjas utan att systemet blir instabilt, och kallas därför amplitudmarginal eller förstärkningsmarginal. Den kan avläsas i ett Bodediagram logaritmiskt som avståndet till , eftersom .Vi kallar frekvensen där argumentkurvan skär , alltså där Nyquistkurvan skär den negativa reella axeln, för fas-skärfrekvensen.Bodediagrammet ger oss en intuitiv tolkning av Nyquistkriteriet. Tag ett öppet system med insignalen , och antag att den är precis på stabilitetsgränsen, så att och . Då har vi alltså att , alltså och . Detta ger Men om vi istället tittar på det slutna systemet utan insignal blir ju då insignalen densamma, alltså kan det slutna systemet självsvänga i det här läget. Om vi tar ett litet steg mot stabilitet är det ju då intuitivt att svängingarna dör ut, medan om vi tar ett litet steg ifrån stabilitet så amplifieras dessa svängningar mot oändligheten. Detta resonemang gäller dock förstås bara för diagram där de antaganden vi gjort gäller, d.v.s att kurvan endast skär den reella axeln och enhetscirkeln en gång.]]></description><link>fysik/reglerteknik/5.1-5.2-kretsförstärkning-och-stabilitet.html</link><guid isPermaLink="false">Fysik/Reglerteknik/5.1-5.2 - Kretsförstärkning och stabilitet.md</guid><pubDate>Mon, 29 Sep 2025 13:51:45 GMT</pubDate></item><item><title><![CDATA[Fields]]></title><description><![CDATA[Definition: Skew field
A skew field or division ring is a <a data-tooltip-position="top" aria-label="Matematik/Groups and rings/Lecture notes/Rings" data-href="Matematik/Groups and rings/Lecture notes/Rings" href="matematik/groups-and-rings/lecture-notes/rings.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Ring</a><a data-tooltip-position="top" aria-label="Matematik/Groups and rings/Lecture notes/Rings" data-href="Matematik/Groups and rings/Lecture notes/Rings" href="matematik/groups-and-rings/lecture-notes/rings.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Ring</a> such that .
Definition: Field
A field is a commutative skew field.
]]></description><link>matematik/groups-and-rings/lecture-notes/fields.html</link><guid isPermaLink="false">Matematik/Groups and rings/Lecture notes/Fields.md</guid><pubDate>Fri, 26 Sep 2025 13:49:27 GMT</pubDate></item><item><title><![CDATA[Groups and rings, Lectures]]></title><description><![CDATA[
<a data-href="Matematik/Groups and rings/Lecture notes/Groups" href="matematik/groups-and-rings/lecture-notes/groups.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Matematik/Groups and rings/Lecture notes/Groups</a><a data-href="Matematik/Groups and rings/Lecture notes/Groups" href="matematik/groups-and-rings/lecture-notes/groups.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Matematik/Groups and rings/Lecture notes/Groups</a>
<br><a data-href="Permutation groups" href="matematik/groups-and-rings/lecture-notes/permutation-groups.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Permutation groups</a><a data-href="Permutation groups" href="matematik/groups-and-rings/lecture-notes/permutation-groups.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Permutation groups</a>
<br><a data-tooltip-position="top" aria-label="Matematik/Groups and rings/Lecture notes/Subgroups" data-href="Matematik/Groups and rings/Lecture notes/Subgroups" href="matematik/groups-and-rings/lecture-notes/subgroups.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Subgroups</a><a data-tooltip-position="top" aria-label="Matematik/Groups and rings/Lecture notes/Subgroups" data-href="Matematik/Groups and rings/Lecture notes/Subgroups" href="matematik/groups-and-rings/lecture-notes/subgroups.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Subgroups</a>
<br><a data-href="Direct groups" href="matematik/groups-and-rings/lecture-notes/direct-groups.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Direct groups</a><a data-href="Direct groups" href="matematik/groups-and-rings/lecture-notes/direct-groups.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Direct groups</a>
<br><a data-href="Matematik/Groups and rings/Lecture notes/Homomorphisms" href="matematik/groups-and-rings/lecture-notes/homomorphisms.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Matematik/Groups and rings/Lecture notes/Homomorphisms</a><a data-href="Matematik/Groups and rings/Lecture notes/Homomorphisms" href="matematik/groups-and-rings/lecture-notes/homomorphisms.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Matematik/Groups and rings/Lecture notes/Homomorphisms</a>
<br><a data-tooltip-position="top" aria-label="Matematik/Groups and rings/Lecture notes/Cosets" data-href="Matematik/Groups and rings/Lecture notes/Cosets" href="matematik/groups-and-rings/lecture-notes/cosets.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Cosets</a><a data-tooltip-position="top" aria-label="Matematik/Groups and rings/Lecture notes/Cosets" data-href="Matematik/Groups and rings/Lecture notes/Cosets" href="matematik/groups-and-rings/lecture-notes/cosets.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Cosets</a>
<br><a data-href="Matematik/Groups and rings/Lecture notes/Normal subgroups" href="matematik/groups-and-rings/lecture-notes/normal-subgroups.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Matematik/Groups and rings/Lecture notes/Normal subgroups</a><a data-href="Matematik/Groups and rings/Lecture notes/Normal subgroups" href="matematik/groups-and-rings/lecture-notes/normal-subgroups.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Matematik/Groups and rings/Lecture notes/Normal subgroups</a>
<br><a data-href="Matematik/Groups and rings/Lecture notes/Cyclic groups" href="matematik/groups-and-rings/lecture-notes/cyclic-groups.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Matematik/Groups and rings/Lecture notes/Cyclic groups</a><a data-href="Matematik/Groups and rings/Lecture notes/Cyclic groups" href="matematik/groups-and-rings/lecture-notes/cyclic-groups.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Matematik/Groups and rings/Lecture notes/Cyclic groups</a>
<br><a data-href="Matematik/Groups and rings/Lecture notes/Group actions" href="matematik/groups-and-rings/lecture-notes/group-actions.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Matematik/Groups and rings/Lecture notes/Group actions</a><a data-href="Matematik/Groups and rings/Lecture notes/Group actions" href="matematik/groups-and-rings/lecture-notes/group-actions.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Matematik/Groups and rings/Lecture notes/Group actions</a>
<br><a data-href="Matematik/Groups and rings/Lecture notes/Centers and centralizers" href="matematik/groups-and-rings/lecture-notes/centers-and-centralizers.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Matematik/Groups and rings/Lecture notes/Centers and centralizers</a><a data-href="Matematik/Groups and rings/Lecture notes/Centers and centralizers" href="matematik/groups-and-rings/lecture-notes/centers-and-centralizers.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Matematik/Groups and rings/Lecture notes/Centers and centralizers</a>
<br><a data-href="Matematik/Groups and rings/Lecture notes/Sylow Groups" href="matematik/groups-and-rings/lecture-notes/sylow-groups.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Matematik/Groups and rings/Lecture notes/Sylow Groups</a><a data-href="Matematik/Groups and rings/Lecture notes/Sylow Groups" href="matematik/groups-and-rings/lecture-notes/sylow-groups.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Matematik/Groups and rings/Lecture notes/Sylow Groups</a>
<br><a data-href="Finite abelian groups" href="matematik/groups-and-rings/lecture-notes/finite-abelian-groups.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Finite abelian groups</a><a data-href="Finite abelian groups" href="matematik/groups-and-rings/lecture-notes/finite-abelian-groups.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Finite abelian groups</a> <br><a data-href="Matematik/Groups and rings/Lecture notes/Rings" href="matematik/groups-and-rings/lecture-notes/rings.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Matematik/Groups and rings/Lecture notes/Rings</a><a data-href="Matematik/Groups and rings/Lecture notes/Rings" href="matematik/groups-and-rings/lecture-notes/rings.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Matematik/Groups and rings/Lecture notes/Rings</a>
<br><a data-href="Ideals" href="matematik/groups-and-rings/lecture-notes/ideals.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Ideals</a><a data-href="Ideals" href="matematik/groups-and-rings/lecture-notes/ideals.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Ideals</a>
<br><a data-href="Ring homomorphisms and Factor rings" href="matematik/groups-and-rings/lecture-notes/ring-homomorphisms-and-factor-rings.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Ring homomorphisms and Factor rings</a><a data-href="Ring homomorphisms and Factor rings" href="matematik/groups-and-rings/lecture-notes/ring-homomorphisms-and-factor-rings.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Ring homomorphisms and Factor rings</a>
<br><a data-href="Euclidean domains" href="matematik/groups-and-rings/lecture-notes/euclidean-domains.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Euclidean domains</a><a data-href="Euclidean domains" href="matematik/groups-and-rings/lecture-notes/euclidean-domains.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Euclidean domains</a>
Rings Comm. rings Integral domains GCD domains UFD's PID's Euclidean domains Fields.]]></description><link>matematik/groups-and-rings,-lectures.html</link><guid isPermaLink="false">Matematik/Groups and rings, Lectures.md</guid><pubDate>Fri, 26 Sep 2025 13:49:27 GMT</pubDate></item><item><title><![CDATA[Subgroups]]></title><description><![CDATA[<a href=".?query=tag:matematik-groups" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-groups">#matematik-groups</a> <br>Definition: Submonoid, subgroup
Let be sets, and . If and are <a data-tooltip-position="top" aria-label="Matematik/Groups and rings/Lecture notes/Groups > ^5927de" data-href="Matematik/Groups and rings/Lecture notes/Groups#^5927de" href="matematik/groups-and-rings/lecture-notes/groups.html#^5927de" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Monoids</a><a data-tooltip-position="top" aria-label="Matematik/Groups and rings/Lecture notes/Groups > ^5927de" data-href="Matematik/Groups and rings/Lecture notes/Groups#^5927de" href="matematik/groups-and-rings/lecture-notes/groups.html#^5927de" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Monoids</a> under the same operation, is called a submonoid of . If and are <a data-tooltip-position="top" aria-label="Matematik/Groups and rings/Lecture notes/Groups > ^5927de" data-href="Matematik/Groups and rings/Lecture notes/Groups#^5927de" href="matematik/groups-and-rings/lecture-notes/groups.html#^5927de" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Groups</a><a data-tooltip-position="top" aria-label="Matematik/Groups and rings/Lecture notes/Groups > ^5927de" data-href="Matematik/Groups and rings/Lecture notes/Groups#^5927de" href="matematik/groups-and-rings/lecture-notes/groups.html#^5927de" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Groups</a> under the same operation, is called a subgroup of and we write .
We see that every group contains two trivial subgroups: itself and the trivial group . If a subgroup is not one of these two trivial subgroups, we often call it nontrivial.From this definition the following theorems follow directly and so we state them without proofs.Theorem
Let be a monoid with unit , and . Then is a submonoid of if and only if and for all .
Theorem
Let be a group and . Then is a subgroup of if and only if is a submonoid of and for all .
We also find that we can find an often easier equivalent requirement for a subset to be a subgroup.Theorem
Let be a group and where . Then is a subgroup of if and only if , or in other words, for all .
Proof:
If is a subgroup of , it is clear that since is a group and thus closed under composition and inverses. If instead , we can take any and know that , so . This also means for all , which finally means that for all . So is indeed a subgroup of . ]]></description><link>matematik/groups-and-rings/lecture-notes/subgroups.html</link><guid isPermaLink="false">Matematik/Groups and rings/Lecture notes/Subgroups.md</guid><pubDate>Fri, 26 Sep 2025 13:48:18 GMT</pubDate></item><item><title><![CDATA[Rings]]></title><description><![CDATA[Definition: Ring
A ring is a set together with two maps and such that is an <a data-tooltip-position="top" aria-label="Matematik/Groups and rings/Lecture notes/Groups > ^5927de" data-href="Matematik/Groups and rings/Lecture notes/Groups#^5927de" href="matematik/groups-and-rings/lecture-notes/groups.html#^5927de" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Abelian group</a><a data-tooltip-position="top" aria-label="Matematik/Groups and rings/Lecture notes/Groups > ^5927de" data-href="Matematik/Groups and rings/Lecture notes/Groups#^5927de" href="matematik/groups-and-rings/lecture-notes/groups.html#^5927de" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Abelian group</a>, is a monoid, is distributive over , meaning for every . The additive and multiplicative unit elements are denoted and , respectively, and are called identities in the context of rings.
The additive inverse of is denoted .
Theorem
Let be a ring. Then for every .
Proof: , and the same for .
, and the same for . Definition: Subring, ring extension
Let be a ring. is a subring of if is a ring and . The pair is called a ring extension.
Definition: Unit
Let be a ring. Then whose elements are called units.
]]></description><link>matematik/groups-and-rings/lecture-notes/rings.html</link><guid isPermaLink="false">Matematik/Groups and rings/Lecture notes/Rings.md</guid><pubDate>Fri, 26 Sep 2025 13:48:18 GMT</pubDate></item><item><title><![CDATA[Permutation groups]]></title><description><![CDATA[<a href=".?query=tag:matematik-groups" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-groups">#matematik-groups</a> Definition: Permutation group
Let be a set. We then define the permutation group on as together with composition .
<br>We can quite easily see that for any , is a <a data-tooltip-position="top" aria-label="Matematik/Groups and rings/Lecture notes/Groups" data-href="Matematik/Groups and rings/Lecture notes/Groups" href="matematik/groups-and-rings/lecture-notes/groups.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Group</a><a data-tooltip-position="top" aria-label="Matematik/Groups and rings/Lecture notes/Groups" data-href="Matematik/Groups and rings/Lecture notes/Groups" href="matematik/groups-and-rings/lecture-notes/groups.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Group</a> under composition . However, it is not necessarily commutative since the compositions of bijections are not necessarily commutative; swapping the first two elements and then the second two among three elements is different from swapping the second two and then the first two.Permutation groups are groups
Let be a set. Then is a group.
Proof:
If we know since the composition of two bijections is a bijection. Compositions are associative, and by definition invertible with the identity on as the unit element.
Notation
If , we define and call it the symmetric group of degree or the group of permutations of elements.
]]></description><link>matematik/groups-and-rings/lecture-notes/permutation-groups.html</link><guid isPermaLink="false">Matematik/Groups and rings/Lecture notes/Permutation groups.md</guid><pubDate>Fri, 26 Sep 2025 13:48:18 GMT</pubDate></item><item><title><![CDATA[Homomorphisms]]></title><description><![CDATA[<a href=".?query=tag:matematik-groups" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-groups">#matematik-groups</a> <br>Definition: Homomorphism
Let and be <a data-tooltip-position="top" aria-label="Matematik/Groups and rings/Lecture notes/Groups > ^5927de" data-href="Matematik/Groups and rings/Lecture notes/Groups#^5927de" href="matematik/groups-and-rings/lecture-notes/groups.html#^5927de" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Monoids</a><a data-tooltip-position="top" aria-label="Matematik/Groups and rings/Lecture notes/Groups > ^5927de" data-href="Matematik/Groups and rings/Lecture notes/Groups#^5927de" href="matematik/groups-and-rings/lecture-notes/groups.html#^5927de" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Monoids</a> with units , respectively. A map is a monoid homomorphism if and for all . If are <a data-tooltip-position="top" aria-label="Matematik/Groups and rings/Lecture notes/Groups > ^5927de" data-href="Matematik/Groups and rings/Lecture notes/Groups#^5927de" href="matematik/groups-and-rings/lecture-notes/groups.html#^5927de" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Groups</a><a data-tooltip-position="top" aria-label="Matematik/Groups and rings/Lecture notes/Groups > ^5927de" data-href="Matematik/Groups and rings/Lecture notes/Groups#^5927de" href="matematik/groups-and-rings/lecture-notes/groups.html#^5927de" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Groups</a>, we call a group homomorphism.
Theorem
Let be a map between groups. If for all , is a group homomorphism, and .
Proof:
Since are units, we have so is a group homomorphism. Furthermore, we have so .
Definition
Let be groups. We define... as the set of all homomorphisms from to , as the set of all endomorphisms of , as the set of all automorphisms of . If a group homomorphism is... Injective, it is called a monomorphism
Surjective, it is called a epimorphism
Bijective, it is called an isomorphism Theorem: Compositions of homomorphisms are homomorphisms
Let be groups, and . Then Furthermore, if is a group, is a group.
Proof:
We have so . Furthermore, we know that the composition of two bijections is a bijection, hence for all . We also know that the identity bijection is in , and that every bijection has an inverse also in , so is indeed a group.
Definition
Let be groups, and . We define the kernel We also define the image Theorem
Let be groups, and . Then is injective is surjective Proof:<br>
7. {a} If , then , so which means according to <a data-tooltip-position="top" aria-label="Matematik/Groups and rings/Lecture notes/Subgroups > ^0fcb50" data-href="Matematik/Groups and rings/Lecture notes/Subgroups#^0fcb50" href="matematik/groups-and-rings/lecture-notes/subgroups.html#^0fcb50" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Theorem</a><a data-tooltip-position="top" aria-label="Matematik/Groups and rings/Lecture notes/Subgroups > ^0fcb50" data-href="Matematik/Groups and rings/Lecture notes/Subgroups#^0fcb50" href="matematik/groups-and-rings/lecture-notes/subgroups.html#^0fcb50" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Theorem</a>.
8. For any we have , meaning .
9. We know , so if is injective we get . If instead and we get so is injective.
10. This is true by definition.
11. For all we get since is a group, so .
12. We have . For any we have since is a group.
Example
Let be a group and be a homomorphism. Then we have So any homomorphism from to a group is uniquely determine by where it maps . Note that this also holds for negative numbers since respects inverses. Definition: Translation
Let be a group and . We define the (left) translation map by as where for all .
We note that translation by is generally not a homomorphism since . However, the map that takes to is an injective group homomorphism since and every if since and . This means can be identified with its image in , which thus is a subgroup of (for finite groups, this is the result of Cayley's theorem).Of course, we can also define the right translation map, and get basically the same results, but the maps are of course generally not the same. This asymmetry leads us to also look at conjugation.Definition Conjugation
Let be a group and . We define the conjugation by as where for all . is also called an inner isomorphism of . And we define .
We know that because , and is clearly an inverse of . This time, however, we only get that the map that takes to is still a homomorphism, but generally no longer an injective one. It's a homomorphism since and , but generally not injective since for example, if is abelian, we get for every .]]></description><link>matematik/groups-and-rings/lecture-notes/homomorphisms.html</link><guid isPermaLink="false">Matematik/Groups and rings/Lecture notes/Homomorphisms.md</guid><pubDate>Fri, 26 Sep 2025 13:48:18 GMT</pubDate></item><item><title><![CDATA[Natural numbers]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a> <br>Definition: Axioms of natural numbers
We define the natural numbers to be a set together with an operation addition and an operation multiplication such that is a <a data-tooltip-position="top" aria-label="Matematik/Groups and rings/Lecture notes/Groups" data-href="Matematik/Groups and rings/Lecture notes/Groups" href="matematik/groups-and-rings/lecture-notes/groups.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">commutative semigroup</a><a data-tooltip-position="top" aria-label="Matematik/Groups and rings/Lecture notes/Groups" data-href="Matematik/Groups and rings/Lecture notes/Groups" href="matematik/groups-and-rings/lecture-notes/groups.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">commutative semigroup</a> and is a <a data-tooltip-position="top" aria-label="Matematik/Groups and rings/Lecture notes/Groups" data-href="Matematik/Groups and rings/Lecture notes/Groups" href="matematik/groups-and-rings/lecture-notes/groups.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">commutative monoid</a><a data-tooltip-position="top" aria-label="Matematik/Groups and rings/Lecture notes/Groups" data-href="Matematik/Groups and rings/Lecture notes/Groups" href="matematik/groups-and-rings/lecture-notes/groups.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">commutative monoid</a> with unit , with the following axioms: If for any , then . for every <br><a data-tooltip-position="top" aria-label="^bb32a3" data-href="#^bb32a3" href="#^bb32a3" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Law of trichotomy</a><a data-tooltip-position="top" aria-label="^bb32a3" data-href="#^bb32a3" href="#^bb32a3" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Law of trichotomy</a>
<br><a data-tooltip-position="top" aria-label="Induction > ^07b21a" data-href="Induction#^07b21a" href="matematik/discrete-mathematics/induction.html#^07b21a" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Principle of induction</a><a data-tooltip-position="top" aria-label="Induction > ^07b21a" data-href="Induction#^07b21a" href="matematik/discrete-mathematics/induction.html#^07b21a" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Principle of induction</a> Definition: Multiple of is a multiple of , denoted , if there exists a such that .
Theorem
If and , then for every .
Proof:
We have and . This gives by the distributative property.
Definition: Less than, greater than
Let . We say that is less than , denoted , if there exists an such that . If is less than , we say that is greater than and write .
Axiom: Law of trichotomy
Exactly one of the three statements is true for any .
An important consequence of this definition is that for every .Theorem: Reflexivity of If and , then .
Proof:
We have and , so .
Theorem
Let . If , then .
Proof:
Assume . WLOG, let . Then , so which means which is clearly false since and only one can be true at once.
]]></description><link>matematik/discrete-mathematics/natural-numbers.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/Natural numbers.md</guid><pubDate>Fri, 26 Sep 2025 13:48:18 GMT</pubDate></item><item><title><![CDATA[5.3 - Specifikationer i frekvensplanet]]></title><description><![CDATA[Vi ska nu se hur felkoefficienterna kan beräknas från Bodediagrammet. Vi har att bestäms alltså av det öppna systemets förstärkning vid frekvensen , d.v.s den statiska förstärkningen. För att ska vara kan innehålla en eller flera integrationer, så att , och vi har då Eftersom för små får amplitudkurvan i ett Bodediagram för låga frekvenser. Motsvarande asymptot skär -axeln i . På samma sätt kan man finna att om innehåller två integrationer så att så har lågfrekvensasymptoten lutningen och skär -axeln i . Felkoefficienterna kan alltså avläsas från Bodediagrammet för låga frekvenser.Vi betraktar nu det slutna systemet Idealt skulle vi ha att för alla eftersom systemet då skulle reproducera alla signaler exakt, oberoende av deras frekvensinnehåll (d.v.s, signalerna är av olika frekvenser men detta påverkar inte deras individuella amplituder till utsignalen). Detta uppfylls approximativt i det frekvensområde där kretsförstärkningen är stor. Speciellt gäller att om .Flera andra prestandamått kan definieras. Bandbredden anger det -värde för vilket förstärkningen sjunker under ( dB). För frekvenser under gäller alltså att .Det är även vanligt att ett system har en resonanstopp, d.v.s en förstärkningstopp i närheten av en viss frekvens. Resonansfrekvensen är frekvensen vid maximumet. Resonanstoppens möjd anges ofta i specifikationer tillsammans med bandbredden. Det är inte lämpligt att resonanstoppen blir för stor eftersom vissa frekvenser då kommer dominera i utsignalen.Vi har ju att där För att få en bild av hur beror av kan man plotta nivåkurvorna för respektive i det komplexa talplanet. Detta kallas för ett halldiagram. Genom att samtidigt plotta , Nyquistkurvan, i ett halldiagram kan det slutna systemets amplitud- och faskarkteristik avläsas.Användbart:
Resonansfrekvensen motsvarar ofta ungefär den fundamentala frekvensen, d.v.s. frekvensen för insvängningen. Så en lägre resonansfrekvens ger långsammare svängningar och vice versa.
]]></description><link>fysik/reglerteknik/5.3-specifikationer-i-frekvensplanet.html</link><guid isPermaLink="false">Fysik/Reglerteknik/5.3 - Specifikationer i frekvensplanet.md</guid><pubDate>Sun, 14 Sep 2025 07:56:14 GMT</pubDate></item><item><title><![CDATA[8.7 - Lösning av tillståndsekvationerna]]></title><link>fysik/reglerteknik/8.7-lösning-av-tillståndsekvationerna.html</link><guid isPermaLink="false">Fysik/Reglerteknik/8.7 - Lösning av tillståndsekvationerna.md</guid><pubDate>Fri, 12 Sep 2025 08:56:15 GMT</pubDate></item><item><title><![CDATA[8.6 - Att gå från tillståndsform till överföringsfunktion]]></title><description><![CDATA[Sats
Överföringsfunktionen för systemet är Systemets poler är lika med -matrisens egenvärden.
Bevis:
Vi får alltså vilket ger vilket alltså motsvarar överföringsfunktionen kan skrivas som där är adjunktmatrisen, och detta har uppenbarligen poler vid precis egenvärdena till .
]]></description><link>fysik/reglerteknik/8.6-att-gå-från-tillståndsform-till-överföringsfunktion.html</link><guid isPermaLink="false">Fysik/Reglerteknik/8.6 - Att gå från tillståndsform till överföringsfunktion.md</guid><pubDate>Fri, 12 Sep 2025 08:56:07 GMT</pubDate></item><item><title><![CDATA[8.5 - Att gå från överföringsfunktion till tillståndsform]]></title><description><![CDATA[När vi har en överföringsfunktion på formen så kan detta skrivas på <a data-tooltip-position="top" aria-label="8.1-8.4 - Tillståndsbeskrivning > ^28155d" data-href="8.1-8.4 - Tillståndsbeskrivning#^28155d" href="fysik/reglerteknik/8.1-8.4-tillståndsbeskrivning.html#^28155d" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">tillståndsform</a><a data-tooltip-position="top" aria-label="8.1-8.4 - Tillståndsbeskrivning > ^28155d" data-href="8.1-8.4 - Tillståndsbeskrivning#^28155d" href="fysik/reglerteknik/8.1-8.4-tillståndsbeskrivning.html#^28155d" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">tillståndsform</a> på olika sätt.Om överföringsfunktionen kan partialbråkuppdelas i termer av formen så får vi alltså och inför som alltså betyder blir då uppenbarligen en diagonalmatris, därav namnet.Idén här är att om vi väljer och väljer alltså så får vi Denna form blir På liknande sätt får man formen Om gradtalet i täljaren är samma som i nämnaren så kan en konstant brytas ut så att och sedan lösas på något av de tidigare sätten.]]></description><link>fysik/reglerteknik/8.5-att-gå-från-överföringsfunktion-till-tillståndsform.html</link><guid isPermaLink="false">Fysik/Reglerteknik/8.5 - Att gå från överföringsfunktion till tillståndsform.md</guid><pubDate>Fri, 12 Sep 2025 08:49:08 GMT</pubDate></item><item><title><![CDATA[8.1-8.4 - Tillståndsbeskrivning]]></title><description><![CDATA[En formell beskrivning av ett systems tillstånd vid en tidpunkt säger vi är information tillräcklig för att förutsäga effekten av en pålagd signal för . Till exempel är en sådan informationsmängd. Ett annat sätt är såklart med begynnelsevillkor för en diffekvation.För en linjär differentialekvation av ordning inför vi Derivatorna av dessa ges då av för , och ges av differentialekvationen. Om vi då har får vi alltså Med en -matris , en kolumnvektor och en radvektor , samt genom att införa kan allt detta skrivas på formen blir då en beskrivning av systemets tillstånd och vi kallar därför för tillståndsvektorn.Vi generaliserar detta något och inför även en konstant , för att i allmänhet få systemet En fördel med detta skrivsätt är också att vi kan skriva ett system med flera insignaler på samma sätt, och då helt enkelt välja till en vektor. ]]></description><link>fysik/reglerteknik/8.1-8.4-tillståndsbeskrivning.html</link><guid isPermaLink="false">Fysik/Reglerteknik/8.1-8.4 - Tillståndsbeskrivning.md</guid><pubDate>Fri, 12 Sep 2025 08:18:29 GMT</pubDate></item><item><title><![CDATA[Free modules]]></title><description><![CDATA[where is on component , elsewhere.These span all of , that is, any element , and these are linearly independent, that is, Observe that that is, each is determined by where it sends which is .
Surjectivity of generate (span) .
Injectivity of linearly ind. isomorphism basis.Proposition a submodule of .
a. If is finitely generated, then so is .
b. If and are finitely generated, then so is .
c. If is free and is free, then is free. In particular, .
Proof:
a. Clear.
b. Let generate , and let be elements in such that generates . Now take any . Then and since the kernel is and these differ by such an element, is indeed generated by and .
c. Similar.
However, if a module is finitely generated, not all submodules of are necessarily finitely generated!]]></description><link>matematik/groups-and-rings/lecture-notes/free-modules.html</link><guid isPermaLink="false">Matematik/Groups and rings/Lecture notes/Free modules.md</guid><pubDate>Thu, 11 Sep 2025 08:32:25 GMT</pubDate></item><item><title><![CDATA[6.5 - En allmän linjär återkoppling]]></title><description><![CDATA[En allmän linjär regulatorstruktur ges enligt tidigare av Vi lägger även till en utsignalstörning och en mätstörning . Vi kallar för förkompensering och för återkopplingskompensering. Konfigurationen vi jobbat med tidigare i detta kapitel är därmed specialfallet där .Vi får följande samband mellan signalerna: där vi infört Resultaten för och som härletts i detta kapitel gäller därmed även för detta mer generella fall, där vi ersätter med .Vi ser att om så blir , medan andra val av gör det möjligt att forma oberoende av och . För givna och kan vi alltså välja fritt så att och uppfyller sina krav, och därefter bestämma så att blir den önskade.]]></description><link>fysik/reglerteknik/6.5-en-allmän-linjär-återkoppling.html</link><guid isPermaLink="false">Fysik/Reglerteknik/6.5 - En allmän linjär återkoppling.md</guid><pubDate>Wed, 10 Sep 2025 08:16:41 GMT</pubDate></item><item><title><![CDATA[6.4 - Modellfel och känslighetsfunktionen]]></title><description><![CDATA[Modellfel som inte påverkar stabilitet kan såklart fortfarande påverka prestanda. Återigen med det verkliga systemet får vi den verkliga utsignalen där är den beräknade utsignalen och är känslighetsfunktionen utvärderad för det verkliga systemet, alltså Vi kan uttrycka detta som ett relativt fel i utsignalen: blir alltså överföringsfunktionen från det relativa felet i modellen till det relativa felet i utsignalen. är ju dock inte känd eftersom den beror av , men kan approximeras som för små fel.]]></description><link>fysik/reglerteknik/6.4-modellfel-och-känslighetsfunktionen.html</link><guid isPermaLink="false">Fysik/Reglerteknik/6.4 - Modellfel och känslighetsfunktionen.md</guid><pubDate>Wed, 10 Sep 2025 08:08:24 GMT</pubDate></item><item><title><![CDATA[6.3 - Robusthet gentemot instabilitet]]></title><description><![CDATA[ är i princip alltid en förenkling av verkligheten. Vi antar att det verkliga systemet ges av där alltså är det relativa modellfelet.Antag att det verkliga systemet ligger precis på stabilitetsgränsen, alltså Detta betyder att där är <a data-tooltip-position="top" aria-label="6.2 - Att minimera känslighetsfunktionen > ^de6532" data-href="6.2 - Att minimera känslighetsfunktionen#^de6532" href="fysik/reglerteknik/6.2-att-minimera-känslighetsfunktionen.html#^de6532" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">den komplementära känslighetsfunktionen</a><a data-tooltip-position="top" aria-label="6.2 - Att minimera känslighetsfunktionen > ^de6532" data-href="6.2 - Att minimera känslighetsfunktionen#^de6532" href="fysik/reglerteknik/6.2-att-minimera-känslighetsfunktionen.html#^de6532" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">den komplementära känslighetsfunktionen</a>. Om vi alltså skulle kräva att för alla så nås aldrig stabilitetsgränsen och det slutna systemet förblir stabilt när ersätts med . Detta kan visas mer formellt och kallas Rouchés sats.Sats: Robusthetskriterium
Låt vara en återkoppling som stabiliserar en modell . Antag att det verkliga systemet är enligt tidigare. Antag att och har samma antal poler i höger halvplan (origo inräknat), samt att och båda går mot noll när går mot oändligheten. Antag slutligen att för alla . Då är även det slutna system som erhålls då återkopplas med stabilt.
Vi använder begreppet robusthet för att beskriva en tolerans mot modellfel. Mer specifikt stabilitetsrobusthet när vi bryr oss om modellfelets påverkan på stabilitet.]]></description><link>fysik/reglerteknik/6.3-robusthet-gentemot-instabilitet.html</link><guid isPermaLink="false">Fysik/Reglerteknik/6.3 - Robusthet gentemot instabilitet.md</guid><pubDate>Wed, 10 Sep 2025 08:02:04 GMT</pubDate></item><item><title><![CDATA[6.2 - Att minimera känslighetsfunktionen]]></title><description><![CDATA[Vi vill nu undersöka vad som sätter gränsen för den prestanda som kan uppnås.Vi har enligt tidigare där är känslighetsfunktionen där i sin tur . Vi ser att reglerfelet kan göras godtyckligt litet om görs tillräckligt liten för alla vinkelfrekvenser , vilket i sin tur görs om görs tillräckligt stor. Det finns dock en del hinder.Vi har även Som sagt måste göras stor för att få liten, så då gäller och därmed Om systemet är väl dämpat så är liten, och om det då finns störningar och/eller bidrag från referenssignalen så behövs alltså mycket stora styrsignaler, vilket det alltid finns tekniska begränsningar på.I verkligheten kan vi aldrig mäta systemets prestation exakt, utan får alltid ett mätbrus när vi mäter utsignalen som vi nu kallar . Vi kallar vår uppmätta utsignal som alltså är Vi får följande överföringsfunktion från mätbruset till utsignalen: När känslighetsfunktionen är liten slår alltså mätbruset igenom direkt till utsignalen. Känslighetsfunktionen kan därmed endast göras liten i de frekvensområden där mätstörningarna är små. Vi inför som alltså motsvarar förstärkningen (med omvänt tecken) från mätbrus till , och kallas för den komplementära känslighetsfunktionen. Även med perfekt modell, perfekta mätgivare och obegränsad styrsignal finns begränsningar.Sats: Bodes integral
Antag att kretsförstärkningen har poler med positiv realdel och att dess övriga poler har strikt negativ realdel. Antag också att skillnaden mellan nämnarens och täljarens gradtal hos är minst . Om det återkopplade systemet har alla poler strikt i vänster halvplan gäller då Detta innebär att vi inte kan göra hur liten som helst för alla vinkelfrekvenser, oavsett reglersystem. Om till exempel inte har poler i höger halvplan blir integralen alltså , vilket betyder att frekvensområden där måste balanseras ut av områden där . Vi ser också att poler i höger halvplan hos det öppna systemet ger en högre genomsnittlig känslighet.]]></description><link>fysik/reglerteknik/6.2-att-minimera-känslighetsfunktionen.html</link><guid isPermaLink="false">Fysik/Reglerteknik/6.2 - Att minimera känslighetsfunktionen.md</guid><pubDate>Wed, 10 Sep 2025 07:56:04 GMT</pubDate></item><item><title><![CDATA[5.5 - Tidsfördröjningar och Bodediagram]]></title><description><![CDATA[En tidsfördröjning på tidsenheter motsvarar . Detta ger Det blir alltså en kraftigt negativ fasförskjutning, alltså en sänkning av fasmarginalen.]]></description><link>fysik/reglerteknik/5.5-tidsfördröjningar-och-bodediagram.html</link><guid isPermaLink="false">Fysik/Reglerteknik/5.5 - Tidsfördröjningar och Bodediagram.md</guid><pubDate>Tue, 09 Sep 2025 09:22:45 GMT</pubDate></item><item><title><![CDATA[5.4 - Kompensering med hjälp av Bodediagram]]></title><description><![CDATA[Nu när vi har sätt att mäta prestandan av ett system behöver vi sätt att givet hitta en kompenseringslänk som uppfyller våra kriterier.Det kanske enklaste är ju en P-regulator, där . Då kan ökas tills skärfrekvensen blir tillräckligt stor. Däremot kan fasmarginalen vara för liten här, så då behövs något mer komplicerat. Det behövs alltså en kompenseringslänk som höjer faskurvan vid . En sådan länk kallas för en fasavancerande länk.Vi har tidigare sätt att PD-reglering har en stabiliserande verkan, så vi undersöker dessa först. Vi använder här en "mjukare" derivering enligt där vår vanliga PD-regulator alltså ges av . Vi ser hur detta påverkar fasmarginalen: vilket har ett maximum av storleken för Förstärkningen hos för detta blir Dessutom gäller och .Mindre ger större fasavancering vilket alltså löser problemet med fasmarginalen. Däremot ger mindre alltså också större förstärkning för höga frekvenser, vilket alltså betyder att högfrekvent mätbrus förstärks. Det känns intuitivt eftersom högfrekvent mätbrus betyder väldigt kaotiska derivator.För att få en viss fasförbättring och en viss skärfrekvens bör alltså ekvationen lösas. Det finns oftast oändligt många lösningar och alltså stor valfrihet.För att dessutom uppfylla krav på stationärt fel (t.ex formulerat genom felkoefficienterna ) måste lågfrekvensförstärkningen vara tillräckligt hög. En I-del uppnår just detta. Vi gör detta genom att lägga till en fasretarderande länk, där ger den vanliga -delen. Förstärkningen blir vid . En fördel med är att länken då blir insignal-utsignalstabil. Medan påverkar storleken av förstärkningen så påverkar hur långt upp i frekvens som förstärkningen sträcker sig, och påverkar därmed hur snabb insvängningen mot stationärt reglerfel är.Ett för litet leder även till att fasmarginalen försämras: För PI-fallet fås Detta kan göras godtyckligt litet genom att öka .PI och PD kombineras till sist till Oavsett hur parametrarna till och väljs bör Bodediagram plottas för hela det öppna systemet Varken amplitudkurvan eller faskurvan ska vara alltför oregelbunden. När diagrammet för ser rimligt ut kan det kompletteras med diagram för och eller andra relevanta överföringsfunktioner.
Undersök om en P-regulator ger tillräcklig prestanda.
Om det inte går att få tillräcklig snabbhet och stabilitetsmarginal samtidigt, inför en fasavancerande kompensering, d.v.s PD-kompensering. Välj så att maximala fasavanceringen blir tillräckligt stor. Ta till extra marginal.
Välj så att fasavanceringen hamnar där den gör nytta (nära tilltänkta skärfrekvensen).
Välj så att skärfrekvensen blir den önskade. Om det stationära felet inte blir tillräckligt litet, inför en fasretarderande kompensering, d.v.s PI-kompensering. Välj så att det stationära felet blir tillräckligt litet.
Välj så att insvängingen mot stabilitet blir tillräckligt snabb utan att stabilitetsmarginalen blir för liten. Rita Bodediagram för , och . Kontrollera att alla krav som kan avläsas i frekvensplanet är uppfyllda.
Beräkna och plotta stegsvar, svar på störningar och andra relevanta svar i tidsplanet. Kontrollera att alla krav som kan avläsas i tidsplanet är uppfyllda.
]]></description><link>fysik/reglerteknik/5.4-kompensering-med-hjälp-av-bodediagram.html</link><guid isPermaLink="false">Fysik/Reglerteknik/5.4 - Kompensering med hjälp av Bodediagram.md</guid><pubDate>Tue, 09 Sep 2025 09:19:50 GMT</pubDate></item><item><title><![CDATA[3.6 - Specifikationer]]></title><description><![CDATA[Med notationerna införda i <a data-href="3.5 - Det återkopplade systemet" href="fysik/reglerteknik/3.5-det-återkopplade-systemet.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">3.5 - Det återkopplade systemet</a><a data-href="3.5 - Det återkopplade systemet" href="fysik/reglerteknik/3.5-det-återkopplade-systemet.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">3.5 - Det återkopplade systemet</a> kan vi beskriva en ideal följning av referenssignalen som att , och en perfekt undertryckning av störningar som att . Detta kan sällan uppnås, så vi undersöker här sätt att beskriva hur långt från idealt vårt system är.Vi börjar med vårt vanliga reglerfel , I specialfallet gäller och alltså Känslighetsfunktionen spelar alltså stor roll. Vi förutsätter att alla poler till det återkopplade systemet har negativ realdel, och får då att blir en insignal-utsignalstabil överföringsfunktion. Eftersom och har samma verkan sätter vi för enkelhetens skull .Om referenssignalen är ett steg med amplituden fås och slutvärdessatsen ger då Om ska gå mot när måste alltså . Om innehåller en ren integration, så uppnås detta. Vi antar att är på denna form och undersöker nu felet vid en ramp; tag , d.v.s . Då får vi Detta blir alltså mindre ju mindre är och försvinner om . Om innehåller två integrationer, d.v.s så uppnås detta. Vi kan då ta vilket ger Talen kallas felkoefficienter. De kan även ses som koefficienter i en serieutveckling av : eftersom de uppfyller ]]></description><link>fysik/reglerteknik/3.6-specifikationer.html</link><guid isPermaLink="false">Fysik/Reglerteknik/3.6 - Specifikationer.md</guid><pubDate>Mon, 08 Sep 2025 09:46:45 GMT</pubDate></item><item><title><![CDATA[3.5 - Det återkopplade systemet]]></title><description><![CDATA[Vi önskar nu moddelera ett återkopplat system mer allmänt. Vi har som vanligt referenssignalen , styrsignalen , utsignalen och störningen .Vi ansätter en generell linjär regulatorstruktur där och alltså översätter respektive till värden som är jämförbara och som vi alltså vill ha lika.Det är inte helt självklart hur störningen ska modelleras, men vi kan i allmänhet låta vara en störning av själva utsignalen , så att Vi får då att Vi inför där kallas det öppna systemet eller kretsförstärkningen, kallas känslighetsfunktionen och kallas för det slutna systemet.Känslighetsfunktionen beskriver alltså hur störningar påverkar systemet, och det slutna systemet beskriver hur referenssignalen påverkar systemet.Ibland bryr vi oss också om hur stor styrsignalen blir. Denna ges av I många system, som P, PI och PID, har vi , d.v.s att styrsignalen är en funktion av . Då får vi alltså att ]]></description><link>fysik/reglerteknik/3.5-det-återkopplade-systemet.html</link><guid isPermaLink="false">Fysik/Reglerteknik/3.5 - Det återkopplade systemet.md</guid><pubDate>Mon, 08 Sep 2025 09:22:48 GMT</pubDate></item><item><title><![CDATA[4.3 - Bodediagram]]></title><description><![CDATA[Ett Bodediagram av en <a data-tooltip-position="top" aria-label="4.2 - Frekvenssvar och frekvensfunktion" data-href="4.2 - Frekvenssvar och frekvensfunktion" href="fysik/reglerteknik/4.2-frekvenssvar-och-frekvensfunktion.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">frekvensfunktion</a><a data-tooltip-position="top" aria-label="4.2 - Frekvenssvar och frekvensfunktion" data-href="4.2 - Frekvenssvar och frekvensfunktion" href="fysik/reglerteknik/4.2-frekvenssvar-och-frekvensfunktion.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">frekvensfunktion</a> är ett diagram där man var för sig plottar och som funktioner av , med logaritmiska skalor för och . Kurvan som visar kallas amplitud- eller beloppskurva och den som visar kallas faskurva. Amplituden anges ofta i decibelskala (dB) som ges av Eftersom seriekoppling av system motsvarar multiplikation av frekvensfunktioner motsvarar detta helt enkelt addition av Bodediagram, eftersom logskala gör om multiplikation till addition, och argumentet likaså. Det följer på samma sätt att potenser av blir räta linjer. Detta ger oss information från asymptoterna i Bodediagrammen av rationella funktioner. Med får vi en lågfrekvensasymptot för små , samt en högfrekvensasymptot för stora , ]]></description><link>fysik/reglerteknik/4.3-bodediagram.html</link><guid isPermaLink="false">Fysik/Reglerteknik/4.3 - Bodediagram.md</guid><pubDate>Fri, 05 Sep 2025 13:25:39 GMT</pubDate></item><item><title><![CDATA[4.2 - Frekvenssvar och frekvensfunktion]]></title><description><![CDATA[Eftersom linjärkombinationer av trigonometriska funktioner kan approximera de flesta funktioner godtyckligt bra och vi ofta antar ett linjärt system av differentialekvationer till våra system så tittar vi på hur trigonometriska insignaler beter sig.Vi tar en godtycklig överföringsfunktion och låter insignalen vara och antar att denna signal legat på sedan . Vi får då att där . Vi kallar vinkelfrekvensen och för förstärkningen vid vinkelfrekvensen . Därmed beskirver entydigt systemets utsignal ("svar") för trigonometriska funktioner med frekvens som insignal. som funktion av kallas därför för frekvenssvaret eller frekvensfunktionen.Om vi låter ser vi att kan ses som en slags statisk förstärkning som gäller vid en konstant insignal.Om vi experimentellt bestämmer amplituden för olika , samt även uppskattar fasförskjutningen, så kan frekvensfunktionen alltså uppskattas. Detta kallas frekvensanalys.Denna härledning utgår som sagt från att insignalen varit trigonometrisk sedan . Om detta antagande tas bort får man en tilläggsterm som försvinner med tiden så länge systemet är stabilt.Om man direkt plottar kurvan i det komplexa talplanet kallas resultatet för ett Nyquistdiagram. Om man istället plottar mot kallas resultatet för ett Nicholsdiagram.]]></description><link>fysik/reglerteknik/4.2-frekvenssvar-och-frekvensfunktion.html</link><guid isPermaLink="false">Fysik/Reglerteknik/4.2 - Frekvenssvar och frekvensfunktion.md</guid><pubDate>Fri, 05 Sep 2025 13:13:27 GMT</pubDate></item><item><title><![CDATA[3.8 - Nyquistkriteriet]]></title><description><![CDATA[Ett annat sätt att undersöka stabiliteten är med Nyquistkriteriet, som bygger på ett sätt att hitta skillnaden mellan antalet nollställen och antalet poler i ett godtyckligt område, och använda detta i hela höger halvplan.För ett återkopplat system ges det slutna systemet samt känslighetsfunktionen av överföringsfunktionerna där är kretsförstärkningen, .Sats (Nyquistkriteriet)
Låt vara kurvan i det högra halvplanet som begränsas av den imaginära axeln, cirkeln med radie med och cirkeln med radie med . Låt sedan . Antalet poler i höger halvplan till ett återkopplat system är lika med antalet poler i höger halvplan hos , plus antalet varv som kurvan omsluter punkten .
Om inte har poler i höger halvplan så blir stabilitetskriteriet alltså att inte får omslutas av .
]]></description><link>fysik/reglerteknik/3.8-nyquistkriteriet.html</link><guid isPermaLink="false">Fysik/Reglerteknik/3.8 - Nyquistkriteriet.md</guid><pubDate>Mon, 01 Sep 2025 11:03:21 GMT</pubDate></item><item><title><![CDATA[3.7 - Rotort]]></title><description><![CDATA[Vi har ofta ett antal parametrar i vår insignal som ger olika poler och därmed olika beteenden hos systemet. Vi vill därför studera hur dessa nollställen beror på våra parametrar. Polynomet i nämnaren kan delas upp i där är en konstant vi varierar, till exempel en parameter i . En plott av polerna som funktion av kallas för en rotort. Polerna är alltså rötterna , Vi antar att högstagradskoefficienterna i och båda är positiva, och kan därför anta att båda dessa är (andra värden kan absorberas av ). Vi antar även att . Vi låter Vi kan WLOG anta att , eftersom vi annars kan låta och byta roller och ersätta med . Låt och vara rötterna till respektive . Vi vet att det finns poler för alla ; vi säger att rotorten har stycken grenar. Polerna är antingen reella eller komplexkonjugerade, alltså är rotorten symmetrisk m.a.p. den reella axeln. Vi skriver om ekvationen som Rötterna till motsvarar då och rötter till motsvarar . De grenarna startar alltså i för , och dessa punkter kallas därför startpunkter. Av dessa så kommer stycken att sluta i som därför kallas ändpunkter. grenar kommer istället att sluta i oändligheten; kurvorna som rotorten närmar sig när kallas asymptoter. Vi sammanfattar detta i följande sats.Sats
Om och är gradtalen för respektive så finns stycken startpunkter, stycken ändpunkter och stycken asymptoter.
Om vi delar upp och i reella polynom får vi där samtliga koefficienter är reella. Vi ser att för reella så är även denna kvot reell, och att kvoten ändrar tecken när passerar en start- eller ändpunkt eftersom en faktor eller då passerar . Kvoten är positiv för stora och ska vara negativ för att vara lika med , vilket ger oss följande sats.Sats
De delar av reella axeln som har ett udda antal reella start- och ändpunkter till höger tillhör rotorten.
Vi kan visa att för något polynom av gradtal . Binomialsatsen ger att vilket ger att för något polynom av gradtal . För stora så är den andra termen i högerledet ovan försumbar, och ekvationen för tillräckligt stora har därför lösningar som med godtyckligt god approximation även är poler.Med detta får vi genom att jämföra argumenten, Vid är , så för att ska hållas reellt bör alltså ligga längs strålar från som går i riktningarna för . Eftersom approximationen gäller bättre för större är dessa strålar vad rotorterna går mot, alltså är detta asymptoterna!Stabilitetsvillkoret är ju att polerna ska ha negativ realdel, så det är extra intressant att undersöka var rotorterna skär den imaginära axeln. Då antar ett rent imaginärt värde, . Skärningspunkterna kan alltså hittas genom att lösa för reella värden på och .]]></description><link>fysik/reglerteknik/3.7-rotort.html</link><guid isPermaLink="false">Fysik/Reglerteknik/3.7 - Rotort.md</guid><pubDate>Mon, 01 Sep 2025 10:26:12 GMT</pubDate></item><item><title><![CDATA[3.4 - Inverkan av återkoppling på olinjäriteter]]></title><description><![CDATA[Antag att en utsignal samt en insignal båda är mellan och . Säg att Tag även en referenssignal . Med en enkel återkoppling fås vilket kan göras godtyckligt litet. Detta är en observation om hur olinjäriteter inte påverkar stabilitet så länge återkoppling används.]]></description><link>fysik/reglerteknik/3.4-inverkan-av-återkoppling-på-olinjäriteter.html</link><guid isPermaLink="false">Fysik/Reglerteknik/3.4 - Inverkan av återkoppling på olinjäriteter.md</guid><pubDate>Mon, 01 Sep 2025 08:14:59 GMT</pubDate></item><item><title><![CDATA[3.2 - Reglering av nivå i vattentank]]></title><description><![CDATA[Nivån i en tank med tvärsnittsarea och utloppsarea ska regleras. Insignalen är inflödet och utsignalen är nivån . Utflödet ges av där är utloppshastigheten, som ges av enligt Bernoullis lag. Volymen ges alltså av och därmed är Insättning ger vilket är en olinjär differentialekvation. Vi vill åstadkomma ett jämviktsläge där alltså , samt där insignalen är konstant, , och kan därmed linjärisera ekvationen runt detta jämviktsläge. betyder att Taylorutveckling av runt ger Detta ger differentialekvationen Med , ges Vi inför även och får Vi betecknar nu istället inflödet med och låter insignalen vara ventilläget. Vi har alltså Vi antar att ventilen är linjär och därmed kan beskrivas av I praktiken beror flödet även på tryckfallet över ventilen (vad det nu betyder) så vi tar även hänsyn till störningar enligt Från differentialekvationen får vi kan sedan väljas till till exempel P, PI eller PID.]]></description><link>fysik/reglerteknik/3.2-reglering-av-nivå-i-vattentank.html</link><guid isPermaLink="false">Fysik/Reglerteknik/3.2 - Reglering av nivå i vattentank.md</guid><pubDate>Mon, 01 Sep 2025 08:03:31 GMT</pubDate></item><item><title><![CDATA[Ö1]]></title><description><![CDATA[Rules of thumb:
Final value theorem: .
Poles: more negative real part -&gt; faster decay. Complex poles with small damping () -&gt; oscillations.
Zeros: affect the initial behaviour, not stability. Zero near the origin -&gt; faster climb toward stationary state. Complex zeros -&gt; distorted oscillations. Zeroes close to poles -&gt; possible "cancellation" of pole.
2nd order: -&gt; overdamped, no oscillation. -&gt; critically damped, fast climb to stationary, no oscillation. -&gt; underdamped, oscillations. -&gt; lots of overshoot.
For we have where and . This means and . The damping is thus low. We get . We thus expect some swinging back and forth before settling. Also, since we expect . Thus C is a reasonable graph.For we expect to approach , which none of the graphs resemble.For we expect to approach . Also, zeroes indicate a faster initial climb towards the stationary state, which occurs in B.For , the addition of the pole means more damping and less oscillations, and since we expect to end up at , this fits with A.For , with similar reasoning as , it fits D.For , we have a pole in the RHP which means the system is unstable, and thus doesn't fit any of the graphs.]]></description><link>fysik/reglerteknik/ö1.html</link><guid isPermaLink="false">Fysik/Reglerteknik/Ö1.md</guid><pubDate>Thu, 28 Aug 2025 14:33:18 GMT</pubDate></item><item><title><![CDATA[2.6 - Samband mellan tidssvar och polplacering]]></title><description><![CDATA[Tag Det är ofta lättare att skriva om på formen där alltså och . Eftersom systemet är linjärt kan vi skala om så att . Vi vet att systemet har en pol i .Med stegsvaret till ett system menas utsignalen som fås med insignalen Till föregående system fås Vi ser att oavsett värdet på så är vid vid alltså har vid tiden nått av sitt slutvärde.Tag ]]></description><link>fysik/reglerteknik/2.6-samband-mellan-tidssvar-och-polplacering.html</link><guid isPermaLink="false">Fysik/Reglerteknik/2.6 - Samband mellan tidssvar och polplacering.md</guid><pubDate>Thu, 28 Aug 2025 11:37:19 GMT</pubDate></item><item><title><![CDATA[2.5 - Stabilitet]]></title><description><![CDATA[Ett system är insignal-utsignalstabilt om en begränsad insignal alltid ger en begränsad utsignal.Vi använder <a data-tooltip-position="top" aria-label="2.3 - Viktfunktionen > ^6f40f8" data-href="2.3 - Viktfunktionen#^6f40f8" href="fysik/reglerteknik/2.3-viktfunktionen.html#^6f40f8" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">viktfunktionsformeln</a><a data-tooltip-position="top" aria-label="2.3 - Viktfunktionen > ^6f40f8" data-href="2.3 - Viktfunktionen#^6f40f8" href="fysik/reglerteknik/2.3-viktfunktionen.html#^6f40f8" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">viktfunktionsformeln</a> och antar att är begränsad, alltså att det finns ett s.a. för alla . Då är Om vi väljer så att får vi , vilket visar följande sats.Sats
Ett system är insignal-utsignalstabilt om och endast om viktfunktionen uppfyller Detta kan ibland överföras till ett krav på polerna. Om nämnaren i har minst lika stort gradtal som täljaren så kallas proper, och om gradtalet är strikt större så kallas strikt proper. Om är strikt proper och polerna är enkla så kan partialbråksuppdelas i för några konstanter , vilket ger För system som endast är propra och eventuellt har olika multipla poler fås där varje är polynom. Detta visar följande sats.Sats
Ett system med en proper överföringsfunktion är insignal-utsignalstabilt om och endast om alla poler till har strikt negativa realdelar.
Med liknande resonemang kan följande sats visas.Slutvärdessatsen
Om har en rationell laplacetransform och alla nollskilda poler till har strikt negativ realdel så är ]]></description><link>fysik/reglerteknik/2.5-stabilitet.html</link><guid isPermaLink="false">Fysik/Reglerteknik/2.5 - Stabilitet.md</guid><pubDate>Thu, 28 Aug 2025 11:24:11 GMT</pubDate></item><item><title><![CDATA[2.3 - Viktfunktionen]]></title><description><![CDATA[Vi antar att utsignalen beror linjärt på samtliga tidigare insignaler, från tiden till . Detta kan beskrivas med en viktfunktion som säger hur mycket insignalen för tidsenheter sedan påverkar den nuvarande utsignalen. Mer specifikt, Om helt enkelt är en impuls vid , d.v.s , får vi därmed , och kallas därför även för impulssvaret.Eftersom vi har ekvationen och detta ger precis där , är viktfunktionen alltså precis laplaceinversen av överföringsfunktionen. Detta visar även den så kallade superpositionsprincipen, som säger att beror linjärt på , d.v.s om så är där är utsignalen till insignalen .]]></description><link>fysik/reglerteknik/2.3-viktfunktionen.html</link><guid isPermaLink="false">Fysik/Reglerteknik/2.3 - Viktfunktionen.md</guid><pubDate>Thu, 28 Aug 2025 11:00:27 GMT</pubDate></item><item><title><![CDATA[2.1-2.2 - Differentialekvationer]]></title><description><![CDATA[De flesta system kan beskrivas, åtminstone approximativt, av en linjär differentialekvation Vi gör antagandena att Då blir laplacetransformationen av helt enkelt , och vi får då (med ) ekvationen Vi kallar för överföringsfunktionen. Rötterna till täljaren kallas nollställen medan rötterna till nämnaren kallas poler. Båda dessa ger information om systemets beteende.]]></description><link>fysik/reglerteknik/2.1-2.2-differentialekvationer.html</link><guid isPermaLink="false">Fysik/Reglerteknik/2.1-2.2 - Differentialekvationer.md</guid><pubDate>Thu, 28 Aug 2025 09:06:44 GMT</pubDate></item><item><title><![CDATA[F1 - Transfer function, poles, zeroes, stability]]></title><description><![CDATA[Laplace is the transfer function between and and describes the relationship between , and .The system's poles are given by the roots of the denominator of .From the example:
Characteristic equation: Roots: .
Solution: if .
When , .]]></description><link>fysik/reglerteknik/f1-transfer-function,-poles,-zeroes,-stability.html</link><guid isPermaLink="false">Fysik/Reglerteknik/F1 - Transfer function, poles, zeroes, stability.md</guid><pubDate>Wed, 27 Aug 2025 07:17:35 GMT</pubDate></item><item><title><![CDATA[Ordlista]]></title><link>fysik/reglerteknik/ordlista.html</link><guid isPermaLink="false">Fysik/Reglerteknik/Ordlista.md</guid><pubDate>Wed, 27 Aug 2025 06:27:53 GMT</pubDate></item><item><title><![CDATA[1.5-1.6 - Principer för återkoppling]]></title><description><![CDATA[Reglerfelet är den ändring vi vill ha i utsignalen , d.v.s Det är vanligt att bestämma styrsignaler baserat på endast reglerfelet.Vid proportionell reglering eller P-reglering är styrsignalen i linjärt förhållande till reglerfelet, d.v.s Vid proportionell och integrerande reglering eller PI-reglering är styrsignalen i linjärt förhållande till reglerfelet samt tidsintegralen av reglerfelet, d.v.s Vid proportionell, integrerande och deriverande reglering eller PID-reglering är styrsignalen i linjärt förhållande till reglerfelet, tidsintegralen av reglerfelet, samt tidsderivatan av reglerfelet, d.v.s (ingen konstant?)
Vid brus kan det vara farligt att bero på derivatan eftersom den beter sig rätt kaotiskt.]]></description><link>fysik/reglerteknik/1.5-1.6-principer-för-återkoppling.html</link><guid isPermaLink="false">Fysik/Reglerteknik/1.5-1.6 - Principer för återkoppling.md</guid><pubDate>Tue, 26 Aug 2025 12:26:12 GMT</pubDate></item><item><title><![CDATA[1.4 - Reglerprinciper]]></title><description><![CDATA[En regulator är en mekanism som bestämmer och ställer ut en styrsignal till systemet. Det är alltså både den teori som används för att beräkna styrsignalen, och det som fysiskt skickar ut styrsignalen.Öppen styrning eller framkoppling från referenssignal är en princip för att lösa ett servoproblem. Då bestämmer man teoretiskt hur utsignalen bör bero av insignalen , och räknar ut vad bör vara för att ge önskad utsignal, d.v.s . En regulator tar då in börvärdet och skickar ut styrsignalen . Nackdelen med denna princip är att man varken tar hänsyn till störsignaler eller tillåter approximationer; man måste veta exakt hur systemet fungerar.Framkoppling från störsignal är en annan princip som bygger på att störningarna är mätbara och man vet hur de påverkar . Då tas även dessa med i beräkningen av styrsignalen så att . Detta kallas ibland endast framkoppling. Nackdelen är att man fortfarande måste veta exakt hur beror av och , och att alla störningar måste vara mätbara.Återkoppling är en tredje princip, där utsignalen mäts och används för att bestämma styrsignalen , så att . Fördelen med detta är att man inte behöver ha en perfekt förståelse av hur utsignalen beror av styrsignalen, och att störningar automatiskt kan tas hänsyn till utan att störningarna i sig behöver vara direkt mätbara. Ett system som är instabilt kan stabiliseras med återkoppling, men en nackdel är att olämplig återkoppling kan leda till instabilitet.]]></description><link>fysik/reglerteknik/1.4-reglerprinciper.html</link><guid isPermaLink="false">Fysik/Reglerteknik/1.4 - Reglerprinciper.md</guid><pubDate>Mon, 25 Aug 2025 12:24:43 GMT</pubDate></item><item><title><![CDATA[1.1-1.3 - Introduktion]]></title><description><![CDATA[Ett system är något godtyckligt man har anledning att studera och påverka.En styrsignal är en storhet som har en påverkan på ett system. Reglerteknik ger metoder att kontrollera styrsignaler för att ge önskat uppträdande.Reglerteknik används alltså för att påverka ett system eller en process för att få ett önskat uppträdande. Utsignaler eller mätsignaler ger information om hur systemet beter sig vid tillfället, och betecknas ofta med . Påverkan av ett system sker genom insignaler eller styrsignaler och betecknas ofta med . Systemen påverkas även av signaler utifrån som vi inte kan påverka; dessa kallas för störsignaler och betecknas ofta med .Syftet är ofta att få utsignalerna att bete sig på önskat sätt, alltså följa en viss signal som kallas referenssignalen eller börvärdet, och betecknas ofta med . Ett sådant problem kallas för ett servoproblem.Ett servoproblem där målet är att hålla utsignalen konstant. Detta kallas för ett regulatorproblem.Ett system är dynamiskt om utsignalen inte bara beror på den nuvarande insignalen, utan även på de föregående. Dynamiska system kan vara instabila, där utsignalen växer okontrollerat trots att insignalen är kontrollerad. Ett system är annars stabilt.]]></description><link>fysik/reglerteknik/1.1-1.3-introduktion.html</link><guid isPermaLink="false">Fysik/Reglerteknik/1.1-1.3 - Introduktion.md</guid><pubDate>Mon, 25 Aug 2025 12:24:39 GMT</pubDate></item><item><title><![CDATA[F13]]></title><description><![CDATA[Theorem
A field has an algebraic closure . Also, for any two algebraic closures ,, there is a -isomorphism , where .
Lemma
Let be a field. Then there is an algebraic field extension such that Proof:
Let and let be a tuple of infinitely many variables. Then, is a ring, namely the polynomial ring in all the variables . Then let Claim 1: . By this claim, there is some maximal ideal containing , making a field where is modded out. The homomorphism is injective by Corollary, meaning can be viewed as an extension field of . Now, let , and let . meaning Left to show is that is algebraic over . For all , is algebraic over since . This means , where , is algebraic over .
Claim 2: .
Proof: We have Proof of Claim 1: Suppose for contradiction that . Then for some and . Applying Kronecker's construction to yields a field extension such that Consider the substitution homomorphism Then which is a contradiction.
Proof of <a data-tooltip-position="top" aria-label="^934f16" data-href="#^934f16" href="#^934f16" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Theorem</a><a data-tooltip-position="top" aria-label="^934f16" data-href="#^934f16" href="#^934f16" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Theorem</a> a)
Applying <a data-tooltip-position="top" aria-label="^69a6e0" data-href="#^69a6e0" href="#^69a6e0" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Lemma</a><a data-tooltip-position="top" aria-label="^69a6e0" data-href="#^69a6e0" href="#^69a6e0" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Lemma</a> iteratively ields a chain of fields such that for every , is algebraic over and for every there is a such that . Then is a field. We need to show that 1) is algebraic over , and that 2) is algebraically closed.
1) We prove by induction on that is algebraic over . If or this is clear. If , is algebraic over , and is algebraic over by the induction hypothesis, meaning is algebraic over .
2) Let . Since has finitely many nonzero coefficients, there is some index such that . This means has a root in the next field, i.e. there is an such that .
Lemma
Let be a field and be a simple algebraic field extension with minimal polynomial . Also, let be a field homomorphism.
a) If is a field homomorphism with , then b) For every root with , there is a unique field homomorphism with and .
Hence Proof:
a) b) Uniqueness: . ]]></description><link>matematik/groups-and-rings/lecture-notes/f13.html</link><guid isPermaLink="false">Matematik/Groups and rings/Lecture notes/F13.md</guid><pubDate>Tue, 13 May 2025 14:46:47 GMT</pubDate></item><item><title><![CDATA[Finite Fields]]></title><description><![CDATA[Theorem
Let be a finite field. Then
a) . contains as its prime subfield.
b) , where .
c) is a splitting field of over . In fact, its elements are precisely the different zeroes of .
Proof:
a) Since is finite, so is its prime subfield . for some prime , and .
b) Since is finite, . Since is a vector space over with dimension , it has elements.
c) The unit group is of order , and by Fermat, for every , meaning for every . Zero also clearly satisfies this. Now let . Since cannot have more than roots, the elements of are in fact all roots of , and the roots must be pairwise distinct.
In practice, we construct by finding an irreducible of degree , and let Theoerm: Construction of finite fields
Let , prime, and .
a) There is a field with .
b) is unique up to -isomorphisms.
Definition
Let be a field.
a) is a multiple zero of b) The derivation map on is Notice that is a linear map but not a ring homomorphism.Theorem
Let be a field, and , and such that . Then is a multiple root of if and only if .
Proof:
If , then . Then, for some maximal , for some such that . Then If is a multiple root, then , meaning is a root of . If is not a multiple root, then and so .
Proof of <a data-tooltip-position="top" aria-label="^80585f" data-href="#^80585f" href="#^80585f" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Theorem</a><a data-tooltip-position="top" aria-label="^80585f" data-href="#^80585f" href="#^80585f" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Theorem</a>
a) Let be an algebraic closure of , and . Taking derivatives on yields . Hence has no multiple roots in . Hence has distinct roots in . Hence define Now check that is indeed a field: let , i.e. and . Then
1. The last equality is true because if is even then meaning is the same as .
2. .
b) Due to uniqueness of splitting fields.
]]></description><link>matematik/groups-and-rings/lecture-notes/finite-fields.html</link><guid isPermaLink="false">Matematik/Groups and rings/Lecture notes/Finite Fields.md</guid><pubDate>Mon, 12 May 2025 14:00:09 GMT</pubDate></item><item><title><![CDATA[Field Extensions]]></title><description><![CDATA[Theorem
Let be an integral domain.
a) There is a unique ring homomorphism .
b) where is either or prime.
Proof:
a) We know and , from which we deduce all since b) We know , hence is an integral domain meaning is a prime ideal.
Definition: Characteristic
Let be an integral domain and be the unique ring homomorphism from to . Then the characteristic is the unique number such that , denoted .
Definition: Subfield
A subring of a field is called a subfield if is a field.
Theorem
Let be a field.
a) For every subfield of , we have .
b) is a field called the prime subfield of , and is the unique smallest subfield of .
c) where is prime .
d) .
Proof:
a) We know . Thus .
c)+d) : Follows from a).
c) : We have , meaning , but by the minimality of we have .
d) : We now have , meaning . Recall that the field of fractions is the smallest field containing . Hence Like in c), we conclude .
Definition: Extension field, field extension, intermediate field, degree
Let be a field with a subfield .
a) is called an extension field of . The pair is called a field extension. Notation: (not to be confused with factor rings).
b) An intermediate field is a subfield of such that .
c) The multiplication on restricts to , making a vector space over . Its vector space dimension is called the degree of over , and is denoted by d) The field extension is called finite or infinite if is finite or infinite, respectively.
Theorem
Let be field extensions. Then Proof:
Case 1: and .
Let be a -vector space basis of , and let be an -vector space basis of . Then can be easily shown to be a -vector space basis of . Case 2: or Let such that and . Then there are linearly independent and over and , respectively. Then, by the same reasoning as in Case 1, is a linearly independent set. This means which we can make as large as we want.
Corollary
Let be field extensions. If is prime, then or .
Definition: Algebraic element
Let be a field extension.
a) An element is algebraic over if it satisfies a nontrivial algebraic equation (same thing as polynomial equation) over , i.e., for some . Otherwise, is transcendental over .
b) The extension field is algebraic over if every element in is algebraic over , and we call an algebraic field extension.
Theorem
Let be a field extension and . Then is transcendental over if and only if the substitution homomorphism is injective.
Proof:
Clear from the definition since it's injective kernel is trivial not algebraic.
Theorem
Let be a field extension and be algebraic over .
a) There is a unique monic polynomial of minimal degree such that , called the minimal polynomial of over .
b) .
c) is irreducible.
d) is an extension field of .
e) .
Proof:
a)+b) We know that is a PID, meaning for some . In particular, , and since is algebraic we know that the kernel is nontrivial so . Since is monic and is of smallest degree in , we know .
c) We know is an integral domain since it lies in . It is also isomorphic to meaning is a prime ideal. Since , is irreducible.
d) Since is a PID, being prime and being maximal is the same thing, so is a maximal ideal, meaning is a field.
e) Let . For all there are unique such that where . We get a homomorphism of -vector spaces This is in fact an isomorphism since its inverse is , i.e., the canonical projection. Hence where the last equality holds because is a basis.
Remark
It follows quite easily that since which can be made arbitrarily large since .
Theorem
Let be a field extension.
a) For every , is a field called the subfield of generated by over , and is the smallest field in containing both and . Note that .
b) If , we write Let be the substitution homomorphism and let . Then Proof of b):
Note that . Then and by the minimality of , we have equality.
c) The field extension is called finitely generated if for some . It is called simple if for some . The degree of over is .
d) For , Proof of d):
: : Exercise.
]]></description><link>matematik/groups-and-rings/lecture-notes/field-extensions.html</link><guid isPermaLink="false">Matematik/Groups and rings/Lecture notes/Field Extensions.md</guid><pubDate>Tue, 29 Apr 2025 11:02:27 GMT</pubDate></item><item><title><![CDATA[Centers and centralizers]]></title><description><![CDATA[<a href=".?query=tag:matematik-groups" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-groups">#matematik-groups</a> Definition: Centralizer, center
Let be a group and be any subset. The centralizer of is and the center of is Remark
Every centralizer is a subgroup since if then . Also, is the kernel of where , since if and only if for every , if and only if for every . Finally, since it is a kernel, and .
Theorem
A group is abelian if and only if is cyclic.
Proof:
If is abelian, so is the trivial group and therefore cyclic.
Now assume is cyclic, so for some . Take any . We get and , meaning and for some , which in turn means Definition: Representative, system of representatives
For any orbit , an element is called a representative of .
A system of representatives for a family of orbits is a family of representatives where is a representative of for every .
Theorem: The Class Equation
Consider the conjugation action of a finite group and let be a system of representatives of the orbits contained in (where '' denotes setminus). Then Proof:<br>
The point of the theorem is that can be partitioned into its conjugacy classes, which are the equivalence classes with conjugacy as equivalence relation. We know each element in has only itself as conjugacy class, since every conjugation does nothing. Hence instead of writing out for every element in we combine them into just . So left to sum is the sizes of all conjugacy classes larger than one. We see that since our group action is conjugation, the conjugacy classes are precisely the orbits. Also, we see that is just the <a data-tooltip-position="top" aria-label="Matematik/Groups and rings/Lecture notes/Group actions > ^64c44e" data-href="Matematik/Groups and rings/Lecture notes/Group actions#^64c44e" href="matematik/groups-and-rings/lecture-notes/group-actions.html#^64c44e" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Stabilizer</a><a data-tooltip-position="top" aria-label="Matematik/Groups and rings/Lecture notes/Group actions > ^64c44e" data-href="Matematik/Groups and rings/Lecture notes/Group actions#^64c44e" href="matematik/groups-and-rings/lecture-notes/group-actions.html#^64c44e" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Stabilizer</a> of , meaning is indeed the size of the orbit of , by the <a data-tooltip-position="top" aria-label="Matematik/Groups and rings/Lecture notes/Group actions > ^30b4c5" data-href="Matematik/Groups and rings/Lecture notes/Group actions#^30b4c5" href="matematik/groups-and-rings/lecture-notes/group-actions.html#^30b4c5" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Orbit stabilizer theorem</a><a data-tooltip-position="top" aria-label="Matematik/Groups and rings/Lecture notes/Group actions > ^30b4c5" data-href="Matematik/Groups and rings/Lecture notes/Group actions#^30b4c5" href="matematik/groups-and-rings/lecture-notes/group-actions.html#^30b4c5" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Orbit stabilizer theorem</a>.
Corollary
If where is prime, then is abelian.
Proof:
We know is a normal subgroup, meaning its order divides , meaning it's either 1, , or .
If , then , meaning is abelian.<br>
If , then , meaning is cyclic by <a data-tooltip-position="top" aria-label="Matematik/Groups and rings/Lecture notes/Cyclic groups > ^a05e78" data-href="Matematik/Groups and rings/Lecture notes/Cyclic groups#^a05e78" href="matematik/groups-and-rings/lecture-notes/cyclic-groups.html#^a05e78" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Theorem</a><a data-tooltip-position="top" aria-label="Matematik/Groups and rings/Lecture notes/Cyclic groups > ^a05e78" data-href="Matematik/Groups and rings/Lecture notes/Cyclic groups#^a05e78" href="matematik/groups-and-rings/lecture-notes/cyclic-groups.html#^a05e78" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Theorem</a>, meaning is abelian. Note that since is abelian, , so this case doesn't even happen.<br>
If , the <a data-tooltip-position="top" aria-label="^1a014f" data-href="#^1a014f" href="#^1a014f" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Class equation</a><a data-tooltip-position="top" aria-label="^1a014f" data-href="#^1a014f" href="#^1a014f" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Class equation</a> gives that where we know each is either , or , and since they are all greater than (since only the orbits of elements of have size ), it's either or . This means the right hand side is divisible by , but clearly the left hand side isn't, meaning .
]]></description><link>matematik/groups-and-rings/lecture-notes/centers-and-centralizers.html</link><guid isPermaLink="false">Matematik/Groups and rings/Lecture notes/Centers and centralizers.md</guid><pubDate>Wed, 16 Apr 2025 11:51:12 GMT</pubDate></item><item><title><![CDATA[Sylow Groups]]></title><description><![CDATA[<a href=".?query=tag:matematik-groups" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-groups">#matematik-groups</a> Definition: Conjugate
Let be a group. is conjugate to if there exists a such that . is conjugate to if there exists a such that .
Definition: -group, -Sylow subgroup
Let be a finite group and be prime. is a -group if for some . is called a -Sylow subgroup if for some and where .
In other words, given a group whose order is the prime factorization , a
-Sylow subgroup is a subgroup with order .Lemma
Let be a group with for some prime and some , and let be the number of subgroups in of order . Then In particular, is dependent only on the order of .
Proof:<br>
Let . Then acts on via . With respect to this group action, let be a system of representatives of the -orbits. Then by the <a data-tooltip-position="top" aria-label="Matematik/Groups and rings/Lecture notes/Group actions > ^30b4c5" data-href="Matematik/Groups and rings/Lecture notes/Group actions#^30b4c5" href="matematik/groups-and-rings/lecture-notes/group-actions.html#^30b4c5" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Orbit stabilizer theorem</a><a data-tooltip-position="top" aria-label="Matematik/Groups and rings/Lecture notes/Group actions > ^30b4c5" data-href="Matematik/Groups and rings/Lecture notes/Group actions#^30b4c5" href="matematik/groups-and-rings/lecture-notes/group-actions.html#^30b4c5" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Orbit stabilizer theorem</a> and <a data-tooltip-position="top" aria-label="Matematik/Groups and rings/Lecture notes/Group actions > ^d689cf" data-href="Matematik/Groups and rings/Lecture notes/Group actions#^d689cf" href="matematik/groups-and-rings/lecture-notes/group-actions.html#^d689cf" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Orbit equation</a><a data-tooltip-position="top" aria-label="Matematik/Groups and rings/Lecture notes/Group actions > ^d689cf" data-href="Matematik/Groups and rings/Lecture notes/Group actions#^d689cf" href="matematik/groups-and-rings/lecture-notes/group-actions.html#^d689cf" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Orbit equation</a>.
Claim 1
For every , is a -group.
Proof: We have , meaning acts on via . Since this action is injective, we know each orbit has size . This means we get meaning so for some . From this claim, we now get that meaning Claim 2
Proof: Each corresponds to a unique such that as -orbits in , since all 's are a system of representatives for the orbits. This defines a map Claim a) Proof: Let . Then, like before, there is an such that , as orbits in . From before, we have We also know so we know meaning , so . Claim b) Proof: Let , meaning . From before, we have that This means must be the only orbit, meaning we can choose a and get that . This means where the first equality holds because . Then, since , we can simply let making a subgroup of that satisfies , meaning , so . Claim c) is injective
Proof: Let such that . Then we have , meaning there is a such that , meaning is a group, so meaning . Hence is a bijection between and . By definition, , so from earlier we have Theorem
Let be a group with for some prime and some , and let be the number of subgroups in of order . Then In particular, .
Proof:<br>
We know the cyclic group has a unique subgroup of order , meaning , meaning , and by <a data-tooltip-position="top" aria-label="^6612b3" data-href="#^6612b3" href="#^6612b3" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Lemma</a><a data-tooltip-position="top" aria-label="^6612b3" data-href="#^6612b3" href="#^6612b3" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Lemma</a>, we thus know that the latter is true for too. Lemma
Let be a group, be a -subgroup and be a -Sylow subgroup. Then there exists a such that .
Proof:
We let act on via . We then get that which is clearly not divisble by . We also know that every orbit has a representative for some , giving which we know divides for some , so for some , and since the sum of all sizes of orbits are not divisible by , some must be , so that for some , meaning that for every , Theorem: Sylow Theorems
Let be a finite group and be prime. contains at least one -Sylow subgroup. More precisely, for any -subgroup there is a -Sylow subgroup such that .
Let be a -Sylow subgroup and . Then is a -Sylow subgroup if and only if is conjugate to .
Let be the number of -Sylow subgroups of . Then and . Proof:<br>
4. {a} First part follows from <a data-tooltip-position="top" aria-label="^c4ecd8" data-href="#^c4ecd8" href="#^c4ecd8" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Theorem</a><a data-tooltip-position="top" aria-label="^c4ecd8" data-href="#^c4ecd8" href="#^c4ecd8" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Theorem</a>, second from the first part and <a data-tooltip-position="top" aria-label="^978877" data-href="#^978877" href="#^978877" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Lemma</a><a data-tooltip-position="top" aria-label="^978877" data-href="#^978877" href="#^978877" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Lemma</a>.
5. : Clear since conjugation is an automorphism.<br>
: By <a data-tooltip-position="top" aria-label="^e42d18" data-href="#^e42d18" href="#^e42d18" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Lemma</a><a data-tooltip-position="top" aria-label="^e42d18" data-href="#^e42d18" href="#^e42d18" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Lemma</a> there exists a such that meaning since they are of the same size.<br>
6. Let . Then by b) we know acts transitively on , meaning only one orbit exists. Hence for every , , so From <a data-tooltip-position="top" aria-label="^c4ecd8" data-href="#^c4ecd8" href="#^c4ecd8" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Theorem</a><a data-tooltip-position="top" aria-label="^c4ecd8" data-href="#^c4ecd8" href="#^c4ecd8" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Theorem</a> we also know . Lemma
Let be a group and such that . Then For every and , . is a group homomorphism. is injective. Proof:
4. {a} We have since is normal, and likewise, since , too, is normal. Since , we thus know .
5. We have by (a).
6. We have Corollary
Let be a finite group and be prime. If , then there exists a such that . is a -group for every there exists a such that .
Let . Then is a -Sylow subgroup of if and only if is a maximal -group in .
Let be a -Sylow subgroup. Then . Proof:<br>
5. {a} We know by <a data-tooltip-position="top" aria-label="^e42d18" data-href="#^e42d18" href="#^e42d18" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Theorem</a><a data-tooltip-position="top" aria-label="^e42d18" data-href="#^e42d18" href="#^e42d18" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Theorem</a> that there is a -Sylow subgroup of some order for some positive . This means there is an where , and we know , so for some positive . We then get that has order since .
6. Let . We have since .
: Let be a prime such that . Then by (a) there is a such that , which by our assumption means .
7. : We know and for some and where . Then we know any -subgroup has order where since its order must divide , meaning it is at most as large as .
: Since is a -group in , there is a -Sylow subgroup containing , and by the maximality of , must be that -Sylow subgroup.<br>
8. By <a data-tooltip-position="top" aria-label="^e42d18" data-href="#^e42d18" href="#^e42d18" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Theorem</a><a data-tooltip-position="top" aria-label="^e42d18" data-href="#^e42d18" href="#^e42d18" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Theorem</a> we know all -Sylow subgroups are conjugate, meaning if and only if there is only one -Sylow subgroup, it is invariant under conjugation, meaning it's normal. ]]></description><link>matematik/groups-and-rings/lecture-notes/sylow-groups.html</link><guid isPermaLink="false">Matematik/Groups and rings/Lecture notes/Sylow Groups.md</guid><pubDate>Sat, 12 Apr 2025 16:06:51 GMT</pubDate></item><item><title><![CDATA[Cyclic groups]]></title><description><![CDATA[<a href=".?query=tag:matematik-groups" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-groups">#matematik-groups</a> Definition: Cyclic group
Let be a group and . Then is the smallest subgroup of containing , called the subgroup generated by .
If for some , we write and call it the cyclic group generated by .
Theorem
Let be a group and . Then Proof:
Let be the right hand expression above. By the construction of , we know . We also know that , so . For any such that it is clear that since each element of is in and thus all compositions of elements of . Thus , so .
Remark
Cyclic groups are abelian since .
Example is called the free cyclic group. For any we also have and If we get which is called the cyclic group of order .
We call them the free cyclic group and cyclic groups of order because we can show that every cyclic group is isomorphic to one of these. For that we first need a few lemmas.Lemma
If is a group, is cyclic if and only if there exists a surjective .
Proof:
: We have .
: Let , and we get , which combined with the fact that is surjective means that .
Lemma
If , then for some .
Proof:
If , then . Otherwise, let (which exists since if then ). Then we clearly have . Finally, let . Then we have for some , which means , implying that by minimality, so .
Theorem
Let be a cyclic group. Then If , .
If , . Proof:
Since is syclic there exists a surjective . The universal property gives a bijection . We know for some , so where in the first case, , and in the second, .
Theorem
Let be a cyclic group and . Then is cyclic.
Proof:<br>
By <a data-tooltip-position="top" aria-label="^481423" data-href="#^481423" href="#^481423" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Lemma</a><a data-tooltip-position="top" aria-label="^481423" data-href="#^481423" href="#^481423" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Lemma</a>, there exists a surjective homomorphism , so . By <a data-tooltip-position="top" aria-label="^7b5424" data-href="#^7b5424" href="#^7b5424" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Lemma</a><a data-tooltip-position="top" aria-label="^7b5424" data-href="#^7b5424" href="#^7b5424" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Lemma</a>, there exists an such that . If , then , and otherwise, which by <a data-tooltip-position="top" aria-label="^481423" data-href="#^481423" href="#^481423" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Lemma</a><a data-tooltip-position="top" aria-label="^481423" data-href="#^481423" href="#^481423" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Lemma</a> means that is cyclic.
Definition: Order of element
Let be a group and . The order of is This means that if , and .Theorem: Fermat's Little Theorem
Let be a finite group and . Then Proof:<br>
The divisibility follows from <a data-tooltip-position="top" aria-label="Matematik/Groups and rings/Lecture notes/Cosets > ^920282" data-href="Matematik/Groups and rings/Lecture notes/Cosets#^920282" href="matematik/groups-and-rings/lecture-notes/cosets.html#^920282" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Lagrange's Theorem</a><a data-tooltip-position="top" aria-label="Matematik/Groups and rings/Lecture notes/Cosets > ^920282" data-href="Matematik/Groups and rings/Lecture notes/Cosets#^920282" href="matematik/groups-and-rings/lecture-notes/cosets.html#^920282" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Lagrange's Theorem</a>, and the second part follows from the fact that .
Corollary: Fermat's Little Theorem, classic version
Let be prime and . Then Proof:
Let (recall that '' denotes exclusion of zero). We first show that is a group with respect to multiplication. Multiplication is well defined as since divides neither nor , so does not divide either since is prime, meaning , so . Furthermore, this multiplication is injective since if , then , so meaning since . This means , so . Since is clearly a unit element, it follows that inverse elements exist, hence is a group under multiplication.
Now, if , then . Otherwise, we have , which by the group theoretic version of Fermat's Little Theorem means that Corollary
Let be a finite group with order , where is prime. Then ,
For all , , and . Proof:
b) Since , we know since is prime and . Hence , so .<br>
a) From <a data-tooltip-position="top" aria-label="^124bb5" data-href="#^124bb5" href="#^124bb5" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Theorem</a><a data-tooltip-position="top" aria-label="^124bb5" data-href="#^124bb5" href="#^124bb5" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Theorem</a>, we know .
]]></description><link>matematik/groups-and-rings/lecture-notes/cyclic-groups.html</link><guid isPermaLink="false">Matematik/Groups and rings/Lecture notes/Cyclic groups.md</guid><pubDate>Fri, 11 Apr 2025 15:55:42 GMT</pubDate></item><item><title><![CDATA[Cosets]]></title><description><![CDATA[<a href=".?query=tag:matematik-groups" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-groups">#matematik-groups</a> <br>Definition: Coset
Let be a group and be a <a data-tooltip-position="top" aria-label="Matematik/Groups and rings/Lecture notes/Subgroups" data-href="Matematik/Groups and rings/Lecture notes/Subgroups" href="matematik/groups-and-rings/lecture-notes/subgroups.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Subgroup</a><a data-tooltip-position="top" aria-label="Matematik/Groups and rings/Lecture notes/Subgroups" data-href="Matematik/Groups and rings/Lecture notes/Subgroups" href="matematik/groups-and-rings/lecture-notes/subgroups.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Subgroup</a>. A left coset of in is a subset of the form where . We define as the set of all left cosets of in .
Right cosets are defined analogously, so and .
The elements of a coset of in are called the coset's representatives.
We quickly see that it doesn't really matter if we are using left or right cosets since they essentially contain the same information. Consider the bijective map . It restricts to a map meaning it defines a bijection .Theorem
Let be a group and . The following are equivalent: ,
,
,
. Proof:
(i)(ii): Clear since .
(ii)(iii): Let . Then for some . This gives .
(iii)(iv): We have for some , so .
(iv)(i): For every we have and for every we have .
Corollary
For any group and subgroup , is the disjoint union of all left cosets of in .
Definition: Order, index
Let be a group and be a subgroup. The index of in is and the order of is Theorem
Let be a group, be a subgroup and . Then there is a bijection; in particular, .
Proof:
Translation by is bijective, hence the result.
Corollary: Theorem of Lagrange
If is a finite group and is a subgroup, then ]]></description><link>matematik/groups-and-rings/lecture-notes/cosets.html</link><guid isPermaLink="false">Matematik/Groups and rings/Lecture notes/Cosets.md</guid><pubDate>Fri, 11 Apr 2025 13:02:49 GMT</pubDate></item><item><title><![CDATA[Modules]]></title><description><![CDATA[Modules are "vector spaces over rings".Definition: Module
Let be a ring. A left -module is an Abelian group equipped with a map such that for all and , Equivalently, a left -module is a ring homomorphism where is an abelian group.In the same way one can define right -modules, with scalar multiplication from the right. If is commutative, the two definitions are basically the same.Examples
If is a field, then -modules are the same as -vector spaces.
If , then -modules are the same as Abelian groups since multiplication by an integer can be deduced directly from the definition: multiplication by is multiplication by , meaning it's the same as adding times.
If , then -modules are the same as -vector spaces equipped with a linear operator , where for .
If is any ring, then is a left -module. If is an ideal in , then is a left -module.
Definition: Module homomorphism
A module homomorphism is a map such that for all and , ]]></description><link>matematik/groups-and-rings/lecture-notes/modules.html</link><guid isPermaLink="false">Matematik/Groups and rings/Lecture notes/Modules.md</guid><pubDate>Tue, 01 Apr 2025 14:37:30 GMT</pubDate></item><item><title><![CDATA[Unique Factorization Domains]]></title><description><![CDATA[Definition: Unique Factorization Domain
An integral domain is called a factorial ring or unique factorization domain (UFD) if every is a finite product of prime elements.
A unit would basically be a product of zero prime elements. Also, notice that we cannot express a unit as a product of non-unit elements (at least in commutative rings) since if , then for some , so and likewise for each , so all are units.Theorem
Let be a factorial ring and let . Consider prime factorizations . Then and one can renumber the 's such that is associated to for every .
Proof:
We induct on . Suppose , so that . Since is an integral domain, we know that is irreducible. Then, if we have implying or is a unit, but this cannot be by our previous observation that units are never products of non-units, so and thus .
Now suppose , so that . This means , so since is prime there is some such that . Renumber the 's so that . This means for some , meaning Since is irreducible, must be a unit, meaning is prime, so we can apply the induction hypothesis, completing the proof.
Theorem
Let be a factorial ring and let . Then Proof:
We already know since is an integral domain. We now show . Let be irreducible. We know has some prime factorization , but if then which cannot be since then either or is not prime. Hence so is prime.
Definition: Noetherian ring
A commutative ring is called Noetherian if every ascending chain of ideals becomes stationary, i.e., Lemma
Every PID is Noetherian.
Proof:
Let be a PID. Take any ascending chain of ideals . Let . Since is a PID, for some , meaning for some , meaning Theorem: PID UFD
Every principal ideal domain is a unique factorization domain.
Proof:
Let be a PID. We know that is Noetherian. Let We want to show that , so assume for contradiction that .
Claim 1 contains a maximal element , i.e. such that .
Proof:
Suppose is not maximal. Then there is some such that . We can keep doing this and we know that the process must eventually stop since is Noetherian, meaning there is some which is maximal. Let be maximal. Then must be reducible, since otherwise is prime. Thus there are such that . We must have and (not equalities since then ). By the maximality of , and thus have prime factorizations, meaning has a prime factorization which is a contradiction.
Theorem (Gauß)
If is a unique factorization domain, then so is .
This is a nontrivial theorem which will take some more theory before we can prove.Corollary
If is a unique factorization domain, then so is .
Proof:
Clear by induction: Theorem
Let be a unique factorization domain, and let be a system of representatives of the primes of (i.e. all primes up to multiplication by units). Let be the field of fractions. Then every (where the here denotes exclusion of ) admits a unique factorization of the form where and with for almost all (that is, only a finite amount of nonzero 's).
Proof:
We know and since and have prime factorizations, it is clear that has a factorization of the requested form. We now prove uniqueness. Suppose Then, by multiplying by for every where and multiplying by for every where we eliminate all denominators obtaining an equality of factorizations of , which <a data-tooltip-position="top" aria-label="^2390f0" data-href="#^2390f0" href="#^2390f0" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Theorem</a><a data-tooltip-position="top" aria-label="^2390f0" data-href="#^2390f0" href="#^2390f0" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Theorem</a> says are unique if they aren't units, and if they are units they are also unique since units only factor into units. Hence uniqueness follows.
Definition: Let be a UFD, and be prime.<br>
a) For every , is the exponent of (the equivalence class of) in the factorization in <a data-tooltip-position="top" aria-label="^96b7c8" data-href="#^96b7c8" href="#^96b7c8" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Theorem</a><a data-tooltip-position="top" aria-label="^96b7c8" data-href="#^96b7c8" href="#^96b7c8" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Theorem</a>. Also, .
b) For every , Lemma
Let be a UFD, and .
a) prime, prime such that b) prime, Proof:
a) Both -directions are clear, and if we know each meaning each so .
b) If the result is clear by a). Otherwise, is clear since then each . Now, if prime, , then every , meaning each so .
Definition: Primitive polynomial
Let be a UFD. is primitive if is a of its coefficients.
Lemma
Let be a UFD and .
a) is primitive prime, .
b) such that and is primitive.
Remark: is called the context of , and is unique up to units.
Proof:
a) : Suppose is primitive and let be prime. Then is a common divisor of the coefficients of , so we must have .
: Let be a common divisor of the coefficients . If we are done, so suppose . Then has a prime factorization, meaning there is some prime common divisor of the coefficients. But this means every so , which is a contradiction. Hence so is a of the coefficients.
b) Let be but for any every prime where , is multiplied by . Then every fraction turns into an element of and any non-unit common divisors are eliminated. Formally, it is then clear that for every prime , which by a) means it's primitive. The final multiplier clearly lies in .
Lemma
Let be a UFD, prime, and . Then, for any , Proof:
Let and . Then there are minimal such that and , meaning the :th coefficient is and since are minimal, only one term in this sum, namely , has , while all other have . This means that . This is also clearly a lower bound of , hence we are done. (not confident about this proof but this is my intuition)
Lemma
Let be a UFD, , be primitive, and . Then Proof:<br>
For every prime we know and by <a data-tooltip-position="top" aria-label="^a0e729" data-href="#^a0e729" href="#^a0e729" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Lemma</a><a data-tooltip-position="top" aria-label="^a0e729" data-href="#^a0e729" href="#^a0e729" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Lemma</a>. Hence if we know meaning for every prime , meaning .
Theorem
Let be a UFD, , and .
a) Let . Then is prime in is prime in .
b) Let . Then is prime in is primitive and prime in .
Proof:
a) Let and let be the canonical projection (we use instead of because we want to distinguish between ideals in and in ). Then is a surjective homomorphism, with Hence With this, we have b) : We know there are and such that . Since we must have . Since is prime, we thus have or , but since we must have , meaning there is a such that . Thus meaning so , meaning is primitive.
We now show that is irreducible in (equivalently, prime). Suppose for some . Then there are and such that and . Then , and is prime, hence irreducible, so either or meaning either or , so is irreducible in .
: Suppose for some . Then clearly in , so since is prime in , we can WLOG assume in , so that for some . Since is primitive, we have for every prime , meaning . Hence in , so is indeed prime in .
<br>Proof of <a data-tooltip-position="top" aria-label="^8deeb1" data-href="#^8deeb1" href="#^8deeb1" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Gauß Theorem</a><a data-tooltip-position="top" aria-label="^8deeb1" data-href="#^8deeb1" href="#^8deeb1" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Gauß Theorem</a>
Let be a UFD and . Let .
By <a data-tooltip-position="top" aria-label="^447400" data-href="#^447400" href="#^447400" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Lemma</a><a data-tooltip-position="top" aria-label="^447400" data-href="#^447400" href="#^447400" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Lemma</a>, there are and a primitive such that . Since , by <a data-tooltip-position="top" aria-label="^a0e729" data-href="#^a0e729" href="#^a0e729" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Lemma</a><a data-tooltip-position="top" aria-label="^a0e729" data-href="#^a0e729" href="#^a0e729" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Lemma</a> we must have . Thus either or for prime 's. By <a data-tooltip-position="top" aria-label="^fdb8c3" data-href="#^fdb8c3" href="#^fdb8c3" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Theorem</a><a data-tooltip-position="top" aria-label="^fdb8c3" data-href="#^fdb8c3" href="#^fdb8c3" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Theorem</a>, the 's are also prime in . Hence it remains to show that has a prime factorization in .
If we have , so we are done. Now assume , i.e. that . Since is a UFD ( is an ID, so is too), we know for prime . Each for and primitive , which means each is prime in . Let . Then By <a data-tooltip-position="top" aria-label="^a0e729" data-href="#^a0e729" href="#^a0e729" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Lemma</a><a data-tooltip-position="top" aria-label="^a0e729" data-href="#^a0e729" href="#^a0e729" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Lemma</a> we must have , meaning is a prime factorization of .
Theorem: Eisenstein's criterion
Let be a UFD, and let be a primitive non-constant polynomial. Suppose that there is a prime such that for Then is irreducible.
Proof:
Suppose for contradiction that for some and . Note that since is primitive. Since , we have . Since and is prime, we can assume WLOG that and (since it cannot be that divides both and ). Now, let be maximal such that for (note that ). Consider . Since , we conclude , and therefore . But then implying , meaning which is a contradiction.
]]></description><link>matematik/groups-and-rings/lecture-notes/unique-factorization-domains.html</link><guid isPermaLink="false">Matematik/Groups and rings/Lecture notes/Unique Factorization Domains.md</guid><pubDate>Tue, 01 Apr 2025 13:41:47 GMT</pubDate></item><item><title><![CDATA[Noether's Theorem and Symmetries]]></title><description><![CDATA[Definition: Symmetry
A transformation of such that is a continuous symmetry of the Lagrangian if Noether's Theorem
For each continuous symmetry of , there exists a <a data-tooltip-position="top" aria-label="Conserved quantities" data-href="Conserved quantities" href="fysik/fysik-för-teknisk-matematik/analytical-mechanics/conserved-quantities.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Conserved quantity</a><a data-tooltip-position="top" aria-label="Conserved quantities" data-href="Conserved quantities" href="fysik/fysik-för-teknisk-matematik/analytical-mechanics/conserved-quantities.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Conserved quantity</a>.
Proof:
We have which means and thus the quantity evaluated at is conserved.
Example: Translations
Consider the system of particles, with Suppose we choose to be a translation in direction of , so that It is clear by the definition of this Lagrangian that such a constant translation leaves invariant. Hence, clearly , so this is a symmetry. Its conserved quantity becomes is just the total linear momentum, which means that the total linear momentum in the direction is preserved, and since we can choose arbitrarily, the total linear momentum is always preserved.
Example: Rotations]]></description><link>fysik/fysik-för-teknisk-matematik/analytical-mechanics/noether's-theorem-and-symmetries.html</link><guid isPermaLink="false">Fysik/Fysik för Teknisk Matematik/Analytical Mechanics/Noether's Theorem and Symmetries.md</guid><pubDate>Thu, 27 Mar 2025 11:45:38 GMT</pubDate></item><item><title><![CDATA[Analytical Mechanics]]></title><description><![CDATA[
<a data-href="A Single Particle" href="fysik/fysik-för-teknisk-matematik/analytical-mechanics/a-single-particle.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">A Single Particle</a><a data-href="A Single Particle" href="fysik/fysik-för-teknisk-matematik/analytical-mechanics/a-single-particle.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">A Single Particle</a>
<br><a data-href="Angular momentum" href="fysik/fysik-för-teknisk-matematik/analytical-mechanics/angular-momentum.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Angular momentum</a><a data-href="Angular momentum" href="fysik/fysik-för-teknisk-matematik/analytical-mechanics/angular-momentum.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Angular momentum</a>
<br><a data-href="Energy" href="fysik/fysik-för-teknisk-matematik/analytical-mechanics/energy.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Energy</a><a data-href="Energy" href="fysik/fysik-för-teknisk-matematik/analytical-mechanics/energy.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Energy</a> <br><a data-href="The Principle of Least Action" href="fysik/fysik-för-teknisk-matematik/analytical-mechanics/the-principle-of-least-action.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">The Principle of Least Action</a><a data-href="The Principle of Least Action" href="fysik/fysik-för-teknisk-matematik/analytical-mechanics/the-principle-of-least-action.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">The Principle of Least Action</a>
<br><a data-href="Conserved quantities" href="fysik/fysik-för-teknisk-matematik/analytical-mechanics/conserved-quantities.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Conserved quantities</a><a data-href="Conserved quantities" href="fysik/fysik-för-teknisk-matematik/analytical-mechanics/conserved-quantities.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Conserved quantities</a>
<br><a data-href="Noether's Theorem and Symmetries" href="fysik/fysik-för-teknisk-matematik/analytical-mechanics/noether's-theorem-and-symmetries.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Noether's Theorem and Symmetries</a><a data-href="Noether's Theorem and Symmetries" href="fysik/fysik-för-teknisk-matematik/analytical-mechanics/noether's-theorem-and-symmetries.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Noether's Theorem and Symmetries</a> ]]></description><link>fysik/fysik-för-teknisk-matematik/analytical-mechanics.html</link><guid isPermaLink="false">Fysik/Fysik för Teknisk Matematik/Analytical Mechanics.md</guid><pubDate>Thu, 27 Mar 2025 11:38:29 GMT</pubDate></item><item><title><![CDATA[Conserved quantities]]></title><description><![CDATA[Definition: Conserved quantity
A function is a constant of motion or conserved quantity if it is constant in time, i.e., whenever satisfies <a data-tooltip-position="top" aria-label="The Principle of Least Action > ^7457da" data-href="The Principle of Least Action#^7457da" href="fysik/fysik-för-teknisk-matematik/analytical-mechanics/the-principle-of-least-action.html#^7457da" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Lagrange's equations</a><a data-tooltip-position="top" aria-label="The Principle of Least Action > ^7457da" data-href="The Principle of Least Action#^7457da" href="fysik/fysik-för-teknisk-matematik/analytical-mechanics/the-principle-of-least-action.html#^7457da" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Lagrange's equations</a>.
A conserved quantity is thus conserved along the path of the system.Theorem
If the Lagrangian does not explicitly depend on time, i.e. , then is a conserved quantity.<br>
Remark: When written as a function of and , is the <a data-tooltip-position="top" aria-label="The Hamiltonian equations" data-href="The Hamiltonian equations" href="fysik/fysik-för-teknisk-matematik/analytical-mechanics/the-hamiltonian-equations.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Hamiltonian</a><a data-tooltip-position="top" aria-label="The Hamiltonian equations" data-href="The Hamiltonian equations" href="fysik/fysik-för-teknisk-matematik/analytical-mechanics/the-hamiltonian-equations.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Hamiltonian</a>.
Proof:
We get and from which it is clear that .
Theorem
Suppose for some . Then is said to be ignorable or cyclic, and is a conserved quantity.
Proof:
Clear directly from Lagrange's equations, giving ]]></description><link>fysik/fysik-för-teknisk-matematik/analytical-mechanics/conserved-quantities.html</link><guid isPermaLink="false">Fysik/Fysik för Teknisk Matematik/Analytical Mechanics/Conserved quantities.md</guid><pubDate>Thu, 27 Mar 2025 08:47:24 GMT</pubDate></item><item><title><![CDATA[The Principle of Least Action]]></title><description><![CDATA[Consider a model of particles at positions . We can model the state of the whole system as an element where . When time elapses, the particles may start moving based on the physical situation we are modelling, but since each particle moves smoothly, we can equivalently say that is moving smoothly. By treating as a function of time, we can thus describe any set of paths that the particles take as a single path that takes. We call the space that lives in the configuration space . This doesn't necessarily have to be , but must formally be a manifold. For example, in the common example of a pendulum, we can choose the configuration space to be the circle of possible positions.Definition: Lagrangian, action
Let for . A Lagrangian is a function For every such function , we define the action as a functional The point of this is that by choosing the right Lagrangian, we can determine the actual path that i.e. the particles take, by the Principle of Least Action.The Principle of Least Action (Hamilton's Principle)
The path taken is a path such that the action is stationary, that is, Since this principle is stated as a differential equation, it makes sense for the solution to be a set of differential equations in terms of and , which are called the Euler-Lagrange equations.Theorem: Euler-Lagrange equations
The path taken is a path such that Proof:
Let be the path taken between times and , and let be any altering of the path , i.e. a path such that . Then, being stationary means that for every such . We evalute this derivative and get Since this last equality holds for all , we must have In the common case of particles moving in a potential, we use the Lagrangian where is total kinetic energy and is total potential energy.Example: Mathematical pendulum
<img alt="center" src="fysik/fysik-för-teknisk-matematik/analytical-mechanics/mathematical_pendulum.png" target="_self" style="width: 150px; max-width: 100%;">
Consider the situation above and say we want to find the angle as a function of time. We want to determine the Lagrangian . We consider to be at the top of the rope and describe the particle's coordinates by and . Its speed is then given by and thus we get its kinetic energy . We also know its potential energy is . Thus our Lagrangian is The Euler-Lagrange equation now gives us If we further assume that is small and make the approximation , and we also let , we obtain the differential equation ]]></description><link>fysik/fysik-för-teknisk-matematik/analytical-mechanics/the-principle-of-least-action.html</link><guid isPermaLink="false">Fysik/Fysik för Teknisk Matematik/Analytical Mechanics/The Principle of Least Action.md</guid><pubDate>Thu, 27 Mar 2025 08:25:10 GMT</pubDate><enclosure url="." length="0" type="false"/><content:encoded>&lt;figure&gt;&lt;img src="."&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Energy]]></title><description><![CDATA[In a closed system, intuitively there should not occur any spontaneous movement; a particle cannot start moving without something acting on it. We can state this as the total momentum being preserved, which is true in Newtonian mechanics and basically true in any reasonable physical model.However, this does not say anything about speed; if the universe consisted of two particles, they could be stationary, or moving in opposite velocities, which gives the same total momentum but somehow we have introduced more speed. Work and energy gives us a way to describe how momentum and forces relates to speed.When working with energy we consider the movement of a particle to be split into infinitesimal parts . If during this short movement the particle experiences a force , the direction of the force is highly relevant to how the speed of the particle changes. More specifically, we can split the force into two forces; one parallel and the other perpendicular to the direction in which the particle moves. Then, the parallel force only changes the particle's speed, while the perpendicular one only changes the particle's direction. By projecting onto , we thus get a quantity that strongly relates to how the particle's speed was changed during the movement. We call this quantity the work by on the particle.Definition: Work
If a particle experiences a force while moving infinitesimally by , the work by on the particle during this movement is If the particle moves from to and experiences the potentially dynamic force during this movement, the work by on the particle during this movement is where the line integral is done along the movement of the particle.
Theorem: Total work only depends on change of speed
If a particle between times and moves from to and has velocity at and at , then the total work by the net force on the particle is Proof:
By simply evaluating the integral we get We see that the time and distance that the particle moves, and the force that the particle experiences during the movement is irrelevant if we just know the start and end speeds. By choosing and we get a well-defined quantity for any particle with speed , which we call kinetic energy.Definition: Kinetic energy
The kinetic energy of a particle is the work done that takes a particle from standing still to its current velocity . Specifically, Corollary
If a particle during some movement changes its speed from to , then Definition: Conservative force
A conservative force is a force that depends only on the position , such that the work done by between any two points is independent of the path taken between those points, that is, This means that there is some potential function such that . We now consider a particle moving through such a field, i.e. only influenced by .Theorem
If a particle moves from to and is only influenced by a potential , then Proof:
By integrating we get From this, we now know that meaning the quantity is constant for any movement in a potential! Also, notice that it is sufficient that the only force doing work on the particle is the potential. We can thus do the same reasoning if the particle experiences some other force perpendicular to the direction it's moving in, which is useful for example in a model of a pendulum.Definition: Mechanical energy
The (mechanical) energy of a particle influenced by a potential is Remark: When the energy is considered to be a function of the position and momentum , it is referred to as the Hamiltonian .
]]></description><link>fysik/fysik-för-teknisk-matematik/analytical-mechanics/energy.html</link><guid isPermaLink="false">Fysik/Fysik för Teknisk Matematik/Analytical Mechanics/Energy.md</guid><pubDate>Mon, 24 Mar 2025 18:04:18 GMT</pubDate></item><item><title><![CDATA[The Hamiltonian equations]]></title><description><![CDATA[Recall that the entire state of a physical model can be described a point in a configuration space , and that the evolution of the model is described by a path that takes in . How the model evolves is described by the <a data-tooltip-position="top" aria-label="The Principle of Least Action > ^a7abcd" data-href="The Principle of Least Action#^a7abcd" href="fysik/fysik-för-teknisk-matematik/analytical-mechanics/the-principle-of-least-action.html#^a7abcd" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Lagrangian</a><a data-tooltip-position="top" aria-label="The Principle of Least Action > ^a7abcd" data-href="The Principle of Least Action#^a7abcd" href="fysik/fysik-för-teknisk-matematik/analytical-mechanics/the-principle-of-least-action.html#^a7abcd" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Lagrangian</a>, giving the differential equation Definition: Generalized momentum
The generalized momentum of is This strongly relates to our usual definitions of momentum. By choosing we get If represents cartesian coordinates of particles, this would be our usual momentum. If instead for example represents polar coordinates, the coordinates of would contain radial and angular momentums.The generalized momentum lets us rewrite the Euler-Lagrange equation as This is a differential equation requiring initial conditions, meaning we can think of as being in a -dimensional space that we call the phase space. A single point in the phase space is thus precisely the information needed to determine the whole evolution of our model. This means that two paths in the phase space cannot cross. We call such a path a flow in the phase space.Our goal now is to rewrite the Euler-Lagrange equation in terms of only and (thus eliminating ).Definition: Hamiltonian
Given a Lagrangian , the Hamiltonian is Hamiltonian equations
Proof:
We see that is a function of only since and we can thus invert this to get as a function of .
We now get which we know is equal to , so by matching terms and using that the Euler-Lagrange equation gave us , we get the Hamiltonian equations.
Example
Consider a single particle moving in a potential . We have the Lagrangian giving us the generalized momentum as before. This gives us the Hamiltonian where the last step was done to eliminate . We now have the Hamiltonian equations which both are familiar equations from the Newtonian formulation.
]]></description><link>fysik/fysik-för-teknisk-matematik/analytical-mechanics/the-hamiltonian-equations.html</link><guid isPermaLink="false">Fysik/Fysik för Teknisk Matematik/Analytical Mechanics/The Hamiltonian equations.md</guid><pubDate>Mon, 24 Mar 2025 12:49:50 GMT</pubDate></item><item><title><![CDATA[mathematical_pendulum]]></title><description><![CDATA[<img src="fysik/fysik-för-teknisk-matematik/analytical-mechanics/mathematical_pendulum.png" target="_self">]]></description><link>fysik/fysik-för-teknisk-matematik/analytical-mechanics/mathematical_pendulum.html</link><guid isPermaLink="false">Fysik/Fysik för Teknisk Matematik/Analytical Mechanics/mathematical_pendulum.png</guid><pubDate>Sun, 23 Mar 2025 17:10:08 GMT</pubDate><enclosure url="." length="0" type="false"/><content:encoded>&lt;figure&gt;&lt;img src="."&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Angular momentum]]></title><description><![CDATA[Definition: Angular momentum, torque
The angular momentum of a <a data-tooltip-position="top" aria-label="A Single Particle" data-href="A Single Particle" href="fysik/fysik-för-teknisk-matematik/analytical-mechanics/a-single-particle.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Particle</a><a data-tooltip-position="top" aria-label="A Single Particle" data-href="A Single Particle" href="fysik/fysik-för-teknisk-matematik/analytical-mechanics/a-single-particle.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Particle</a> is and the net torque acting on the particle is where is the net force acting on the particle.
These measurements clearly depend heavily on where we choose the origin of our frame of reference. is in a way a measurement of "how much" the particle is spinning around the origin, and that torque is to angular momentum what force is to (linear) momentum. We see this more clearly with that Some intuitive results follow, such as if a force does not affect the angular momentum, we must have meaning meaning has to be parallel to which makes sense intuitively. Such a force is called a central force.]]></description><link>fysik/fysik-för-teknisk-matematik/analytical-mechanics/angular-momentum.html</link><guid isPermaLink="false">Fysik/Fysik för Teknisk Matematik/Analytical Mechanics/Angular momentum.md</guid><pubDate>Sat, 22 Mar 2025 13:09:53 GMT</pubDate></item><item><title><![CDATA[A Single Particle]]></title><description><![CDATA[Definition: Particle
A particle is an object of insignificant size, like an electron, or sometimes a tennis ball or even the earth.
The position of a particle is usually denoted by , and the mass by .
Definition: Momentum
Consider a particle at position with mass . We define its momentum as and the net force acting on the particle as Definition: Force, inertial frame
A force acting on a particle is a contribution to the particle's net force.
An inertial frame of reference is a frame of reference where a particle's net force is the sum of the forces acting on it.
This definition of a force may seem vague, but when modelling a situation, we may for example describe a particle as only being influenced by two equally large forces acting in opposite directions. In this case, the net force of the particle is clearly zero, but we still distinguish this model from the case when the particle is free, i.e., no forces act on it, even if these are physically equivalent.The classical definition is not used here because this formulation assumes constant mass. In most cases of classical mechanics we will consider the mass to be constant, and we see that if this is the case, The classical way of defining an inertial frame is as a frame of reference where Newton's first law holds, i.e., a free particle (with constant mass) has constant velocity. This is in some sense an equivalent definition, since what the definitions really try to say is that there are no magical unmodeled forces acting on the particles; if the sum of the forces acting on the particle is zero, then the net force is zero and hence the velocity is constant (assuming constant mass).We may see a frame of reference as a camera observing the whole situation that we are modelling, which may be moving, rotating or even accelerating. However, we see quite clearly that an accelerating camera in an inertial frame of reference cannot possibly also be an inertial frame of reference, since it will then observe previously unobserved forces. However, we can classify all inertial frames of reference that a single one gives rise to.
Rotations: for some Translations: for some constant vector Boosts: for some constant velocity Time translations: for some constant These transformations of inertial reference frames together form a group called the Galilean Group, under which Newton's laws are invariant.]]></description><link>fysik/fysik-för-teknisk-matematik/analytical-mechanics/a-single-particle.html</link><guid isPermaLink="false">Fysik/Fysik för Teknisk Matematik/Analytical Mechanics/A Single Particle.md</guid><pubDate>Sat, 22 Mar 2025 12:44:59 GMT</pubDate></item><item><title><![CDATA[Kursplan]]></title><description><![CDATA[Kurslitteratur:
Tong's kompendium innehåller mycket mer än vi behöver kunna
Innehåll:
Mekanik (AM, Analytisk Mekanik)
Elektrodynamik (ED)
Kvantmekanik (QM)
]]></description><link>fysik/fysik-för-teknisk-matematik/kursplan.html</link><guid isPermaLink="false">Fysik/Fysik för Teknisk Matematik/Kursplan.md</guid><pubDate>Sat, 22 Mar 2025 09:56:46 GMT</pubDate></item><item><title><![CDATA[Euclidean domains]]></title><description><![CDATA[<a href=".?query=tag:matematik-groups" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-groups">#matematik-groups</a> Theorem: Polynomial division
Let be a commutative ring and such that the leading coefficient is a unit in . Then, for every there are unique such that where .
Proof:
Claim for every .
Proof:
Clear if . Otherwise, so . We now first prove existence. Let and . We induct on . If , then meaning we can pick . Now assume . If then we can choose and and we are done. Now assume . Then, consider which cancels out the highest degree term, meaning , so by induction can be divided with remainder, and plugging everything in we are done.
We now prove uniqueness. Let such that and where . We then get and we have but also , meaning , so .
Definition: Euclidean domain
A Euclidean domain is an integral domain together with a map that satisfies that for every and there are such that where or . is called the Euclidean function or degree function.
In summary, we need some notion of the remainder of division being "smaller" than the divisor. In the context of integers, it is just being smaller in an absolute sense, but for polynomials it's the degrees being smaller. Also, any field is a Euclidean domain with any function since division is always defined in with zero rest.Theorem
Every Euclidean domain is a PID.
Proof:
Let be the Euclidean delta function on , and let . Then, if it is principal so we are done. Now assume , and choose an such that is minimal. Let . Then for some where or . But we have so we can't have , meaning so , meaning .
Not every PID is a Euclidean domain though. A horrific example is which is a non-Euclidean PID.Also, from this theorem we immediately know that is a PID.Euclidean algorithm
Let be a Euclidean domain. The Euclidean algorithm takes and outputs a of and .
First, let and .
Then, for if then , and if then compute and such that and or .
Return such that and .
Proof:
For , the sequence of is strictly decreasing (as long as ) meaning it must at some point hit . Furthermore, for and we have and if and only if and , meaning any of and is also a of and . Hence , which is a of and , is a of and .
Corollary: Extended Euclidean algorithm
Let be a Euclidean domain and . Then the Eucliden algorithm yields such that the returned is .
Proof:
From the Euclidean algorithm we will have , , and so on until we have . By isolating the remainders, we see that in the end, is a sum of multiples of and multiples of .
Corollary:
The extended Euclidean algorithm can be used to compute a , , of many elements , and to find such that .
Proof:
Exercise, but show that .
Theorem
Let be an integral domain and . Then, if , then is a of . In particular, in PID's, 's always exist.
Proof:
Since every , we know , so is a common divisor. Also, since we have , so is a .
<br>Corollary
In PID's, the implication in <a data-tooltip-position="top" aria-label="^d96f03" data-href="#^d96f03" href="#^d96f03" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Theorem</a><a data-tooltip-position="top" aria-label="^d96f03" data-href="#^d96f03" href="#^d96f03" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Theorem</a> is an equivalence.
Proof:
Since is a PID, for some . Then is a . In integral domains 's are unique up to units, so for any we have , in particular, .
]]></description><link>matematik/groups-and-rings/lecture-notes/euclidean-domains.html</link><guid isPermaLink="false">Matematik/Groups and rings/Lecture notes/Euclidean domains.md</guid><pubDate>Thu, 20 Mar 2025 10:40:10 GMT</pubDate></item><item><title><![CDATA[Error correcting codes]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a> Definition: Binary code
A binary code of length is a subset , and an element of is called a code word.
Definition: Distance between words
The distance between two code words is The distance is thus the number of differing bits.Theorem
The distances between code words define a metric on .
Proof:
It is clear that and . Furthermore, we have Definition: Smallest distance
Given a binary code the smallest distance is Definition: Nearest neighbour decryption
Nearest neighbour decryption means interpreting a potentially erroneous received word as the closest word in , in terms of distance .
Theorem
A code can correct errors with nearest neighbour decryption if Proof:
Begin with any word and apply errors to get , meaning . Now, let where . Then, by the triangle inequality we have meaning is the nearest neighbour of .
Linear code
A binary code is linear if it is a linear subspace of , i.e. for every .
Here, is thus treated as a vector space over the field . The dimension of is the dimension of when treated as such a vector space.
Weight of a word
For any , the weight of is the number of 's in , i.e. Theorem
Let be linear, and . Then Proof:
For any we have , and since is linear, we have , so we must have .
Definition: Parity-check matrix
A parity check matrix is a matrix . It gives rise to a linear code Theorem
Let and . If all columns of are nonzero and distinct from each other, then can correct error.
Proof:<br>
All columns of being nonzero and distinct from each other means that they are pairwise linearly independent. Hence any has at least 's. By <a data-tooltip-position="top" aria-label="^10f6b6" data-href="#^10f6b6" href="#^10f6b6" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Theorem</a><a data-tooltip-position="top" aria-label="^10f6b6" data-href="#^10f6b6" href="#^10f6b6" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Theorem</a>, this means , which by <a data-tooltip-position="top" aria-label="^8628ac" data-href="#^8628ac" href="#^8628ac" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Theorem</a><a data-tooltip-position="top" aria-label="^8628ac" data-href="#^8628ac" href="#^8628ac" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Theorem</a> means can correct error.
Definition: Hamming code
Given an , a Hamming code is the kernel of a parity-check matrix containing all nonzero column vectors exactly once.
Importantly, by our previous theorem Hamming codes can thus correct error.Theorem
A Hamming code generated by has dimension .
Proof:
It is clear that the image of is of dimension , and that the preimage is of dimension , meaning .
Theorem
Among all binary codes with words of length that can correct error, the Hamming code with words of length is the largest, i.e. has the greatest number of words. Because of this, we call the Hamming code perfect.
Proof:
Let be a code that can correct error. We must clearly have . For any we can define and thus get that all are disjoint since . We also know meaning We already know the dimension of the hamming code is , so its size is (we have -coefficients to choose) meaning it's at least as large as .
]]></description><link>matematik/discrete-mathematics/error-correcting-codes.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/Error correcting codes.md</guid><pubDate>Wed, 12 Mar 2025 19:37:33 GMT</pubDate></item><item><title><![CDATA[Matchings]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a> <br>Definition: Matching, maximum matching, complete matching
A matching in a <a data-tooltip-position="top" aria-label="Graphs" data-href="Graphs" href="matematik/discrete-mathematics/graphs.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Graph</a><a data-tooltip-position="top" aria-label="Graphs" data-href="Graphs" href="matematik/discrete-mathematics/graphs.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Graph</a> is a subset such that no two edges in have a common vertex. If a vertex is contained in an edge in it is called matched, otherwise unmatched.
A maximum matching of is a matching such that no matching of has strictly greater cardinality. A complete matching or perfect matching of is a matching such that every vertex is matched.
Definition: Alternating path
Let be a graph and be a matching of . Then the path where is an alternating path for if the edges are in but the edges are not in , for all , and and are unmatched.
Theorem
Let be a graph and be a matching of . Then there is an alternating path for if and only if is not maximal.
Proof:
: Let be an alternating path for . Then, remove all edges from and insert all edges . The resulting matching is still a matching since and were previously unmatched. We thus removed edges and added edges, resulting in a larger matching, hence was not maximal to begin with.
: Let be a maximum matching, and let be the set of edges which are in or but not in both. is nonempty since is not maximal. Consider the graph . Every vertex must have degree at most , meaning consists of cycles and/or paths. In a cycle, precisely every second edge would need to belong to and every second to , so if consisted only of cycles we would have which is false. Hence can only have more edges if there is a path with an odd number of edges where there is one more edge from than from . This is then clearly an alternating path for .
We now focus on matchings in bipartite graphs. Here, we sometimes say that is complete as long as . If there will be some unmatched vertices of , but we sometimes ignore these.Definition: Deficiency
The deficiency of a bipartite graph is where is the set of neighbours of .
Theorem
The size of a maximum matching in a bipartite graph is Proof:<br>
It is clear that is an upper bound for since if such that , then for any matching, at least vertices of are unmatched. By <a data-tooltip-position="top" aria-label="^0556cf" data-href="#^0556cf" href="#^0556cf" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Theorem</a><a data-tooltip-position="top" aria-label="^0556cf" data-href="#^0556cf" href="#^0556cf" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Theorem</a>, it is thus sufficient to show that given any matching with , there is an alternating path for .
Let contain the unmatched vertices of , so . Then so there is a . If is unmatched we are done. Otherwise, let be matched with . Then , so there is another vertex which is a potential end of an alternating path. By continuing this process, we always reach new vertices of , so since the graph is finite, we will eventually reach an unmatched vertex of , to which we can create an alternating path from some vertex in .
Definition: Hall's condition
Let be a bipartite graph. Hall's condition is the statement that for every . In other words, that .
Corollary
A bipartite graph has a complete matching if and only if Hall's condition is satisfied.
Method: Finding a maximum matching Start with any matching with at least one edge.
Search for an alternating path for .
If an alternating path is found, construct a larger matching by "inverting" the alternating path, i.e. adding every edge of the path that was not in , and removing the ones that were. Then repeat from 2.
If no alternating path can be found, we are done. Method: Finding an alternating path
The brute force way is to choose an unmatched vertex and see if there is an alternating path starting with by just trying everything recursively.
]]></description><link>matematik/discrete-mathematics/matchings.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/Matchings.md</guid><pubDate>Wed, 12 Mar 2025 18:54:34 GMT</pubDate></item><item><title><![CDATA[Discrete mathematics]]></title><description><![CDATA[
<a data-href="Sets" href="matematik/discrete-mathematics/sets.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Sets</a><a data-href="Sets" href="matematik/discrete-mathematics/sets.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Sets</a>
<br><a data-href="The logical framework" href="matematik/discrete-mathematics/the-logical-framework.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">The logical framework</a><a data-href="The logical framework" href="matematik/discrete-mathematics/the-logical-framework.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">The logical framework</a> <br><a data-href="Natural numbers" href="matematik/discrete-mathematics/natural-numbers.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Natural numbers</a><a data-href="Natural numbers" href="matematik/discrete-mathematics/natural-numbers.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Natural numbers</a>
<br><a data-href="Induction" href="matematik/discrete-mathematics/induction.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Induction</a><a data-href="Induction" href="matematik/discrete-mathematics/induction.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Induction</a>
<br><a data-href="Minimality and maximality" href="matematik/discrete-mathematics/minimality-and-maximality.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Minimality and maximality</a><a data-href="Minimality and maximality" href="matematik/discrete-mathematics/minimality-and-maximality.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Minimality and maximality</a>
<br><a data-href="Counting" href="matematik/discrete-mathematics/counting.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Counting</a><a data-href="Counting" href="matematik/discrete-mathematics/counting.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Counting</a>
<br><a data-href="Infinite sets" href="matematik/discrete-mathematics/infinite-sets.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Infinite sets</a><a data-href="Infinite sets" href="matematik/discrete-mathematics/infinite-sets.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Infinite sets</a>
<br><a data-href="Relations" href="matematik/discrete-mathematics/relations.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Relations</a><a data-href="Relations" href="matematik/discrete-mathematics/relations.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Relations</a> <br><a data-href="Integers" href="matematik/discrete-mathematics/integers.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Integers</a><a data-href="Integers" href="matematik/discrete-mathematics/integers.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Integers</a>
<br><a data-href="Divisibility" href="matematik/discrete-mathematics/divisibility.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Divisibility</a><a data-href="Divisibility" href="matematik/discrete-mathematics/divisibility.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Divisibility</a>
<br><a data-href="Representations of integers" href="matematik/discrete-mathematics/representations-of-integers.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Representations of integers</a><a data-href="Representations of integers" href="matematik/discrete-mathematics/representations-of-integers.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Representations of integers</a>
<br><a data-href="Greatest common divisor" href="matematik/discrete-mathematics/greatest-common-divisor.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Greatest common divisor</a><a data-href="Greatest common divisor" href="matematik/discrete-mathematics/greatest-common-divisor.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Greatest common divisor</a>
<br><a data-href="Prime numbers" href="matematik/discrete-mathematics/prime-numbers.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Prime numbers</a><a data-href="Prime numbers" href="matematik/discrete-mathematics/prime-numbers.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Prime numbers</a>
<br><a data-href="Rational numbers" href="matematik/discrete-mathematics/rational-numbers.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Rational numbers</a><a data-href="Rational numbers" href="matematik/discrete-mathematics/rational-numbers.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Rational numbers</a> <br><a data-href="Combinatorics" href="matematik/discrete-mathematics/combinatorics.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Combinatorics</a><a data-href="Combinatorics" href="matematik/discrete-mathematics/combinatorics.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Combinatorics</a>
<br><a data-href="Euler's function" href="matematik/discrete-mathematics/euler's-function.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Euler's function</a><a data-href="Euler's function" href="matematik/discrete-mathematics/euler's-function.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Euler's function</a>
<br><a data-href="Words and selections" href="matematik/discrete-mathematics/words-and-selections.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Words and selections</a><a data-href="Words and selections" href="matematik/discrete-mathematics/words-and-selections.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Words and selections</a>
<br><a data-href="Permutations" href="matematik/discrete-mathematics/permutations.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Permutations</a><a data-href="Permutations" href="matematik/discrete-mathematics/permutations.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Permutations</a>
<br><a data-href="Unordered selections" href="matematik/discrete-mathematics/unordered-selections.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Unordered selections</a><a data-href="Unordered selections" href="matematik/discrete-mathematics/unordered-selections.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Unordered selections</a>
<br><a data-href="The Sieve principle" href="matematik/discrete-mathematics/the-sieve-principle.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">The Sieve principle</a><a data-href="The Sieve principle" href="matematik/discrete-mathematics/the-sieve-principle.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">The Sieve principle</a>
<br><a data-href="Möbius function" href="matematik/discrete-mathematics/möbius-function.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Möbius function</a><a data-href="Möbius function" href="matematik/discrete-mathematics/möbius-function.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Möbius function</a> <br><a data-href="Partitions" href="matematik/discrete-mathematics/partitions.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Partitions</a><a data-href="Partitions" href="matematik/discrete-mathematics/partitions.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Partitions</a>
<br><a data-href="Multinomial numbers" href="matematik/discrete-mathematics/multinomial-numbers.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Multinomial numbers</a><a data-href="Multinomial numbers" href="matematik/discrete-mathematics/multinomial-numbers.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Multinomial numbers</a>
<br><a data-href="Partitions of integers" href="matematik/discrete-mathematics/partitions-of-integers.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Partitions of integers</a><a data-href="Partitions of integers" href="matematik/discrete-mathematics/partitions-of-integers.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Partitions of integers</a>
<br><a data-href="Classification of permutations" href="matematik/discrete-mathematics/classification-of-permutations.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Classification of permutations</a><a data-href="Classification of permutations" href="matematik/discrete-mathematics/classification-of-permutations.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Classification of permutations</a> <br><a data-href="Congruence" href="matematik/discrete-mathematics/congruence.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Congruence</a><a data-href="Congruence" href="matematik/discrete-mathematics/congruence.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Congruence</a>
<br><a data-href="Latin squares" href="matematik/discrete-mathematics/latin-squares.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Latin squares</a><a data-href="Latin squares" href="matematik/discrete-mathematics/latin-squares.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Latin squares</a>
<br><a data-href="Chinese Remainder Theorem" href="matematik/discrete-mathematics/chinese-remainder-theorem.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Chinese Remainder Theorem</a><a data-href="Chinese Remainder Theorem" href="matematik/discrete-mathematics/chinese-remainder-theorem.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Chinese Remainder Theorem</a>
<br><a data-href="Error correcting codes" href="matematik/discrete-mathematics/error-correcting-codes.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Error correcting codes</a><a data-href="Error correcting codes" href="matematik/discrete-mathematics/error-correcting-codes.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Error correcting codes</a> <br><a data-href="Graphs" href="matematik/discrete-mathematics/graphs.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Graphs</a><a data-href="Graphs" href="matematik/discrete-mathematics/graphs.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Graphs</a>
<br><a data-href="Paths and cycles" href="matematik/discrete-mathematics/paths-and-cycles.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Paths and cycles</a><a data-href="Paths and cycles" href="matematik/discrete-mathematics/paths-and-cycles.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Paths and cycles</a>
<br><a data-href="Trees" href="matematik/discrete-mathematics/trees.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Trees</a><a data-href="Trees" href="matematik/discrete-mathematics/trees.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Trees</a>
<br><a data-href="Graph colouring" href="matematik/discrete-mathematics/graph-colouring.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Graph colouring</a><a data-href="Graph colouring" href="matematik/discrete-mathematics/graph-colouring.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Graph colouring</a>
<br><a data-href="Bipartite graphs" href="matematik/discrete-mathematics/bipartite-graphs.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Bipartite graphs</a><a data-href="Bipartite graphs" href="matematik/discrete-mathematics/bipartite-graphs.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Bipartite graphs</a>
<br><a data-href="Edge colouring" href="matematik/discrete-mathematics/edge-colouring.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Edge colouring</a><a data-href="Edge colouring" href="matematik/discrete-mathematics/edge-colouring.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Edge colouring</a>
<br><a data-href="Latin rectangles" href="matematik/discrete-mathematics/latin-rectangles.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Latin rectangles</a><a data-href="Latin rectangles" href="matematik/discrete-mathematics/latin-rectangles.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Latin rectangles</a>
<br><a data-href="Matchings" href="matematik/discrete-mathematics/matchings.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Matchings</a><a data-href="Matchings" href="matematik/discrete-mathematics/matchings.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Matchings</a>
<br><a data-href="Directed graphs" href="matematik/discrete-mathematics/directed-graphs.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Directed graphs</a><a data-href="Directed graphs" href="matematik/discrete-mathematics/directed-graphs.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Directed graphs</a>
<br><a data-href="Networks and flows" href="matematik/discrete-mathematics/networks-and-flows.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Networks and flows</a><a data-href="Networks and flows" href="matematik/discrete-mathematics/networks-and-flows.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Networks and flows</a>
]]></description><link>matematik/discrete-mathematics.html</link><guid isPermaLink="false">Matematik/Discrete mathematics.md</guid><pubDate>Wed, 12 Mar 2025 18:54:13 GMT</pubDate></item><item><title><![CDATA[Matchings (old)]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a> <br>Definition: Matching, maximum matching, complete matching
A matching in a <a data-tooltip-position="top" aria-label="Bipartite graphs" data-href="Bipartite graphs" href="matematik/discrete-mathematics/bipartite-graphs.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Bipartite graph</a><a data-tooltip-position="top" aria-label="Bipartite graphs" data-href="Bipartite graphs" href="matematik/discrete-mathematics/bipartite-graphs.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Bipartite graph</a> is a subset such that no two edges in have a common vertex. If no matching of with greater cardinality than exists, is a maximum matching of .
If , is called a complete matching or perfect matching of .
Definition: Alternating path
Let be a bipartite graph and be a matching of . Then the path is an alternating path for if the edges are in but the edges are not in , for all , and and do not belong to any edge of .
Definition: Hall's condition
Let be a bipartite graph. Hall's condition is the statement that for every , where is the set of neighbours of .
Theorem
A bipartite graph has a complete matching if and only if Hall's condition is satisfied.
Proof:
: Let be a complete matching and let . We know each has a corresponding such that , and since this is unique with respect to , we know there are at least neighbours of .
: The idea is to show that given a matching where we can construct a matching with . Let be a vertex not matched by . Then we have , so there is a where is an edge not in . Now add to . If was previously unmatched, we are done. Otherwise, there was an , which we remove from . Then we have , so we can repeat this process for , always reaching unique elements of and , meaning the process must eventually stop since the graph is finite. Since the process stops once we hit an unmatched , we will have added an edge to , meaning .
Definition: Deficiency
The deficiency of a bipartite graph is Note that since we can choose . This means Hall's condition is equivalent to .Theorem
The size of a maximum matching in a bipartite graph is Proof:
Let such that . This means that for any matching, at least elements of are unmatched, so . Remains to show that such a matching exists. Let be a set of elements and let where , , and where is the set of all edges between and . Then we have meaning by the definition of . This means satisfies Hall's condition, meaning has a complete matching . Now we can remove the edges from which have a vertex in , and obtain a matching of size .
Theorem
If a matching in a bipartite graph is not a maximum matching, then contains an alternating path for .
Proof:
Let be a maximum matching, and let be the set of edges which are in or but not in both. Then the graph with edges and vertices of the edges of is a graph where each vertex has either degree or , meaning the components of it are paths or cycles. For any cycle, precisely every second edge must be in , since both and are matchings, meaning if all components are cycles, then , which is not true, so there must be at least one component which is a path of odd number of edges, and this must be an alternating path of .
This proof gives a strategy to find a maximum matching.Method: Finding a maximum matching Start with any matching with at least one edge.
Search for an alternating path for .
If an alternating path is found, construct a larger matching by "inverting" the alternating path, i.e. adding every edge of the path that was not in , and removing the ones that were. Then repeat from 2.
If no alternating path can be found, we are done. Method: Finding an alternating path
The brute force way is to choose an unmatched vertex and see if there is an alternating path starting with by just trying everything recursively.
]]></description><link>matematik/discrete-mathematics/matchings-(old).html</link><guid isPermaLink="false">Matematik/Discrete mathematics/Matchings (old).md</guid><pubDate>Wed, 12 Mar 2025 17:28:48 GMT</pubDate></item><item><title><![CDATA[Networks and flows]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a> <br>Definition: Network, weight function
A network is a <a data-tooltip-position="top" aria-label="Directed graphs" data-href="Directed graphs" href="matematik/discrete-mathematics/directed-graphs.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Directed graph</a><a data-tooltip-position="top" aria-label="Directed graphs" data-href="Directed graphs" href="matematik/discrete-mathematics/directed-graphs.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Directed graph</a> together with a weight or capacity function .
Definition: Source, sink
Let be a network.
A source is a vertex such that all arcs containing are directed away from .
A sink is a vertex such that all arcs containing are directed towards .
We will consider networks where a source and sink exist.Definition: Flow
A flow from the source to the sink in a network is a function such that for every where , and where for every , where is the capacity function.
Theorem
Let be a network with a source and sink , and let be a flow in . Then Proof:
We have Definition: Value
The value of a flow is Definition: Cut
Let be a network and be a partition of such that and . Then is called a cut, and the capacity of the cut is Theorem
If is a flow from to and is any cut, then Proof: Theorem
If is a flow from to and is any cut, then .
Proof:<br>
By <a data-tooltip-position="top" aria-label="^ce748b" data-href="#^ce748b" href="#^ce748b" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Theorem</a><a data-tooltip-position="top" aria-label="^ce748b" data-href="#^ce748b" href="#^ce748b" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Theorem</a>, we have Now, suppose that is a maximum flow and that is a cut with corresponding minimum capacity. From the theorem above we thus know that
We will see later that this is actually an equality.Definition: Residual graph
Let be a network and a flow in . The residual graph of with respect to is a network with the vertices of , and where For each edge , if , there is a forward edge in with capacity .
For each edge , if , there is a backward edge in with capacity . The residual graph thus shows where additional flow can be added or removed. The forward edges show where additional flow can be introduced without exceeding the capacity, and the backward edges show where flow can be removed without introducing negative flow.Definition: Augmenting path
Let be a network and a flow in with residual graph . An augmenting path is a directed path in from to .
A way to maximize flow given a network is now clear; find an augmenting path and introduce more flow along it. Specifically, it is clear that a flow of precisely the minimum capacity along the augmenting path can be introduced. We can show that by repeating this process until no augmenting path exists, always yields a maximum flow.Theorem
Let be a network. The maximum value of a flow from to is equal to the minimum capacity of a cut separating and .
Proof:<br>
Let be a flow in , and let be its residual graph. If there is an augmenting path in , introduce a flow of precisely the minimum capacity along the augmenting path. Repeat this process until there is no augmenting path.
We are thus left with a flow in such that there is no augmenting path. Let be the vertices reachable by directed paths from in (such paths are sometimes called incomplete augmenting paths), and be the rest of the vertices. Then, for any and where we know since if there would be an edge in making reachable from , contradicting that . Hence Furthermore, for any and where we know since if there would be an edge in making reachable from , contradicting that . Hence By <a data-tooltip-position="top" aria-label="^2c5455" data-href="#^2c5455" href="#^2c5455" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Theorem</a><a data-tooltip-position="top" aria-label="^2c5455" data-href="#^2c5455" href="#^2c5455" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Theorem</a>, this flow is maximal.
]]></description><link>matematik/discrete-mathematics/networks-and-flows.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/Networks and flows.md</guid><pubDate>Tue, 11 Mar 2025 13:31:46 GMT</pubDate></item><item><title><![CDATA[Classification of permutations]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a> <br>Definition: Type
Let . The type of is the partition of corresponding to the sizes of the <a data-tooltip-position="top" aria-label="Permutations > ^2310b0" data-href="Permutations#^2310b0" href="matematik/discrete-mathematics/permutations.html#^2310b0" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Cycles</a><a data-tooltip-position="top" aria-label="Permutations > ^2310b0" data-href="Permutations#^2310b0" href="matematik/discrete-mathematics/permutations.html#^2310b0" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Cycles</a> of .
Definition: Conjugate permutations
Let . If there exists a permutation such that we say that and are conjugate.
Theorem
Let . Then and are conjugate if and only if they have the same type.
Proof:
: Let such that . We see that , meaning any -cycle of has a corresponding -cycle of which is just the image of on the cycle.
: We want to construct a such that . For each subset on which is a -cycle there is a corresponding on which is a -cycle. Let and . Then define for every . We then get that We see then that on is a bijection to , so when this is done for every and corresponding , we get a bijection which satisfies .
Definition: Transposition
A transposition is a permutation which only interchanges two elements, meaning it has only one -cycle and the rest -cycles.
We quickly see that any cycle can be written as a composition of transpositions, for example, . Since any permutation is the composition of cycles, any permutation is the composition of transpositions.Notation
For any we denote the number of cycles of by .
Lemma
For any and transposition , is either or .
Proof:
We have two cases: either transposes elements in a single cycle of , or elements in different cycles of . In the first case, suppose we have a transposition of and , meaning we have since and , meaning . In the second case, suppose we have a transposition of and , meaning we have since and , meaning .
Theorem
Let be the composition of transpositions and also the composition of compositions. Then and have the same parity.
Proof:<br>
Follows from <a data-tooltip-position="top" aria-label="^368ef8" data-href="#^368ef8" href="#^368ef8" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Lemma</a><a data-tooltip-position="top" aria-label="^368ef8" data-href="#^368ef8" href="#^368ef8" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Lemma</a> since adding or removing from the total number of cycles must be done an even amount of times when going from the first composition of transpositions to the second, for the total number of cycles to be unchanged.
Definition: Even permutation, odd permutation
A permutation is even if it is the composition of an even number of transpositions, otherwise odd.
The sign of a permutation , denoted , is if it is odd, otherwise .
We notice that if can be written as transpositions and can be written as transpositions, then .Theorem
For any integer exactly half of the permutations in are even and exactly half of them are odd.
Proof:
Let be all unique even permutations of . Then take any transposition . We see that are all odd, and that if , then , so they are all unique. Now for any odd we know is even, meaning for some , meaning , but so meaning every odd permutation is among .
]]></description><link>matematik/discrete-mathematics/classification-of-permutations.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/Classification of permutations.md</guid><pubDate>Mon, 10 Mar 2025 14:35:58 GMT</pubDate></item><item><title><![CDATA[Combinatorics]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a> <br>Take two finite sets and consider a subset of the <a data-tooltip-position="top" aria-label="Sets > ^7919a5" data-href="Sets#^7919a5" href="matematik/discrete-mathematics/sets.html#^7919a5" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Set of all pairs</a><a data-tooltip-position="top" aria-label="Sets > ^7919a5" data-href="Sets#^7919a5" href="matematik/discrete-mathematics/sets.html#^7919a5" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Set of all pairs</a> of elements from and . Now say we want to count the elements in . For each we can define and then get Likewise, we can define and then get Theorem: The multiplication principle
Let be finite nonempty sets and be nonempty. Then with and . Also, if for every and for every , then .
Furthermore, .
Proof:<br>
Each set in is clearly disjoint, so the sum of their sizes is the size of the union, by <a data-tooltip-position="top" aria-label="Counting > ^a3c50f" data-href="Counting#^a3c50f" href="matematik/discrete-mathematics/counting.html#^a3c50f" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Theorem</a><a data-tooltip-position="top" aria-label="Counting > ^a3c50f" data-href="Counting#^a3c50f" href="matematik/discrete-mathematics/counting.html#^a3c50f" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Theorem</a>. The union of them is precisely since if then is in the set of . The same goes for , so the first part is done. From this, it immediately follows that if and for every and .
Finally, if we let we have for every , so .
]]></description><link>matematik/discrete-mathematics/combinatorics.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/Combinatorics.md</guid><pubDate>Mon, 10 Mar 2025 14:13:19 GMT</pubDate></item><item><title><![CDATA[Unordered selections]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a> We notice that an unordered selection without repetition is just a subset.Definition: Binomial numbers
Let be a set of size . Then which are called binomial numbers, pronounced choose .
Theorem
Let such that . Then Proof:
Let be a set of size and . Any -subset of either contains or doesn't, and is the number of ones that do contain , and is the number of ones that don't.
The fact that Pascal's triangle contains binomial numbers follows directly from this theorem.Theorem
Let such that . Then Proof:
Can be shown with induction using Pascal's triangle. Alternatively, we know that is the number of ordered selections of elements from without repetition, so dividing by removes the ordering, obtaining the formula above.
Theorem
The number of unordered selections with repetition of objects from is Proof:
Informally, we imagine placing "separators" in a list of places and let the empty places represent the chosen elements; specifically if an empty place is between separator and separator , the separator represents element being chosen.
Theorem: The Binomial theorem
Let . Then Proof:
Follows from commutativity of multiplication; the number of 's is the number of way to choose 's among the factors .
]]></description><link>matematik/discrete-mathematics/unordered-selections.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/Unordered selections.md</guid><pubDate>Mon, 10 Mar 2025 13:40:49 GMT</pubDate></item><item><title><![CDATA[Words and selections]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a> We notice that a function is uniquely determined by the -tuple Definition: Word
Let be a nonempty set. A function is called a word of length in the alphabet .
Remark can also be seen as a mathematical model of an ordered selection with repetition of things from the set .
For example, we may let denote the function where , , .Theorem
Let be nonempty finite sets where and . The set of all functions from to has size .
Proof:<br>
We know a function is corresponds to an -tuple of elements of , meaning an element from . The <a data-tooltip-position="top" aria-label="Combinatorics > ^5d1e05" data-href="Combinatorics#^5d1e05" href="matematik/discrete-mathematics/combinatorics.html#^5d1e05" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Multiplication principle</a><a data-tooltip-position="top" aria-label="Combinatorics > ^5d1e05" data-href="Combinatorics#^5d1e05" href="matematik/discrete-mathematics/combinatorics.html#^5d1e05" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Multiplication principle</a> gives .
We notice that if we restrict the functions to only injective ones, we instead model selection without repetition of things from .Theorem
The number of ordered selections without repetition of things from a set of size is the number of injections from to , which is given by Proof:
We have options for , options for and so on.
]]></description><link>matematik/discrete-mathematics/words-and-selections.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/Words and selections.md</guid><pubDate>Mon, 10 Mar 2025 13:35:13 GMT</pubDate></item><item><title><![CDATA[Bipartite graphs]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a> <br>Definition: Bipartite graph
A graph is bipartite if , where is the <a data-tooltip-position="top" aria-label="Graph colouring > ^6d0273" data-href="Graph colouring#^6d0273" href="matematik/discrete-mathematics/graph-colouring.html#^6d0273" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Chromatic number</a><a data-tooltip-position="top" aria-label="Graph colouring > ^6d0273" data-href="Graph colouring#^6d0273" href="matematik/discrete-mathematics/graph-colouring.html#^6d0273" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Chromatic number</a> of .
Theorem
A graph is bipartite if and only if it contains no cycles with odd length.
<br>Given any <a data-tooltip-position="top" aria-label="Relations" data-href="Relations" href="matematik/discrete-mathematics/relations.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Relation</a><a data-tooltip-position="top" aria-label="Relations" data-href="Relations" href="matematik/discrete-mathematics/relations.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Relation</a> between disjoint sets and , we can create a bipartite graph describing the relation, by taking and letting . Clearly it's bipartite, since we can simply color all of with the first color, and all of with the second.Theorem
Let be a bipartite graph and let be the degree of in . Then Proof:
We get and likewise for .
]]></description><link>matematik/discrete-mathematics/bipartite-graphs.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/Bipartite graphs.md</guid><pubDate>Sun, 09 Mar 2025 15:42:56 GMT</pubDate></item><item><title><![CDATA[Directed graphs]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a> Definition: Directed graph
A directed graph or digraph is a set whose members are called vertices, together with a set , whose members are called edges or arcs, and we write .
<br>Note that formally, a directed graph is just a <a data-tooltip-position="top" aria-label="Relations" data-href="Relations" href="matematik/discrete-mathematics/relations.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Relation</a><a data-tooltip-position="top" aria-label="Relations" data-href="Relations" href="matematik/discrete-mathematics/relations.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Relation</a> on : This means that results for relations can be applied to digraphs too. For example, a symmetric relation implies that , so all arcs in the graph can be thought of as double pointed.Definition: Directed walk, directed path
A directed walk in a digraph is a sequence of vertices such that for every .
A directed path is a directed walk where all vertices are distinct.
A directed cycle is a directed walk where all vertices are distinct except that .
Definition: Tournament
A tournament is a digraph with exactly one arc between each pair of vertices.
Theorem
In any tournament there is a directed path containing all vertices.
Proof:
We show that any path not containing all vertices can be made longer. Take any path and suppose is a vertex not in the path. Then, if is an edge, we can prepend and we are done. Otherwise, is an edge. Then if is an edge, can be inserted between and . Otherwise, is an edge. Continue this process until either is inserted, or we reach , meaning is an edge, meaning can be appended to the end and we are done.
]]></description><link>matematik/discrete-mathematics/directed-graphs.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/Directed graphs.md</guid><pubDate>Sun, 09 Mar 2025 15:41:11 GMT</pubDate></item><item><title><![CDATA[Graphs]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a> Definition: Graph, vertex, edge
A graph is a pair where is a finite set whose members are called vertices and is a set of -subsets of whose members are called edges.
Definition: Adjacent, neighbour
Two vertices and of a graph are adjacent or neighbours whenever .
Definition: Isomorphic graphs
Two graphs and are isomorphic if there exists a bijection such that for every . In this case, is called an isomorphism from to .
Determining if two graphs are isomorphic is nontrivial, but two graphs can quickly be shown not to be isomorphic if for example they have a different number of vertices or edges.Definition: Degree of vertex
Let be a graph and . Then and the degree of is , or in other words, the number of edges containing .
So then of course, the multiset of degree counts of two isomorphic graphs must be the same, meaning this is another fast way of determining that two graphs are not isomorphic.Theorem
Let be a graph. Then Proof:
Each edge is counted twice; once for each vertex in the edge. In other words, Definition: Odd vertex, even vertex
A vertex is odd if its degree is odd, or even if its degree is even.
Given a graph we define This makes a partition of .Theorem: The number of odd vertices is even
Let be a graph. Then is even.
Proof:
We know that both and are even, and we also know meaning must also be even. But it's a sum of odd numbers, meaning it must be a sum of an even number of odd numbers.
Definition: Regular graph
A graph in which all vertices have the same degree is said to be regular with degree .
In the case of regular graphs, we thus see that .]]></description><link>matematik/discrete-mathematics/graphs.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/Graphs.md</guid><pubDate>Sun, 09 Mar 2025 14:46:13 GMT</pubDate></item><item><title><![CDATA[Infinite sets]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a> Definition: Finite set, infinite set
A set is called finite if it is empty or for some . If is not finite it is called infinite.
Theorem is infinite.
Proof:
We know is nonempty since . Now suppose is finite, so that . Then let . Then should be in , but we have for every , so , which is a contradiction.
Theorem
The set of all prime numbers is infinite.
Proof:
Suppose the set of all prime numbers is . Then let . Clearly is not a multiple of any since . This means is a prime not in , which is a contradiction.
Theorem
Let be a set and be a bijection. Then is infinite.
Proof:
If is finite, there is a bijection , so is a bijection between and , which is a contradiction.
Definition: Countable set
Let be a set. If there exists a bijection between and , we call countably infinite or just countable.
]]></description><link>matematik/discrete-mathematics/infinite-sets.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/Infinite sets.md</guid><pubDate>Mon, 03 Mar 2025 14:58:27 GMT</pubDate></item><item><title><![CDATA[Chinese Remainder Theorem]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a> Theorem: Chinese Remainder Theorem
Let and . Then there are solutions to The solutions are unique up to congruence mod .
Proof:<br>
By <a data-tooltip-position="top" aria-label="Greatest common divisor > ^d37e34" data-href="Greatest common divisor#^d37e34" href="matematik/discrete-mathematics/greatest-common-divisor.html#^d37e34" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Theorem</a><a data-tooltip-position="top" aria-label="Greatest common divisor > ^d37e34" data-href="Greatest common divisor#^d37e34" href="matematik/discrete-mathematics/greatest-common-divisor.html#^d37e34" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Theorem</a> we know there are such that . Then is a solution to the equation since we have Now suppose and are solutions to the equation. Then and , meaning and since we thus have meaning .
]]></description><link>matematik/discrete-mathematics/chinese-remainder-theorem.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/Chinese Remainder Theorem.md</guid><pubDate>Mon, 24 Feb 2025 09:35:10 GMT</pubDate></item><item><title><![CDATA[Ideals]]></title><description><![CDATA[<a href=".?query=tag:matematik-groups" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-groups">#matematik-groups</a> Definition: Ideal
Let be a ring. A subset is an ideal in if is an additive subgroup of , and for every and .
If is an ideal of , we write . Example for any . for any .
These are the trivial ideals. Note that if is a field, then these are the only ideals since every element can be reached from any element (except ) by multiplication. This also means has no non-trivial ideals.
. These are all the ideals of ! Theorem
Let be a commutative ring. Let . Then is the smallest ideal in containing . This is called the principal ideal generated by .
Let . Then is the smallest ideal in containing . If , we say that is finitely generated. Theorem
Let be a ring, . Then .
.
. ]]></description><link>matematik/groups-and-rings/lecture-notes/ideals.html</link><guid isPermaLink="false">Matematik/Groups and rings/Lecture notes/Ideals.md</guid><pubDate>Mon, 24 Feb 2025 09:16:29 GMT</pubDate></item><item><title><![CDATA[Power series and Polynomials]]></title><description><![CDATA[<a href=".?query=tag:matematik-groups" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-groups">#matematik-groups</a> ]]></description><link>matematik/groups-and-rings/lecture-notes/power-series-and-polynomials.html</link><guid isPermaLink="false">Matematik/Groups and rings/Lecture notes/Power series and Polynomials.md</guid><pubDate>Mon, 24 Feb 2025 09:15:37 GMT</pubDate></item><item><title><![CDATA[Ring homomorphisms and Factor rings]]></title><description><![CDATA[<a href=".?query=tag:matematik-groups" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-groups">#matematik-groups</a> Definition: Ring homomorphism
Let be rings. Then is a ring homomorphism if is a group homomorphism with respect to , meaning and is a monoid homomorphism with respect to , meaning Theorem
Let be a ring homomorphism. . is a subring of . is a group homomorphism with respect to is a subring of . Corollary
Let be a ring homomorphism, and a field. Then is injective.
Proof:
, so either or . But , so .
Remark
Let and be ring homomorphisms. Then is a ring homomorphism.
Also, if is bijective, then is a ring homomorphism.
Example
Let be rings. The following are ring homomorphisms. The inclusion map .
We can send formal polynomial to polynomial functions by . Note that in general, is not injective.
For example, let . Then , but .
For a fixed , . Theorem
Let be a ring and . Then defines an equivalence relation on . The equivalence classes .
The set of all equivalence classes is a ring with This is called a factor ring or quotient ring, or residue class ring.
The canonical projection is a surjectve ring homomorphism with . Proof:
Recall that is an abelian group. This means is a normal subgroup. This means is a factor group and that is a surjective group homomorphism, and . We still need to show that the multiplication is well defined. Let and . We know and . This means since every term has a factor in and .
Hence, is also a monoid homomorphism with respect to .
Theorem: Universal property of Let be a ring homomorphism and . Then there is a unique ring homomorphism such that . is injective . Proof:
We already know that there is a unique group homomorphism with the same properties. Left to show is that is a ring homomorphism. Exercise!
Corollary
Let be a surjective ring homomorphism and . Then .
Example
Constructive way:
Can also use the universal property.
Theorem: First isomorphism theorem
Let be a ring and a subring, and . Then .
Theorem: Second isomorphism theorem
Let be a ring and be ideals. Then .
]]></description><link>matematik/groups-and-rings/lecture-notes/ring-homomorphisms-and-factor-rings.html</link><guid isPermaLink="false">Matematik/Groups and rings/Lecture notes/Ring homomorphisms and Factor rings.md</guid><pubDate>Thu, 20 Feb 2025 09:12:01 GMT</pubDate></item><item><title><![CDATA[Finite abelian groups]]></title><description><![CDATA[<a href=".?query=tag:matematik-groups" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-groups">#matematik-groups</a> Corollary
Let be a finite abelian group. Then has exactly one -Sylow subgroup, namely Proof:<br>
Since all -Sylow subgroups are conjugate, and conjugations does nothing to an abelian group, there can only be one -Sylow subgroup, call it . Now we must show .
: From <a data-tooltip-position="top" aria-label="^ab87c1" data-href="#^ab87c1" href="#^ab87c1" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Corollary (b)</a><a data-tooltip-position="top" aria-label="^ab87c1" data-href="#^ab87c1" href="#^ab87c1" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Corollary (b)</a> we know that since is a -group, .
: Firstly we show that is a subgroup. For any there are such that . WLOG, let . Then , meaning , so . Furthermore, we have , so , meaning is a subgroup. By <a data-tooltip-position="top" aria-label="^ab87c1" data-href="#^ab87c1" href="#^ab87c1" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Corollary (b)</a><a data-tooltip-position="top" aria-label="^ab87c1" data-href="#^ab87c1" href="#^ab87c1" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Corollary (b)</a> we know is a -subgroup, meaning there must exist a -Sylow subgroup containing , and since is the only -Sylow subgroup, .
Lemma
Let be a finite abelian group and be subgroups of . The homomorphism is injective if and only if for every , .
Proof:
: We know , so being injective means that this is the only product that gives . If there would be some in and , then so would be obtainable by making and , and .
: We want to show that . Assume . Then meaning , meaning . By induction, every .
Theorem
Let be a finite abelian group. Then is the direct product of its -Sylow subgroups. In other words, if Proof:<br>
The map given by is a homomorphism since is abelian. We first use <a data-tooltip-position="top" aria-label="^d2038f" data-href="#^d2038f" href="#^d2038f" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Lemma</a><a data-tooltip-position="top" aria-label="^d2038f" data-href="#^d2038f" href="#^d2038f" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Lemma</a> to show that is injective. Assume where each . Then there are 's such that , meaning where are exponents we don't care about since they are multiplying anyway. Since we also know , we have that is a common factor of and , but these are all distinct primes, so meaning . Hence is injective. It follows from the sizes of -Sylow groups that the product of the sizes of is , meaning is also surjective. So is indeed an isomorphism between and .
Furthermore, we can show that each of these subgroups are products of cyclic groups.Lemma
Let be an abelian -group. Let be of maximal order. Then there exists a subgroup such that is an isomorphism.
Proof:
Since is a -group, . We induce on . If then , so the base case holds. Now take any . We know for some .
Claim 1: For every , .
Proof: We know for some , implying If we are done. Otherwise, we can find a such that . Pick such of minimal order.
Claim 2: .
Proof: Consider . Since is of minimal order, . This means for some . Now we get So . Also, since , and so meaning since we are in a -group. This means , so let . Define . We firstly observe that since , and secondly we see that . So we conclude that and by the minimality of we have .
Now let . Since is of prime order, we know is cyclic. Furthermore, we know . To apply the induction hypothesis, we need to find an element of maximal order in . We can show that is such an element. By Cor. 1.3.13, for every where , . This means since if there is some in , then it generates all of , but . We now get that since , meaning . But the order is clearly at most , which is the next possible order, so , meaning indeed has maximal order. Now we can apply the induction hypothesis, meaning there exists a subgroup such that . Now notice that there is a one-to-one correspondence between each and each . this is true since we can use the canonical projection and map and . We now use this to get meaning and .
Claim 3: .
Proof: Let . We then get by the previous paragraph. This in turn means that .
Finally, this means is injective, meaning Theorem: Fundamental Theorem of Finite Abelian Groups
Every finite abelian group is the direct product of cyclic groups of prime-power order.
Proof:
We have which by Theorem means . Furthermore we know where and has maximal order. This means and is a -group, . By induction we get .
Example 1.5.10
a) abelian and .
b) and abelian or .
c) , non-abelian, could be , but also the quaternion group. The quaternion group has 8 elements with relations Exercise: show that this is a group!
Subgroups: , , , , . We can show that these are all normal!
Theorem: Fundamental Theorem of Finitely Generated Abelian Groups
Let be abelian such that there is a subset such that , where . Then where is a finite abelian group.
Lemma
Consider for some with factorization . Then there is a unique subgroup of order , namely .
Proof:
Clearly it's a subgroup of order . We know show uniqueness. Let . By Fermat, we know that for every meaning every element in is a multiple of , more concretely, . We also know so .
]]></description><link>matematik/groups-and-rings/lecture-notes/finite-abelian-groups.html</link><guid isPermaLink="false">Matematik/Groups and rings/Lecture notes/Finite abelian groups.md</guid><pubDate>Wed, 12 Feb 2025 18:48:57 GMT</pubDate></item><item><title><![CDATA[Group actions]]></title><description><![CDATA[<a href=".?query=tag:matematik-groups" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-groups">#matematik-groups</a> Definition: Group action
Let be a group and any set. An action or operation of on is a map such that for every . for every and . Remark
A group action can be seen as just a homomorphism from to . This is because for any , the map is a bijection since it has the inverse . So the map that takes to the map is a homomorphism from to .
Definition: Stabilizer subgroup, orbit
Let be a group, any set, and be a group action by on . The stabilizer subgroup or isotropy subgroup of in is The -orbit of is and the orbit space is Furthermore, we call the group action transitive if .
Remark
We can define an equivalence relation on by This makes the orbits of equivalence classes with respect to :Thus we see that is the disjoint union of its orbits.
Corollary: The orbit equation
Let be a group, any set and a group action by on . If is finite, Theorem: The orbit stabilizer theorem
Let be a group, any set, and be a group action by on . Then the -map induces a bijection where is the stabilizer of . Furthermore, Proof:
The map given by is clearly surjective. For any we see that This means that the map given by is well-defined, surjective and injective, so a bijection.
Definition
Let be a group, any set, and be a group action by on . Then Lemma: Burnside's lemma
Let be a finite group, any set and . Then Proof:
Let . We then have and meaning that The sizes of the left and right expression being the same then gives from which the statement follows directly.
]]></description><link>matematik/groups-and-rings/lecture-notes/group-actions.html</link><guid isPermaLink="false">Matematik/Groups and rings/Lecture notes/Group actions.md</guid><pubDate>Wed, 12 Feb 2025 09:32:05 GMT</pubDate></item><item><title><![CDATA[Latin rectangles]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a> Definition: Latin rectangle
A latin rectangle, where , is an array with rows and columns in which each of symbols occur such that each row contains every symbol once and no symbol occurs more than once in a column.
<br>Theorem
Any latin rectangle can be completed to form a <a data-tooltip-position="top" aria-label="Latin squares" data-href="Latin squares" href="matematik/discrete-mathematics/latin-squares.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Latin square</a><a data-tooltip-position="top" aria-label="Latin squares" data-href="Latin squares" href="matematik/discrete-mathematics/latin-squares.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Latin square</a>.
Proof:<br>
Call the symbols , the column and the rows . We can now create a <a data-tooltip-position="top" aria-label="Bipartite graphs" data-href="Bipartite graphs" href="matematik/discrete-mathematics/bipartite-graphs.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Bipartite graph</a><a data-tooltip-position="top" aria-label="Bipartite graphs" data-href="Bipartite graphs" href="matematik/discrete-mathematics/bipartite-graphs.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Bipartite graph</a> with vertices where all are connected to every . Now, if a column contains , we <a data-tooltip-position="top" aria-label="Edge colouring" data-href="Edge colouring" href="matematik/discrete-mathematics/edge-colouring.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Colour</a><a data-tooltip-position="top" aria-label="Edge colouring" data-href="Edge colouring" href="matematik/discrete-mathematics/edge-colouring.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Colour</a> that edge where is the row where appears in . Now, let be the set of uncoloured edges. The graph is now bipartite and regular with degree since each column contains distinct colours. By <a data-tooltip-position="top" aria-label="Edge colouring > ^42cfba" data-href="Edge colouring#^42cfba" href="matematik/discrete-mathematics/edge-colouring.html#^42cfba" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Theorem</a><a data-tooltip-position="top" aria-label="Edge colouring > ^42cfba" data-href="Edge colouring#^42cfba" href="matematik/discrete-mathematics/edge-colouring.html#^42cfba" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Theorem</a> we know has an edge colouring with at most colours, call them . We can now extend the latin rectangle to a latin square where we put in where is the colour of .
Theorem
Let be a partial latin rectangle in which the symbols are used, and let be the number of times occurs in . Then can be completed to a latin square if and only if for every .
Proof:<br>
: We know should appear exactly times in the completed square. We also know it appears exactly times among the added rows, and times among the added columns, meaning it appears additional times, so we have : We construct a bipartite graph with vertices where denote the rows, and let be an edge if does not occur in row . We know each row contains distinct symbols, so for every . Now, since , and we know every occurence of is in a different row, we know has an edge to at most rows, so . This means that the maximum degree of our graph is , so by <a data-tooltip-position="top" aria-label="Edge colouring > ^42cfba" data-href="Edge colouring#^42cfba" href="matematik/discrete-mathematics/edge-colouring.html#^42cfba" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Theorem</a><a data-tooltip-position="top" aria-label="Edge colouring > ^42cfba" data-href="Edge colouring#^42cfba" href="matematik/discrete-mathematics/edge-colouring.html#^42cfba" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Theorem</a> there exists an edge colouring with at most colours, call them . Finally, we extend the rectangle to an latin rectangle by putting in where is the colour of the edge . Now, by <a data-tooltip-position="top" aria-label="^0bb68d" data-href="#^0bb68d" href="#^0bb68d" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Theorem</a><a data-tooltip-position="top" aria-label="^0bb68d" data-href="#^0bb68d" href="#^0bb68d" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Theorem</a>, this can be extended to an latin square.
]]></description><link>matematik/discrete-mathematics/latin-rectangles.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/Latin rectangles.md</guid><pubDate>Tue, 11 Feb 2025 10:38:28 GMT</pubDate></item><item><title><![CDATA[Edge colouring]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a> Definition: Edge colouring
Let be a graph. An edge colouring of is a function such that for every pair of distinct edges such that and share a vertex.
<br>Theorem
If is a finite <a data-tooltip-position="top" aria-label="Bipartite graphs" data-href="Bipartite graphs" href="matematik/discrete-mathematics/bipartite-graphs.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Bipartite graph</a><a data-tooltip-position="top" aria-label="Bipartite graphs" data-href="Bipartite graphs" href="matematik/discrete-mathematics/bipartite-graphs.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Bipartite graph</a>, then the minimum number of colours needed for an edge colouring of is the maximum <a data-tooltip-position="top" aria-label="Graphs > ^bb336e" data-href="Graphs#^bb336e" href="matematik/discrete-mathematics/graphs.html#^bb336e" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Degree</a><a data-tooltip-position="top" aria-label="Graphs > ^bb336e" data-href="Graphs#^bb336e" href="matematik/discrete-mathematics/graphs.html#^bb336e" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Degree</a> of .
Proof:
Clearly an edge colouring is not possible with fewer colours than the maximum degree of , since all edges of the vertex with that degree must have distinct colors. Left to prove is that it is always possible to colour with that many colours. We induce on . If , then we clearly only need one colour. Now take a bipartite graph with edges and maximum degree . Take any edge and consider with . Clearly is also a bipartite graph, whose maximum degree is either or . By our induction hypothesis, we know this can be coloured with at most colours. Since we removed the edge , we know , meaning there is a colour in the graph, not used among the edges of , and a colour in the graph, not used among the edges of . If we can choose we can give this colour to and we are done.
Otherwise, we show that we can modify the colouring of to do this. In this case, we hence know that has an edge with colour to a node . Instead, colour that edge , and if already had an edge with colour , colour that , and if the neighbour already had an edge with colour , colour that , and so on. Since is finite and bipartite, this process will stop, and we know it will not stop at since that would imply has an edge with colour . This now means no longer has an edge with colour , meaning we can colour with and we are done.
]]></description><link>matematik/discrete-mathematics/edge-colouring.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/Edge colouring.md</guid><pubDate>Tue, 11 Feb 2025 10:05:06 GMT</pubDate></item><item><title><![CDATA[Graph colouring]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a> <br>Definition: Vertex colouring, chromatic number
A vertex colouring of a <a data-tooltip-position="top" aria-label="Graphs" data-href="Graphs" href="matematik/discrete-mathematics/graphs.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Graph</a><a data-tooltip-position="top" aria-label="Graphs" data-href="Graphs" href="matematik/discrete-mathematics/graphs.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Graph</a> is a function such that for every adjacent pair of vertices .
The chromatic number is the least integer for which there is a vertex colouring of using colours. Equivalently, a vertex colouring is a partition of the vertices such that no two vertices in the same partition are adjacent. A greedy way to find a vertex colouring is to color the first vertex with some color, then for each non-colored vertex, color it with an already used color if possible, otherwise a new color. Note that this algorithm does not garuantee a colouring with minimal colors.Theorem
Let be a graph with maximum degree . Then .<br>
Furthermore, if is connected and not <a data-tooltip-position="top" aria-label="Graphs > ^4f14d7" data-href="Graphs#^4f14d7" href="matematik/discrete-mathematics/graphs.html#^4f14d7" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Regular</a><a data-tooltip-position="top" aria-label="Graphs > ^4f14d7" data-href="Graphs#^4f14d7" href="matematik/discrete-mathematics/graphs.html#^4f14d7" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Regular</a>, then .
]]></description><link>matematik/discrete-mathematics/graph-colouring.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/Graph colouring.md</guid><pubDate>Mon, 10 Feb 2025 18:31:26 GMT</pubDate></item><item><title><![CDATA[Inre produktrum]]></title><description><![CDATA[<a href=".?query=tag:matematik/linjär-algebra" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik/linjär-algebra">#matematik/linjär-algebra</a> Ett inre produktrum är ett vanligt vektorrum över eller tillsammans med ett sätt att kombinera två vektorer och få ut ett tal från kroppen. Denna avbildning från två vektorer till ett tal kallar vi för den inre produkten, och vi sätter några speciella krav på den.Definition: Inre produkt En inre produkt på ett vektorrum med kroppen eller är en avbildning som uppfyller <br><a data-tooltip-position="top" aria-label="Linjära avbildningar" data-href="Linjära avbildningar" href="matematik/linjär-algebra/linjära-avbildningar.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Linjäritet</a><a data-tooltip-position="top" aria-label="Linjära avbildningar" data-href="Linjära avbildningar" href="matematik/linjär-algebra/linjära-avbildningar.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Linjäritet</a> i det andra argumentet: för alla och ,
Konjugatsymmetri: för alla ,
Positivt definit: och (alltså även ) för alla . <br>Vi märker att om kroppen är får vi att den inre produkten är <a data-tooltip-position="top" aria-label="Multilinjära avbildningar" data-href="Multilinjära avbildningar" href="matematik/linjär-algebra/multilinjära-avbildningar.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Bilinjär</a><a data-tooltip-position="top" aria-label="Multilinjära avbildningar" data-href="Multilinjära avbildningar" href="matematik/linjär-algebra/multilinjära-avbildningar.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Bilinjär</a> och symmetrisk, samt att om kroppen är så är den inre produkten så kallat seskvilinjär, alltså att den är linjär i det högra argumentet och antilinjär i det vänstra.Vi märker att eftersom den inre produkten är seskvilinjär bestäms den givet någon bas av värdena för . Vi ordnar dessa tal i en matris som vi kallar för metriken av vår inre produkt.Definition: Metrik
Om är ett inre produktrum med bas så är metriken med avseende på basen en konjugatsymmetrisk positivt definit matris som ges av Sats: Inre produkt från metrik
Om är ett inre produktrum med bas och metrik med avseende på , och så är
Bevis:
Vi vet att och . Då är Vi måste alltså ha kravet på att metriken ska vara konjugatsymmetrisk och positivt definit för att den ska kunna representera en inre produkt. Men förutom de kraven märker vi att vi kan välja talen helt själva, vilket ger oss följande sats.Sats: Metriken definierar inre produkten
Om är ett vektorrum och är en matris som är om , eller oändligt stor om är oändligdimensionellt, så definierar en inre produkt om och endast om är konjugatsymmetrisk och positivt definit.
Bevis:<br>
Låt vara en bas för . Vi visar först . Antag alltså att definierar en inre produkt. Då har vi att , samt att är positivt definit eftersom enligt <a data-tooltip-position="top" aria-label="^74283e" data-href="#^74283e" href="#^74283e" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Sats</a><a data-tooltip-position="top" aria-label="^74283e" data-href="#^74283e" href="#^74283e" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Sats</a> är positivt definit.
Nu visar vi . För är alltså och . Om vi alltså sätter följer då enligt tidigare att , och det följer också att , vilket ger att den inre produkten är konjugatsymmetrisk. Slutligen vet vi att den inre produkten är positivt definit eftersom vi sa att är det.
Vi visar även följande sats som är användbar i många bevis.Sats: för alla Om är ett inre produktrum och så är om och endast om för alla .
Bevis: är uppenbar, så kvar är . Linjäriteten ger att för alla . Välj så får vi ]]></description><link>matematik/linjär-algebra/inre-produktrum.html</link><guid isPermaLink="false">Matematik/Linjär algebra/Inre produktrum.md</guid><pubDate>Tue, 04 Feb 2025 16:32:33 GMT</pubDate></item><item><title><![CDATA[Congruence]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a> <br>Definition: Congruent, modulo
Let and . We say that is congruent to modulo and write if is <a data-tooltip-position="top" aria-label="Divisibility > ^920140" data-href="Divisibility#^920140" href="matematik/discrete-mathematics/divisibility.html#^920140" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Divisible</a><a data-tooltip-position="top" aria-label="Divisibility > ^920140" data-href="Divisibility#^920140" href="matematik/discrete-mathematics/divisibility.html#^920140" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Divisible</a> by .
<br>Note that congruence clearly is an <a data-tooltip-position="top" aria-label="Relations > ^f15c87" data-href="Relations#^f15c87" href="matematik/discrete-mathematics/relations.html#^f15c87" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Equivalence relation</a><a data-tooltip-position="top" aria-label="Relations > ^f15c87" data-href="Relations#^f15c87" href="matematik/discrete-mathematics/relations.html#^f15c87" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Equivalence relation</a> since if and then .Theorem
Let and such that and . Then and .
Proof:
We know and . This gives and Notation
We let denote the equivalence class of with respect to congruence modulo .
For example, . Also notice that since any can be expressed as for some , the unique equivalence classes are .Definition: Integers modulo is the set of distinct equivalence classes under the relation of congruence modulo in , so We also define the following operations: <br>From <a data-tooltip-position="top" aria-label="^5d0c4c" data-href="#^5d0c4c" href="#^5d0c4c" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Theorem</a><a data-tooltip-position="top" aria-label="^5d0c4c" data-href="#^5d0c4c" href="#^5d0c4c" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Theorem</a>, it follows easily that these operations are well-defined. Of course, since we transfer the operations to integer operations, the properties of these integer operations, like associativity, commutativity and additive inverses, apply here too. However, for example the rule that if then , does not apply here: we only get that meaning , which of course does not necessarily imply that .Definition: Invertibility
An element is invertible if there exists a such that . In this case, is called the multiplicative inverse or just inverse of . From the commutativity of multiplication we see immediately that is the inverse of , so for every invertible .Theorem is invertible if and only if and are coprime in .
Proof:
: We have for some , meaning . Any common divisor of and must divide , so .<br>
: By <a data-tooltip-position="top" aria-label="Greatest common divisor > ^d37e34" data-href="Greatest common divisor#^d37e34" href="matematik/discrete-mathematics/greatest-common-divisor.html#^d37e34" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Bézout's identity</a><a data-tooltip-position="top" aria-label="Greatest common divisor > ^d37e34" data-href="Greatest common divisor#^d37e34" href="matematik/discrete-mathematics/greatest-common-divisor.html#^d37e34" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Bézout's identity</a> there are such that , meaning , so in equivalence class terms, so .
We thus see that if is prime, then every element of except is invertible. Also, the number of invertible elements in is .Theorem
If is invertible, then .
Proof:
We let , and see that is invariant under multiplication with elements in ; we define , and then see that for any , , so, , meaning for every . Let be the invertible elements of , where . Since is invertible and , we see that meaning since is invertible.
Corollary: Euler's theorem, Fermat's little theorem
From the previous theorem it immediately follows that if , then , which is Euler's theorem. It also follows that if is prime, then , which is Fermat's little theorem.
]]></description><link>matematik/discrete-mathematics/congruence.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/Congruence.md</guid><pubDate>Tue, 04 Feb 2025 16:24:46 GMT</pubDate></item><item><title><![CDATA[Trees]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a> <br>Definition: Tree
A graph is a tree if it is <a data-tooltip-position="top" aria-label="Paths and cycles > ^df1fd8" data-href="Paths and cycles#^df1fd8" href="matematik/discrete-mathematics/paths-and-cycles.html#^df1fd8" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Connected</a><a data-tooltip-position="top" aria-label="Paths and cycles > ^df1fd8" data-href="Paths and cycles#^df1fd8" href="matematik/discrete-mathematics/paths-and-cycles.html#^df1fd8" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Connected</a>, and there are no <a data-tooltip-position="top" aria-label="Paths and cycles > ^1c4d2b" data-href="Paths and cycles#^1c4d2b" href="matematik/discrete-mathematics/paths-and-cycles.html#^1c4d2b" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Cycles</a><a data-tooltip-position="top" aria-label="Paths and cycles > ^1c4d2b" data-href="Paths and cycles#^1c4d2b" href="matematik/discrete-mathematics/paths-and-cycles.html#^1c4d2b" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Cycles</a> in .
Theorem
Let be a tree with at least two vertices. Then For every pair of vertices there is a unique path in from to .
The graph obtained from by removing any edge has two components, each of which is a tree.
. Proof: Take any . Since is connected, there exists a path from to . Now suppose is a different path from to . Then since and there exists such that for every but and , meaning is a cycle, contradicting that is a tree. Hence the path is unique.
Removing any edge creates two components: each of which consisting of the nodes whose paths to each other did not contain the removed edge. Clearly these are connected and no cycles can be introduced, so they are both trees.
Follows from (b); we induce on the number of edges. The base case is trivial. Now given , remove any edge, resulting in components and , each having strictly less vertices than . By our induction hypothesis, and , meaning ]]></description><link>matematik/discrete-mathematics/trees.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/Trees.md</guid><pubDate>Tue, 04 Feb 2025 16:18:57 GMT</pubDate></item><item><title><![CDATA[Paths and cycles]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a> Definition: Walk, path
A walk in a graph is a sequence of vertices such that and are adjacent for every . A walk where the vertices are distinct is called a path.
We notice that we can define a relation on which describes if there exists a path between two vertices. This clearly is an equivalence relation, since if you can get from to and from to , then you can get from to .Definition: Components, connected graph
Let be a graph and be the equivalence relation where there exists a path from to . Let be the partition of with respect to , and let for every . The graphs are called the components of , and if , is called connected.
Definition: Cycle
A walk is called a cycle if is a path and .
]]></description><link>matematik/discrete-mathematics/paths-and-cycles.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/Paths and cycles.md</guid><pubDate>Tue, 04 Feb 2025 15:30:54 GMT</pubDate></item><item><title><![CDATA[Latin squares]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a> Definition: Latin square
A latin square of order is an array in which each of symbols occur once in each row and once in each column. Typically, the symbol in row column is denoted .
Theorem
Let . The array defined by where is a latin square.
Proof:
Given any row and symbol we can let and then get that , and we know that for some , so the symbol will be in cell . Same goes for any column, so it is indeed a latin square.
Definition: Orthogonal latin squares
Two latin squares of the same order are orthogonal if for every ordered pair of symbols there is exactly one position such that and .
Theorem
Let be prime and be nonzero. Then where defines a latin square. Furthermore, if is nonzero and , then and are orthogonal.
Proof:<br>
Since is <a data-tooltip-position="top" aria-label="Congruence > ^c463cf" data-href="Congruence#^c463cf" href="matematik/discrete-mathematics/congruence.html#^c463cf" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Invertible</a><a data-tooltip-position="top" aria-label="Congruence > ^c463cf" data-href="Congruence#^c463cf" href="matematik/discrete-mathematics/congruence.html#^c463cf" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Invertible</a>, it follows from the proof of <a data-tooltip-position="top" aria-label="^f7759c" data-href="#^f7759c" href="#^f7759c" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Theorem</a><a data-tooltip-position="top" aria-label="^f7759c" data-href="#^f7759c" href="#^f7759c" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Theorem</a> that is a latin square. Furthermore, given a pair we have the system of equations where we used that meaning and , meaning both of these are invertible.
]]></description><link>matematik/discrete-mathematics/latin-squares.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/Latin squares.md</guid><pubDate>Tue, 04 Feb 2025 10:06:10 GMT</pubDate></item><item><title><![CDATA[Greatest common divisor]]></title><description><![CDATA[<a href=".?query=tag:matematik-groups" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-groups">#matematik-groups</a> <br>Definition: Greatest common divisor
Let . The greatest common divisor of and is where and are the <a data-tooltip-position="top" aria-label="Divisibility > ^920140" data-href="Divisibility#^920140" href="matematik/discrete-mathematics/divisibility.html#^920140" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Divisors</a><a data-tooltip-position="top" aria-label="Divisibility > ^920140" data-href="Divisibility#^920140" href="matematik/discrete-mathematics/divisibility.html#^920140" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Divisors</a> of and , respectively.
Theorem
Let . If there are such that , then Proof:
Let . We then have since , hence .
Theorem: Bézout's identity
Let , and . Then there exists integers such that Proof:
By repeating the theorem above, we get since it must end somewhere since the 's are strictly decreasing. This means that . Thus we have that and we can keep doing this until we get the form which means ]]></description><link>matematik/discrete-mathematics/greatest-common-divisor.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/Greatest common divisor.md</guid><pubDate>Tue, 04 Feb 2025 09:14:50 GMT</pubDate></item><item><title><![CDATA[Permutations]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a> Definition: Permutation
A permutation of a non-empty finite set is a bijection .
<br>We notice that by <a data-tooltip-position="top" aria-label="Words and selections > ^4683f7" data-href="Words and selections#^4683f7" href="matematik/discrete-mathematics/words-and-selections.html#^4683f7" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Theorem</a><a data-tooltip-position="top" aria-label="Words and selections > ^4683f7" data-href="Words and selections#^4683f7" href="matematik/discrete-mathematics/words-and-selections.html#^4683f7" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Theorem</a>, it follows that the number of permutations of a set of size is .Notation
For every we let Theorem If , then .
If , then .
If there is an such that . Proof:
Follows directly from the properties of bijections.
Definition: Cycle
A cycle is a permutation of given by where .
Notice that the starting number when denoting a cycle does not matter. Also note that every permutation can be written as the composition of cycles, such as ]]></description><link>matematik/discrete-mathematics/permutations.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/Permutations.md</guid><pubDate>Fri, 31 Jan 2025 12:03:52 GMT</pubDate></item><item><title><![CDATA[Partitions of integers]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a> Definition
Given an integer , a partition of into parts is a sum where all , and we write a partition as (where is omitted if ) where is the number of 's among .
]]></description><link>matematik/discrete-mathematics/partitions-of-integers.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/Partitions of integers.md</guid><pubDate>Fri, 31 Jan 2025 12:00:41 GMT</pubDate></item><item><title><![CDATA[Multinomial numbers]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a> Definition: Multinomial numbers
Let and . Also, let . Then the number of surjections such that where for every is denoted and is called a multinomial number.
<br>In this way, multinomial numbers are generalizations of <a data-tooltip-position="top" aria-label="Unordered selections > ^2ce857" data-href="Unordered selections#^2ce857" href="matematik/discrete-mathematics/unordered-selections.html#^2ce857" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Binomial numbers</a><a data-tooltip-position="top" aria-label="Unordered selections > ^2ce857" data-href="Unordered selections#^2ce857" href="matematik/discrete-mathematics/unordered-selections.html#^2ce857" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Binomial numbers</a>. In possibly easier terms, it is the number of ways to choose a sequence of sets where such that is the disjoint union of all 's.Theorem: Formula for multinomial numbers
Given and nonnegative integers such that , Proof:
Each possible sequence of sets can be done by choosing a permutation of all elements of and then letting contain the first elements, contain the following elements, and so on. There are ways of doing this, but each possible partition is then counted times since that's the number of ways to order the elements and still preserving the sets .
Theorem: Multinomial theorem
For any and , where the sum is over all possible such that .
Proof:
We consider a multiset of copies of , and we are thus interested in the product of these elements, and we see that every term in the result is of the form where , and that the coefficient for that term is the number of ways to choose exactly one term from from each factor such that in total, the number of chosen is precisely , which we see corresponds to the definition of the multinomial number.
]]></description><link>matematik/discrete-mathematics/multinomial-numbers.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/Multinomial numbers.md</guid><pubDate>Fri, 31 Jan 2025 11:42:20 GMT</pubDate></item><item><title><![CDATA[Partitions]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a> Definition: Index set
Let be a set, . Suppose for every we have a set . Then is called the index set for the family of sets .
Definition: Partition
A partition of a set is a family of nonempty sets such that is the disjoint union of all sets in . Each is called a part of the partition.
Definition: Stirling numbers
We define the Stirling numbers as the number of partitions of an -set into parts (where ).
Theorem: Formula for Stirling numbers
, and for every where , Proof:
Let . When , the only possible partition is , and when , the only possible partition is , since sets in the partition must be nonempty. Finally, when , we have two cases for a partition: is in its own set, or is not in its own set. If is in its own set, the remaining elements must be partitioned into sets, which there are ways of doing. If is not in its own set, it must belong to a set in a -partition of the remaining items, which there are ways of doing, and since for each such way there are sets to which can belong, we multiply this by and arive at the formula.
<br>Theorem: Equivalence classes form a partition
Let be an <a data-tooltip-position="top" aria-label="Relations > ^f15c87" data-href="Relations#^f15c87" href="matematik/discrete-mathematics/relations.html#^f15c87" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Equivalence relation</a><a data-tooltip-position="top" aria-label="Relations > ^f15c87" data-href="Relations#^f15c87" href="matematik/discrete-mathematics/relations.html#^f15c87" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Equivalence relation</a> on a set . Then the equivalence classes with respect to form a partition of .
Proof:<br>
Follows directly from <a data-tooltip-position="top" aria-label="Relations > ^715201" data-href="Relations#^715201" href="matematik/discrete-mathematics/relations.html#^715201" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Theorem</a><a data-tooltip-position="top" aria-label="Relations > ^715201" data-href="Relations#^715201" href="matematik/discrete-mathematics/relations.html#^715201" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Theorem</a>.
For any partition of , we can define a function by . We see that is surjective and well defined, by the definition of a partition. On the other hand, for any surjective map we can for every define which makes a partition of .Theorem
Let be the set of surjections from an -set to a -set . Then Proof:
A surjection from to is uniquely determined by the sets for every . These are partitions, and there are to create such partitions, and since each partition corresponds to exactly one element in , there are surjections for each such partition of .
<br>If we also restrict the size of each , we get the <a data-href="Multinomial numbers" href="matematik/discrete-mathematics/multinomial-numbers.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Multinomial numbers</a><a data-href="Multinomial numbers" href="matematik/discrete-mathematics/multinomial-numbers.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Multinomial numbers</a>.]]></description><link>matematik/discrete-mathematics/partitions.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/Partitions.md</guid><pubDate>Fri, 31 Jan 2025 11:41:42 GMT</pubDate></item><item><title><![CDATA[Relations]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a> Definition: Relation
Let be a set. A relation on is a subset of . If is a relation on , we define the statement for every .
For example, , and are relations on .Definition: Equivalence relation
Let be a relation on a set . We define the following properties of : Reflexivity: for every Symmetry: for every Transitivity: for every If is reflexive, symmetric, and transitive, we call it an equivalence relation.
Definition: Equivalence class
Let be an equivalence relation on a set , and let . Then is called an equivalence class with respect to if for every and for every . denotes the equivalence class of , and contains every such that .
Theorem
Let be an equivalence relation on a set . Then every element is contained in one and only one equivalence class of with respect to , namely .
Proof:
Suppose is an equivalence class of with respect to such that . Then we have , so .
]]></description><link>matematik/discrete-mathematics/relations.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/Relations.md</guid><pubDate>Fri, 31 Jan 2025 09:39:45 GMT</pubDate></item><item><title><![CDATA[Möbius function]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a> Definition: Möbius function
The Möbius function is Theorem
Let such that . Then Proof:
Let the prime factorization of be . Every nonzero contribution to the sum above comes from divisors of with non-repeated prime factors, so the product of a subset of . So our sum is Theorem: Möbius inversion formula
Let be functions of and let Then Proof:
We have where we see that the inner sum is zero unless , so we do indeed have that this equals .
]]></description><link>matematik/discrete-mathematics/möbius-function.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/Möbius function.md</guid><pubDate>Wed, 29 Jan 2025 15:05:18 GMT</pubDate></item><item><title><![CDATA[Euler's function]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a> Definition: Euler's function
Euler's function is given by We see immediately that if is prime.Theorem
For any , Proof:
Let . From the definition of we then have Our goal now is to show that , so we construct a bijection . We define for every , which is well-defined since , and since , we also have . We now show injectivity. If we have and since and are coprime we have and , so is injective. Finally we show surjectivity. Let , and let . Then let and . and must then be coprime, by the maximality of . We also have since , so , and we have so is also surjective, hence a surjection.
Theorem
Let such that and whose prime factorization is Then Proof:
Let . Then we have . A general intersection of these is , which thus contains multiples of , which there are of. So we have which can be verified is equal to the statement by carrying out the multiplications.
]]></description><link>matematik/discrete-mathematics/euler's-function.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/Euler's function.md</guid><pubDate>Wed, 29 Jan 2025 14:44:45 GMT</pubDate></item><item><title><![CDATA[The Sieve principle]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a> Theorem
If are finite sets, then where each is the sum of the sizes of the intersections of each subset of sets from .
Proof:<br>
Given an we want to show that contributes exactly to the sum. Suppose lies in exactly of the sets. Then contributes exactly to since that's the number of subsets of where each set contains (the intersection contains ). So to the total sum, contributes From the <a data-tooltip-position="top" aria-label="Unordered selections > ^39ea4b" data-href="Unordered selections#^39ea4b" href="matematik/discrete-mathematics/unordered-selections.html#^39ea4b" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Binomial theorem</a><a data-tooltip-position="top" aria-label="Unordered selections > ^39ea4b" data-href="Unordered selections#^39ea4b" href="matematik/discrete-mathematics/unordered-selections.html#^39ea4b" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Binomial theorem</a>, this is simply except it's missing the term , so contributes exactly to the total sum.
]]></description><link>matematik/discrete-mathematics/the-sieve-principle.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/The Sieve principle.md</guid><pubDate>Wed, 29 Jan 2025 14:09:45 GMT</pubDate></item><item><title><![CDATA[Counting]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a> Definition: We define .
Definition: Size of set
Let be a set. If there exists a bijection between and , we say that has size or cardinality , and write .
Theorem
Let . If there exists an injection from to , then .
Proof:<br>
We use <a data-tooltip-position="top" aria-label="Induction" data-href="Induction" href="matematik/discrete-mathematics/induction.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Induction</a><a data-tooltip-position="top" aria-label="Induction" data-href="Induction" href="matematik/discrete-mathematics/induction.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Induction</a> on . If the result is clear. Now suppose is an injection. Firstly, we know , so for some . We can then define an injection : If for every , then let for every .
If for some , let (we know because is injective), and let for every .
Then, from our induction hypothesis, we get that , so . Remark
The contrapositive of the theorem above is called the pidgeonhole principle and says that if then there does not exist a injection from to .
Theorem: Size is unique
Let be a set with size and . Then .
Proof:
There exists bijections and . This means and are injections from to and to , respectively, meaning and , so .
Theorem
Let be finite, nonempty, disjoint sets. Then Proof:
We have , and since we have meaning .
Corollary
Let be finite, nonempty, disjoint sets. Then ]]></description><link>matematik/discrete-mathematics/counting.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/Counting.md</guid><pubDate>Tue, 28 Jan 2025 10:20:21 GMT</pubDate></item><item><title><![CDATA[Sets]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a>Definition: Set
A set is an unordered collection of objects. We can construct in different ways, such as by a list: or by a property: If is an element of a set we write .
Notation
In this course we use the notation .
Definition: Subset
If are sets such that every element of is in , we call a subset of and write . If also , we call a proper subset of and write .
Remark
The expression in this course refers to .
Definition: Intersection, union
For any two sets the intersection of and is and the union of and is Definition: Cartesian product
For any two sets We define the Cartesian product ]]></description><link>matematik/discrete-mathematics/sets.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/Sets.md</guid><pubDate>Tue, 28 Jan 2025 10:08:33 GMT</pubDate></item><item><title><![CDATA[Rational numbers]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a> We don't have division yet, but know what we are aiming for; we know that once we have division, we can represent a rational number as for some (where ). This representation, however, is not unique; more specifically we have . We have defined multiplication, so we can make equivalence classes from this!Definition: Construction of the rational numbers
Let and define an equivalence relation so that . We then define the rational numbers as the set of equivalence classes of with respect to , and denote with respect to . We also define for any , so . We define addition as and multiplication as Finally, we define for every positive .
We now show that these definitions of operations on rational numbers are well-defined and sufficient. If and we get We want to show that , which is equivalent to which we know from the previous equalities. Hence, addition is well-defined. If and we get and , so multiplication is also well-defined.Now, for addition we get so addition is sufficiently defined. For multiplication, we first note that meaning This means that in general we get so multiplication is also sufficiently defined.We also see that Theorem: Density of rational numbers
Let where . Then there exists a such that .
Proof:
Let . Then we have and .
Theorem
A decimal number terminates or repeats it corresponds to a rational number.
Definition: Irrational number, real number
A decimal number that does not terminate or repeat we say correspond to an irrational number. Any number that is rational or irrational we call real, and we let the set of all real numbers be informally defined as When we introduced we added subtraction, when we introduced we added division, and when we have we have added for example roots.We will now consider sequences of numbers. Let , and consider a sequence of numbers in .Definition: Strictly increasing, strictly decreasing
A sequence of numbers is said to be strictly increasing if for every , or strictly decreasing if for every .
We see that if , there are no strictly decreasing infinite sequences, since it must reach and then can't continue. But if , there are of course strictly decreasing infinite sequences, such as Definition: Greatest lower bound
For a set of numbers , is a greatest lower bound if it is a lower bound of and for every lower bound .
We see that for example the sequence of rational numbers is strictly decreasing yet still has a greatest lowest bound in , namely . However, we can construct a strictly decreasing sequence of rational numbers, that has a lower bound but not a greatest lower bound in . For example, the recursive sequence is strictly decreasing but approaches . So for any rational lower bound, we can always find a greater rational lower bound. With real numbers, the problem is solved.Theorem
A bounded, strictly decreasing sequence of real numbers has a greatest lower bound in .
<br>Theorem
The set of rational numbers is <a data-tooltip-position="top" aria-label="Infinite sets > ^641bc9" data-href="Infinite sets#^641bc9" href="matematik/discrete-mathematics/infinite-sets.html#^641bc9" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Countable</a><a data-tooltip-position="top" aria-label="Infinite sets > ^641bc9" data-href="Infinite sets#^641bc9" href="matematik/discrete-mathematics/infinite-sets.html#^641bc9" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Countable</a>.
Theorem
The set of real numbers is uncountable.
Proof:
It is enough to prove that the set is uncountable (the set of all nonnegative real numbers with integer part ). Suppose this set is countable. Then we have a map from to , so . We can then create a real number with integer part and with decimal being any decimal different from decimal of , for every . is thus different from every , but , which is a contradiction.
Lemma
For any such that , there exists an such that .
Proof:
Let . We know and that is a lower bound of , meaning has a minimal element (so but ). This means and , so .
Theorem
If such that , there exists an such that .
Proof:<br>
Let . Then there exists an such that . This means that which by <a data-tooltip-position="top" aria-label="^1a367a" data-href="#^1a367a" href="#^1a367a" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Lemma</a><a data-tooltip-position="top" aria-label="^1a367a" data-href="#^1a367a" href="#^1a367a" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Lemma</a> means there exists an so that . This means .
Theorem: Archimedean property
If where , there exists an such that .
Proof:
The set does not have an upper bound.
]]></description><link>matematik/discrete-mathematics/rational-numbers.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/Rational numbers.md</guid><pubDate>Fri, 24 Jan 2025 13:52:39 GMT</pubDate></item><item><title><![CDATA[Prime numbers]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a> Definition: Coprime
Let . If , are called coprime.
<br>From <a data-tooltip-position="top" aria-label="Greatest common divisor > ^d37e34" data-href="Greatest common divisor#^d37e34" href="matematik/discrete-mathematics/greatest-common-divisor.html#^d37e34" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Bézout's identity</a><a data-tooltip-position="top" aria-label="Greatest common divisor > ^d37e34" data-href="Greatest common divisor#^d37e34" href="matematik/discrete-mathematics/greatest-common-divisor.html#^d37e34" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Bézout's identity</a> we thus have that for any coprime numbers there are such that .Theorem
Let . If and are coprime and , then .
Proof:
We know there are such that . Multiply by and we get Since , divides the left hand side, so it also divides .
Definition: Prime number
A number is prime if and are the divisors of . If a number is not prime, it is called composite.
Theorem
Let be prime and such that . Then for some .
Proof:<br>
We induct on . If the theorem is clear. Now suppose the theorem is true for some , and take numbers such that . If we are done. Otherwise, since is prime, so by <a data-tooltip-position="top" aria-label="^4dd604" data-href="#^4dd604" href="#^4dd604" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Theorem</a><a data-tooltip-position="top" aria-label="^4dd604" data-href="#^4dd604" href="#^4dd604" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Theorem</a>, which by our induction hypothesis means divides one of . Theorem: Fundamental Theorem of Arithmetic
Let where . Then can be written uniquely (up to permutation) as a product of prime numbers.
Proof:<br>
We use induction. We know the theorem is true for . Now suppose the theorem is true for every . If is prime, the theorem is also true for . Otherwise, by definition where and . It follows that , so both and can be written as products of prime numbers by our induction hypothesis. Since , we have proven the existence of a prime factorization for every .
Now suppose there are numbers with non-unique prime factorizations. Let be the least such integer. Then for primes and . We know , which by <a data-tooltip-position="top" aria-label="^7c4304" data-href="#^7c4304" href="#^7c4304" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Theorem</a><a data-tooltip-position="top" aria-label="^7c4304" data-href="#^7c4304" href="#^7c4304" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Theorem</a> means divides some , WLOG, let it be . Since they are both prime, we know . Thus , which contradicts the minimality of .
]]></description><link>matematik/discrete-mathematics/prime-numbers.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/Prime numbers.md</guid><pubDate>Fri, 24 Jan 2025 10:30:12 GMT</pubDate></item><item><title><![CDATA[Representations of integers]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a> Theorem
Let and where . Then for some unique and such that .
Proof:
We repeatedly divide by and let the remainders be our 's. Specifically, we have where . We know that such an exists since the 's get strictly smaller every step, since . This gives and we know each satisfies . Every is also unique, so too is unique.
]]></description><link>matematik/discrete-mathematics/representations-of-integers.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/Representations of integers.md</guid><pubDate>Thu, 23 Jan 2025 15:16:37 GMT</pubDate></item><item><title><![CDATA[Divisibility]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a> Definition: Multiple of , divisibility is a multiple of , denoted , if there exists a such that . If is a multiple of , we say that is a divisor or factor of , and we say that divides or that is divisible by . Notation
We let .
Theorem
Let . Then there exists unique such that where .
Proof:
Let . We notice that if we get , so . Thus has a least element with corresponding such that . We have so if then , so . But clearly , so then is not a least element of which is a contradiction. Hence . Since the least element is unique, both and are unique.
Terminology
For any , if where and , is called the quotient and the remainder.
]]></description><link>matematik/discrete-mathematics/divisibility.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/Divisibility.md</guid><pubDate>Thu, 23 Jan 2025 15:14:37 GMT</pubDate></item><item><title><![CDATA[Integers]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a> <br>We don't have subtraction yet, but know what we are aiming for; we know that once we have subtraction we can represent any integer as where . However, there are many ways express the same integer this way, so it makes sense to group together all possible ways to express the same integer into <a data-tooltip-position="top" aria-label="Relations > ^d3a7bb" data-href="Relations#^d3a7bb" href="matematik/discrete-mathematics/relations.html#^d3a7bb" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Equivalence classes</a><a data-tooltip-position="top" aria-label="Relations > ^d3a7bb" data-href="Relations#^d3a7bb" href="matematik/discrete-mathematics/relations.html#^d3a7bb" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Equivalence classes</a>. We know that , which is a relation solely on natural numbers! We also see that the relation is reflexive, , symmetric, , and transitive, so it is indeed an equivalence relation, meaning equivalence classes make sense.Definition: Construction of the integers
Let , and define an equivalence relation on by The set of integers is the set of equivalence classes of with respect to . We define addition and multiplication on integers as follows: We also define and for every . For every we denote with .
Theorem and for every .
Proof:
We have , so for every . We also have , so for every .
Theorem
If for some where , then .
Proof:
We have . Since , we have either or for some . If , then meaning giving . The proof is analogous for if .
Definition: Less than, greater than
Let . We say that is less than , denoted , if where and . If is less than we say that is less than and write .
Theorem
If such that and such that , then .
Proof:
We have where . This means We thus want to show that which is true because we know and .
Definition: Lower bound, upper bound
For a subset , an integer is a lower bound if for every , and an integer is an upper bound if for every .
Notice that since and do not need to be in , they are not unique.Theorem
If a nonempty set has a lower bound, it has a least element.
Proof:
Let be a lower bound for , and let Thus for every we have , so meaning has a least element . We can then simply let to get , and we then have that for every , so , so is a least element of .
]]></description><link>matematik/discrete-mathematics/integers.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/Integers.md</guid><pubDate>Thu, 23 Jan 2025 14:23:41 GMT</pubDate></item><item><title><![CDATA[Induction]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a> Axiom: Principle of induction
Let be a statement for every . If and if for every , then for every . Terminology
In an inductive proof, we call the induction basis, the induction hypothesis and the induction step.
Note that for an inductive proof to work, there only needs to exist a bijective map between the sequence of statements and the natural numbers. For example, if we want to prove that is true for every , then we can of course use induction here too. To use the principle of induction, we would for example let , but this is not a step one would actually take in a proof.Theorem
Every number can be written as for some number of ones, that we call .
Proof:
We use induction. The statement is obviously true for . Now, if can be written as , then can be written as the sum of one more , so the statement is also true for . Hence, it is true for every .
]]></description><link>matematik/discrete-mathematics/induction.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/Induction.md</guid><pubDate>Wed, 22 Jan 2025 15:27:45 GMT</pubDate></item><item><title><![CDATA[Minimality and maximality]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a> Definition: Least and greatest member
Let . An element is called a least member if for every , and an element is called a greatest member if for every .
Theorem
Every nonempty subset of has a least member.
Proof:<br>
Since is nonempty, by <a data-tooltip-position="top" aria-label="Induction > ^6a7f31" data-href="Induction#^6a7f31" href="matematik/discrete-mathematics/induction.html#^6a7f31" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Theorem</a><a data-tooltip-position="top" aria-label="Induction > ^6a7f31" data-href="Induction#^6a7f31" href="matematik/discrete-mathematics/induction.html#^6a7f31" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Theorem</a>, we can sum until we reach a number in , which must be a least member of .
]]></description><link>matematik/discrete-mathematics/minimality-and-maximality.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/Minimality and maximality.md</guid><pubDate>Wed, 22 Jan 2025 14:02:45 GMT</pubDate></item><item><title><![CDATA[The logical framework]]></title><description><![CDATA[<a href=".?query=tag:matematik-discmath" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-discmath">#matematik-discmath</a> Definition: Statement
A statement is a declarative sentence that is either true, denoted by or false, denoted by .
Definition: Logical operations
For any statements , we define the following statements: pronounced "not " is if , otherwise . pronounced " and " is if both and , otherwise . pronounced " or " is if either or , otherwise . pronounced " implies " is if and not , otherwise . pronounced " if and only if " is if either both and , or neither nor , otherwise . If , we call and logically equivalent. The definition of deserves a small discussion. This definition feels weird since we are not used to " implies " being a logical statement in the same sense as " and ", but of course, it is either true or false, so it is a logical statement. What we need to keep in mind is that we are often not interested in knowing if , but rather in proving . So we often know that the statement is true, without knowing whether or are true. Then if we succeed in proving , we also know .Remark
We can define Theorem: Contrapositive
For any two statements , Proof:
We have So proving that can be done by proving that , which is often useful. The statement is called the contrapositive of the statement , and the theorem above thus shows that these are logically equivalent.]]></description><link>matematik/discrete-mathematics/the-logical-framework.html</link><guid isPermaLink="false">Matematik/Discrete mathematics/The logical framework.md</guid><pubDate>Wed, 22 Jan 2025 13:37:16 GMT</pubDate></item><item><title><![CDATA[Normal subgroups]]></title><description><![CDATA[<a href=".?query=tag:matematik-groups" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-groups">#matematik-groups</a> Definition: Normal subgroup
Let be a group and be a subgroup. Then is a normal subgroup if for every , , i.e. for every , , and we write Remark
Let be a group and be a subgroup. Then the following are equivalent: For every , For every , For every , Proof:
(i)(iv): Clear since .
(i)(ii): Clear since .
(iii)(iv): We have meaning and . Since the same applies for , we are done.
(ii)(iii): Since every is bijective, we have (ii) for every , for every , for every , since is a group.
Theorem: Kernels are normal subgroups
Let be a homomorphism. Then .
Proof:
For every and every we have so .
Remark For any group , the trivial subgroup is normal.
If is abelian, then every subgroup is normal since . Theorem
Let be a group and . Then for every . is a group with respect to the multiplication in a), with unit element .
The canonical projection defined by for every , is a surjective group homomorphism with . Proof:
a) Since is normal, we have and , so b) For any we have , so inverses exist.
c) It is surjective by definition. It is a homomorphism since . Finally, we have since if then .
From this we see that there is a strong correlation between kernels and normal subgroups; every kernel is a normal subgroup and every normal subgroup is the kernel of some homomorphism.Theorem: Universal property of quotient groups
Let be a group homomorphism and such that . Then there is a unique group homomorphism such that . Moreover, is injective if and only if . Proof:
Define by . This is well-defined since if then , so . This also shows uniqueness of , since it must satisfy .
a) Clear by definition of .
b) .
c) Follows from b).
d) We know is injective iff , iff .
Corollary
If is a surjective group homomorphism, then is canonically isomorphic to .
Theorem: First isomorphism theorem
Let be a group, , and . Then: The canonical homomorphism is an isomorphism. Proof:
c) Clear because .
b) Since is normal we have which implies so is closed under composition, and we also have , so , so is closed under inverses, meaning .
a) We construct a homomorphism by which is surjective and .
d) From the universal property we get a bijection .
Theorem: Second isomorphism theorem
Let be a group, and such that . Then The canonical homomorphism is an isomorphism. Proof:
a) Clear since .
b) We define as . This is well defined since if then so . is also surjective, and so .
c) From the universal property we get a bijection .
]]></description><link>matematik/groups-and-rings/lecture-notes/normal-subgroups.html</link><guid isPermaLink="false">Matematik/Groups and rings/Lecture notes/Normal subgroups.md</guid><pubDate>Wed, 22 Jan 2025 09:13:48 GMT</pubDate></item><item><title><![CDATA[Direct groups]]></title><description><![CDATA[<a href=".?query=tag:matematik-groups" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-groups">#matematik-groups</a>Definition: Direct product
Let be a family of groups. We define the direct product of these groups as the group given by the set where denotes a disjoint union, meaning we preserve the information about which group each element comes from. The operation of the direct product is defined componentwise as for all .
We want to preserve the information about which group each element comes from because we want to, given two elements, know which operation should be used on them. This definition allows for index sets of any size and cardinality. If the family of groups is of finite size we can see each element of the product as an -tuple, and if the index set is countably infinite we can see each element of the product as an infinite tuple, just like with regular set products.Notation
If each group in a family is the same, so that for all , we denote the product as . If we denote the product as .
]]></description><link>matematik/groups-and-rings/lecture-notes/direct-groups.html</link><guid isPermaLink="false">Matematik/Groups and rings/Lecture notes/Direct groups.md</guid><pubDate>Tue, 21 Jan 2025 17:30:57 GMT</pubDate></item><item><title><![CDATA[Groups]]></title><description><![CDATA[<a href=".?query=tag:matematik-groups" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik-groups">#matematik-groups</a>Definition: Properties
Let be a set together with a map .
Associativity: for all Unit element: There exists such that for all Inverse elements: For all there exists such that Commutativity: for all Definition
Let be a set together with a map . Then is called a...
Semigroup if it is associative
Monoid if it is associative and has a unit element
Group if it is associative, has a unit element, and has inverse elements
Abelian group if it is a commutative group
Notation
As associativity holds, we can simply write without parentheses. We also commonly define for all , and if a unit element exists. Finally, we often write as .
Theorem: Basic properties of groups
Let be a group. Then the following holds: The unit element is unique.
The inverse of any is unique (denoted by ). for all .
If or , then . Proof: If and are unit elements, then by the properties of unit elements.
If and are inverses of , then .
We know , so is the inverse of .
If , then , and analogous if . Theorem: Left/right unit &amp; inverse is sufficient
Let be a set with a map that satisfies is associative has a left unit such that for all . has left inverses; for all there exists a such that . Then, is a group.
Proof:
We first show that if is the left inverse of , it is also the right inverse of . We have , where the last step follows by applying the left inverse of on both sides. Hence, inverses exist. We also get , so is indeed also a left unit.
Examples is an abelian group.
If is a field, is an abelian group.
If is a field and is a vector space over , is an abelian group.
If is a field, is an abelian group. and are abelian groups.
, , and are commutative monoids, but not groups. ]]></description><link>matematik/groups-and-rings/lecture-notes/groups.html</link><guid isPermaLink="false">Matematik/Groups and rings/Lecture notes/Groups.md</guid><pubDate>Thu, 16 Jan 2025 13:16:43 GMT</pubDate></item><item><title><![CDATA[Ortogonalitet och projektioner]]></title><description><![CDATA[<a href=".?query=tag:matematik/linjär-algebra" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik/linjär-algebra">#matematik/linjär-algebra</a> <br><a data-href="Inre produktrum" href="matematik/linjär-algebra/inre-produktrum.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Inre produktrum</a><a data-href="Inre produktrum" href="matematik/linjär-algebra/inre-produktrum.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Inre produktrum</a> ger oss ett sätt att definiera vad som menas med att två vektorer är ortogonala, alltså vinkelräta mot varandra.Definition: Ortogonalitet
Om där är ett inre produktrum och så kallar vi och ortogonala mot varandra och skriver . Vi kallar en mängd vektorer ortogonal om vektorerna är parvis ortogonala.
Definition: Ortogonal och ortonormal bas
En ortogonal bas till ett inre produktrum är en bas som är ortogonal. Om även för alla så kallas basen ortonormal.
En mycket användbar egenskap hos ortonormala baser är att vi enkelt kan få fram koordinatvektorer.Sats: Koordinatvektorer från ortonormal bas
Om har en ortonormal bas och så är
Bevis:
Vi vet att det finns tal s.a. samt . Detta ger att
eftersom är om och annars ( är ortonormal).
Vi kan använda detta för projektioner.Definition: Projektion
Om är en idempotent linjär avbildning, alltså om , så kallas för en projektion på delrummet .
Idempotensen är geometriskt intuitiv eftersom projektion av en vektor borde ta vektorn till ett visst delrum på vilket projektionen inte gör något.Definition: Ortogonala delrum
Om och är delrum till ett inre produktrum och och är ortogonala för alla och så kallas och för ortogonala och vi skriver .
Definition: Ortogonalt komplement
Om är ett delrum av ett inre produktrum så är det ortogonala komplementet ett delrum som ges av Definition: Ortogonal projektion
Om är en projektion och så kallas för en ortogonal projektion på delrummet .
Sats: Ortogonal projektion finns
Låt vara ett delrum till . Då är om och endast om det finns en ortogonal projektion på . Då är den ortogonala projektionen på även unikt bestämd av att är identiteten på och noll på .
Bevis:
Om så kan vi definiera till att vara identiteten på och noll på , och då är en ortogonal projektion på . Om istället det finns en ortogonal projektion på så är för alla och vi vet att medan alltså är , och vi vet att . Eftersom är därmed .
Till slut visar vi att är unik. Tag ett . Då är , vilket ger att eftersom . Men vi vet att , alltså är . Därmed måste den ortogonala projektionen vara identiteten på och på , och är då unikt bestämd eftersom .
<br>Sats: Ortogonal projektion Självadjungerad projektion
En projektion är en ortogonal projektion om och endast om den är <a data-tooltip-position="top" aria-label="Adjungerade avbildningar > ^433201" data-href="Adjungerade avbildningar#^433201" href="matematik/linjär-algebra/adjungerade-avbildningar.html#^433201" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Självadjungerad</a><a data-tooltip-position="top" aria-label="Adjungerade avbildningar > ^433201" data-href="Adjungerade avbildningar#^433201" href="matematik/linjär-algebra/adjungerade-avbildningar.html#^433201" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Självadjungerad</a>.
Bevis:
Vi visar först . Antag alltså att är en ortogonal projektion. För alla så är . För alla är därmed
och på samma sätt är eftersom den inre produkten respekterar addition även i första argumentet, alltså är , så är självadjungerad.
Nu visar vi . Antag alltså att är självadjungerad. Vi får då att för alla så är
för alla , alltså är .
Sats: Projektion med ändlig ortonormal bas
Om är ett inre produktrum och är ett delrum med ändlig ortonormal bas så ges den ortogonala projektionen på av
för alla .
Bevis:<br>
Om så är enligt <a data-tooltip-position="top" aria-label="^299a88" data-href="#^299a88" href="#^299a88" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Sats</a><a data-tooltip-position="top" aria-label="^299a88" data-href="#^299a88" href="#^299a88" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Sats</a>, alltså är och , så är en projektion på . Vi har även att om så är för alla eftersom alla är linjärt oberoende. För alla finns då tal s.a. vilket ger att
alltså är , så är en ortogonal projektion.
^a63929
Från detta följer alltså att den ortogonala projektionen på alltid finns om är ändligdimensionellt. Tillsammans med tidigare sats följer även att om är ändligdimensionellt.Sats: Uppräknelig bas ger ortogonal bas
Om är ett inre produktrum och har en uppräknlig bas så finns en ortogonal bas till .
Bevis:
Vi kan använda Gram-Schmidt för att göra om den uppräkneliga basen till en ortogonal bas. Alltså, antag att är en bas till . Vi kallar vektorerna i vår ortogonala bas för . För alla låter vi och vara projektionen på . Välj först och definiera sedan för alla Vårt induktionsantangande är att är en ortogonal bas till . Vi vet att , alltså är , vilket ger att spänner upp och måste vara linjärt oberoende eftersom dimensionen ökade när vi lade till . Slutligen är , alltså är , alltså är , så för alla , så är per induktion en ortonormal bas för för alla .
Vi vet att spänner upp eftersom alla kan skrivas som en linjärkombination av ett ändligt antal basvektorer med något största index , och då är , alltså spänns upp av , så spänner upp . Vi vet även att mängden är linjärt oberoende eftersom, på samma sätt, alla linjärkombinationer av basvektorer i har ett ändligt antal termer med något största index , och vi har tidigare visat att är linjärt oberoende. Vi har även visat att för alla så är , så är en ortogonal bas till . ]]></description><link>matematik/linjär-algebra/ortogonalitet-och-projektioner.html</link><guid isPermaLink="false">Matematik/Linjär algebra/Ortogonalitet och projektioner.md</guid><pubDate>Sat, 11 Jan 2025 17:48:07 GMT</pubDate></item><item><title><![CDATA[Tensorer]]></title><description><![CDATA[<a href=".?query=tag:matematik/linjär-algebra" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik/linjär-algebra">#matematik/linjär-algebra</a><br>Ofta i linjär algebra stöter vi på avbildningar som inte är linjära, utan <a data-tooltip-position="top" aria-label="Multilinjära avbildningar" data-href="Multilinjära avbildningar" href="matematik/linjär-algebra/multilinjära-avbildningar.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Multilinjära</a><a data-tooltip-position="top" aria-label="Multilinjära avbildningar" data-href="Multilinjära avbildningar" href="matematik/linjär-algebra/multilinjära-avbildningar.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Multilinjära</a>, d.v.s linjära i varje argument. För att notationen inte ska bli alltför överflödig betraktar vi en bilinjär avbildning . Ibland vore det bra om vi kunde "göra om" denna bilinjära avbildning till en linjär avbildning, eftersom det ändå är linjära avbildningar vi studerar i linjär algebra.<br>Vår strategi blir att införa ett nytt vektorrum som vi kallar , som vi vill använda som "mellanhand" mellan och . När vi genom en bilinjär avbildning går från till vill vi alltså istället gå från till , och sedan från till . Vi säger att avbildningen från till kallas och är bilinjär, och att avbildningen från till kallas och är <a data-tooltip-position="top" aria-label="Linjära avbildningar" data-href="Linjära avbildningar" href="matematik/linjär-algebra/linjära-avbildningar.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Linjär</a><a data-tooltip-position="top" aria-label="Linjära avbildningar" data-href="Linjära avbildningar" href="matematik/linjär-algebra/linjära-avbildningar.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Linjär</a>.Men då känns det ju faktiskt som att vi inte kommit någonstans alls; vi har bara delat upp vår bilinjära avbildning i en bilinjär och en linjär ! Men det speciella är att genom att välja vektorrummet och den bilinjära avbildningen på rätt sätt, kan vi se till att och fungerar som mellanhand för alla ! I den meningen har vi alltså överfört en bilinjär avbildning på en linjär, eftersom det enda som beror av den bilinjära avbildningen är den linjära avbildningen . Vi lägger även till kravet att ska vara unik givet , så att varje bilinjära avbildning överförs på en unik linjär avbildning. Vi kallar vektorrummet tillsammans med den bilinjära avbildningen för en tensorprodukt av och .Anmärkning
Begreppet tensorprodukt används ganska slarvigt, men refererar alltså i sin tydligaste mening till vektorrummet tillsammans med den bilinjära avbildningen . Notera här att symbolen i inte är den bilinjära avbildningen , utan betraktas bäst bara som en symbol för ett vektorrum. För att vara tydlig kan vi alltså skriva en tensorprodukt av och som . Begreppet refererar dock ofta till endast vektorrummet , eller till en vektor i .
Men hur väljer vi då och "på rätt sätt"? Det är inte uppenbart, men vi behöver inte ge en entydig definition av vad vektorrummet och den bilinjära avbildningen är, utan vi kan nöja oss med att ge en rad krav för att ett visst vektorrum och bilinjär avbildning utgör en tensorprodukt av och .Definition: Tensorprodukt
Tensorprodukten av två vektorrum och är ett vektorrum tillsammans med en bilinjär avbildning , så att det för alla vektorrum och alla bilinjära avbildningar finns en unik linjär avbildning sådan att , det vill sägaför alla och .
Denna definition kallas för den universella egenskapen för tensorprodukten, och är ett av flera sätt att definiera tensorprodukten. Vi säger alltså inte explicit vad och är för något, utan endast att det är ett vektorrum och bilinjär avbildning som uppfyller vissa krav.Ett skäl till att vi lägger till kravet att ska vara unik är för att se till att är precis så stort det behöver vara för sitt syfte, nämligen att överföra bilinjära avbildningar på linjära. Vi kan förtydliga detta med en sats.Sats: spänner upp .
Om och är vektorrum och är en tensorprodukt av och så är Bevis:
Antag att och att . Då kan vi välja ett vektorrum med fler än en vektor, samt en bilinjär avbildning . Eftersom är en tensorprodukt finns då ett unikt linjärt så att för alla , men eftersom så sätter detta inte ett krav på . Vi kan alltså genom avbilda på godtycklig vektor i , så då är inte unikt bestämd, vilket motsäger definitionen.
Men för att gå tillbaka till definitionen finns ju alltså flera möjliga vektorrum som vi får kalla . Det kanske känns förvirrande, men just att definitionen är en universell egenskap är viktigt. Den säger nämligen att alla möjliga vektorrum som uppfyller definitionen är isomorfa med unik isomorfi. Det betyder att det givet två möjliga sådana vektorrum finns ett unikt sätt att para ihop alla vektorer i vektorrummen med varandra, alltså kan vi se alla dessa möjliga tensorprodukter av och som samma, bara att vi har bytt namn på varje vektor. Vi sammanfattar detta i en sats.Sats: Tensorprodukten är unik upp till unik isomorfi
Om samt är möjliga tensorprodukter av och så finns en unik isomorfi mellan och .
Bevis:
Enligt definitionen finns unika linjära avbildningar och sådana att vilket ger attoch då måste och vara identiteterna på respektive . Eftersom de är linjära och och enligt tidigare spänner upp respektive måste de även vara identiteterna på respektive , alltså är en isomorfi mellan och . Eftersom de är unikt bestämda måste även isomorfin vara unikt bestämd.
Detta visar alltså att vår ursprungliga definition faktiskt är en universell egenskap, och säger att alla möjliga tensorprodukter mellan och kan ses som samma eftersom skillnaden helt enkelt kan ses som ett namnbyte av varje vektor.Men vi har ju dock aldrig visat att tensorprodukter finns; vi kanske gav en definition som är omöjlig att uppfylla. Som tur är ska det visa sig att så inte är fallet. Vi börjar med att titta på vad som händer när vi inför baser till och .Sats: Tensorprodukt med baser
Låt och vara baser för respektive . Låt även vara ett vektorrum och vara en bilinjär avbildning. Då är en tensorprodukt av och om och endast om är en bas för .
Bevis:
Vi börjar med att visa . Antag alltså att är en tensorprodukt av och . Givet och finns tal så att och . Det ger att alltså spänner upp vilket spänner upp . För att visa linjärt oberoende utnyttjar vi att, som vi precis sett, en bilinjär avbildning bestäms unikt av alla . Vi kan därmed välja och så att alla är linjärt oberoende, vilket ger att den motsvarande linjära avbildningen uppfyller att alla är linjärt oberoende, vilket ger att alla är linjärt oberoende. Därmed är en bas.
Vi visar nu . Antag alltså att är en bas för . Givet en bilinjär avbildning vet vi då att den motsvarande linjära avbildningen unikt måste bestämmas av att
för alla och , eftersom vi antagit att alla utgör en bas. Då gäller att för alla och , alltså är och är unikt bestämd, så mycket riktigt är en tensorprodukt av och .
Från detta följer att om och är ändligdimensionella så ärVi noterar att även om vi här fortfarande inte givit ett exempel på en möjlig explicit definition av en tensorprodukt har vi ändå visat existens, eftersom vi nu bara kan välja ett vektorrum som är av "rätt storlek" så att vi kan kalla varje basvektor i en bas till det rummet för .Exempel
En möjlig tensorprodukt av med är . Detta eftersom är en bas till och därför är en bas till . Vi kan då till exempel låta , , , där högerleden alltså är enhetsvektorerna till .
Om både och är ändligdimensionella kan vi ge en explicit definition av en tensorprodukt med hjälp av bilinjära former, d.v.s bilinjära avbildningar till kroppen .Sats: Tensorprodukt med bilinjära former
Om och är ändligdimensionella vektorrum över kroppen är en möjlig tensorprodukt med .
Bevis:
Att avbildningarna från ligger i och att är bilinjär framgår av att multiplikation i en kropp är bilinjärt och att är linjära. Låt nu och vara baser för respektive . Vi vill visa att alla utgör en bas till . Vi visar först att de spänner upp . Vi har attEftersom och är ändligdimensionella finns duala baser och till respektive . Alltså bestäms alla unikt av alla . Vi ser alltså attför alla och konstanter . Tag nu ett . Genom att välja alla ser vi då att alla spänner upp , och genom att välja alla ser vi att alla är linjärt oberoende, alltså är de en bas till .
Vi såg tidigare att om och är ändligdimensionella så är
Vi vet ju att linjära avbildningar från till bestäms unikt av stycken tal som vi kan ordna i en matris, så en naturlig fråga är hur detta relaterar till tensorer. I nästa sats ser vi att om är ändligdimensionellt ( får vara oändligdimensionellt) så kan vi välja som rummet av alla linjära avbildningar från till , alltså så att är en linjär avbildning från till !Sats: Om och är vektorrum och är ändligdimensionellt så är en möjlig tensorprodukt
med .
Bevis:
Att är bilinjär framgår av att multiplikation i en kropp är bilinjärt, samt att alla är linjära. Välj nu baser och för respektive . Då finns en dual bas , och vi vill då visa att är en bas till . Vi ser att och att detta är en bas för eftersom alla kan skrivas entydigt som en matris så att ]]></description><link>matematik/linjär-algebra/tensorer.html</link><guid isPermaLink="false">Matematik/Linjär algebra/Tensorer.md</guid><pubDate>Sat, 11 Jan 2025 09:20:13 GMT</pubDate></item><item><title><![CDATA[Alternerande och skevsymmetriska avbildningar]]></title><description><![CDATA[<a href=".?query=tag:matematik/linjär-algebra" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik/linjär-algebra">#matematik/linjär-algebra</a> <br>Definition: Alternerande multilinjär avbildning
En <a data-tooltip-position="top" aria-label="Multilinjära avbildningar" data-href="Multilinjära avbildningar" href="matematik/linjär-algebra/multilinjära-avbildningar.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Multilinjär avbildning</a><a data-tooltip-position="top" aria-label="Multilinjära avbildningar" data-href="Multilinjära avbildningar" href="matematik/linjär-algebra/multilinjära-avbildningar.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Multilinjär avbildning</a> kallas alternerande om
för alla och vektorer .
<br>Vi ser att definitionen inte är identisk med den för <a data-tooltip-position="top" aria-label="Symmetriska och alternerande tensorer" data-href="Symmetriska och alternerande tensorer" href="matematik/linjär-algebra/symmetriska-och-alternerande-tensorer.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Alternerande tensorer</a><a data-tooltip-position="top" aria-label="Symmetriska och alternerande tensorer" data-href="Symmetriska och alternerande tensorer" href="matematik/linjär-algebra/symmetriska-och-alternerande-tensorer.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Alternerande tensorer</a>, men något som liknar den definitionen mycket mer är skevsymmetriska multilinjära avbildningar.Definition: Skevsymmetrisk multilinjär avbildning
En multilinjär avbildning kallas skevsymmetrisk om byter tecken när två argument byter plats, alltså om
för alla permutationer och vektorer .
Och som vi ganska enkelt kan se betyder alternerande och skevsymmetrisk (nästan) samma sak.Sats: Skevsymmetrisk vs Alternerande
Om en multilinjär avbildning är alternerande så är skevsymmetrisk. Om är skevsymmetrisk och även i kroppen så är alternerande.
Bevis:
Antag först att är alternerande och fixera godtyckligt alla förutom två argument till . Då är
Antag nu istället att är skevsymmetrisk och att . Då är ]]></description><link>matematik/linjär-algebra/alternerande-och-skevsymmetriska-avbildningar.html</link><guid isPermaLink="false">Matematik/Linjär algebra/Alternerande och skevsymmetriska avbildningar.md</guid><pubDate>Fri, 10 Jan 2025 16:45:37 GMT</pubDate></item><item><title><![CDATA[Symmetriska och alternerande tensorer]]></title><description><![CDATA[<a href=".?query=tag:matematik/linjär-algebra" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik/linjär-algebra">#matematik/linjär-algebra</a><br>Vi studerar speciella slags <a data-href="Tensorer" href="matematik/linjär-algebra/tensorer.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Tensorer</a><a data-href="Tensorer" href="matematik/linjär-algebra/tensorer.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Tensorer</a> som utgör delrum av tensorprodukter.Definition: Symmetriska och alternerande tensorer
Låt vara ett vektorrum med bas . En tensor kallas symmetrisk om
för alla permutationer , där . kallas alternerande om istället byter tecken när två index byter plats, alltså om betecknar mängden av alla symmetriska -tensorer för , och betecknar mängden av alla alternerande -tensorer för .
Det kan vara lite svårt att från denna definition få en bra bild av vad som sägs, så vi tar ett exempel. Exempel
Om vi betraktar -tensorer till något -dimensionellt vektorrum så kan ju de beskrivas med en uppsättning tal enligt Om vi ordnar talen i en matris så är tensorn symmetrisk om och endast om är symmetrisk, och tensorn är alternerande om och endast om är antisymmetrisk. Vi ser det eftersom den enda (icke-identitet) permutationen av element är att byta plats på elementen, så för att tensorn ska vara symmetrisk ska alltså och för att den ska vara alternerande så ska .
För att få en bättre bild av vad som händer när vi går till tensorer av högre ordning än 2 kan vi betrakta dimensionerna av rummen av symmetriska respektive alternerande tensorer.Sats: , Om är ett vektorrum så är
Bevis:
För att hitta dimensionen av alla symmetriska -tensorer undrar vi alltså hur många linjärt oberoende uppsättningar tal som finns sådana att motsvarande tensor är symmetrisk. Det är då antalet uppsättningar index som inte är permutationer av varandra, vilket är samma som antalet sätt att välja element från stycken, eftersom vi kan se det som att placera ut stycken "separerare" bland dessa element (klassiskt kombinatoriktänk).
För alternerande -tensorer gör vi på samma sätt men måste tänka på att vi nu inte får ha något ; alla sådana är eftersom vi kan byta plats på och vilket ger att . Alltså blir antalet uppsättningar index som vi får välja fritt till helt enkelt antalet sätt att välja element från , alltså .
]]></description><link>matematik/linjär-algebra/symmetriska-och-alternerande-tensorer.html</link><guid isPermaLink="false">Matematik/Linjär algebra/Symmetriska och alternerande tensorer.md</guid><pubDate>Fri, 10 Jan 2025 16:45:06 GMT</pubDate></item><item><title><![CDATA[Markovkedjor och sannolikhetsmatriser]]></title><description><![CDATA[<a href=".?query=tag:matematik/linjär-algebra" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik/linjär-algebra">#matematik/linjär-algebra</a> Definition: Markovkedja
En Markovkedja är en slags stokastisk process som vid varje tidpunkt befinner sig i ett av ett ändligt antal möjliga tillstånd, och där sannolikheten att vid en viss tidpunkt befinna sig i ett visst tillstånd endast beror på tillståndet i föregående tidpunkt.
En Markovkedja med stycken möjliga tillstånd bestäms alltså unikt av sannolikheterna att gå från tillstånd till tillstånd för alla . Vi kan ordna dessa sannolikheter i en kvadratisk matris som vi kallar för en sannolikhetsmatris. I en sådan matris beskriver alltså kolumn sannolikheterna att gå till de olika möjliga tillstånden om det aktuella tillståndet är tillstånd . Definition: Sannolikhetsvektor, sannolikhetsmatris En sannolikhetsvektor är en reell kolumnvektor sådan att varje element är ickenegativt och sådan att summan av alla element är .
En sannolikhetsmatris är en kvadratisk matris där varje kolumn är en sannolikhetsvektor.
Sats: Markovkedjor beskrivs av sannolikhetsmatriser
Om en Markovkedja med stycken tillstånd vid tidpunkten befinner sig i tillstånd med sannolikheten där är en kolumnvektor, så finns en sannolikhetsmatris sådan att
för alla .
Bevis:
Vi ordnar enligt tidigare så att är sannolikheten att gå från tillstånd till tillstånd , som vi vet finns enligt definitionen av en Markovkedja. Då är I många fall kommer en Markovkedja gå mot ett stationärt tillstånd, där alltså gränsvärdet
existerar. Isåfall vet vi alltså att , så är en egenvektor med egenvärde . Vi ser dock att vi inte alltid kommer gå mot ett stationärt tillstånd, eftersom vi kan bilda en Markovkedja och välja ett utgångsläge så att den med hundra procents sannolikhet går i en bestämd cykel mellan flera tillstånd. Däremot kan vi visa att det åtminstone alltid finns ett stationärt tillstånd.Lemma
Om är en sannolikhetsmatris och är egenvektor med egenvärde där , och så är en sannolikhetsvektor och egenvektor med egenvärde .
Bevis:
För alla har vi att men vi vet också att eftersom är en sannolikhetsmatris, och eftersom alla termer är ickenegativa måste alltså . När detta skaleras med blir det en sannolikhetsvektor.
Sats: Stationärt tillstånd finns alltid
Om är en sannolikhetsmatris så finns en sannolikhetsvektor som är en egenvektor med egenvärde .
Bevis:<br>
Om vi låter så vet vi att eftersom varje kolumn i är en sannolikhetsvektor. Ekvivalent har vi alltså att , och eftersom har samma egenvärden som måste även ha egenvärdet ( och har samma karaktäristiska polynom eftersom determinanten inte påverkas av transponering). Det finns alltså en egenvektor med egenvärde till , och enligt <a data-tooltip-position="top" aria-label="^0c2ecd" data-href="#^0c2ecd" href="#^0c2ecd" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Lemma</a><a data-tooltip-position="top" aria-label="^0c2ecd" data-href="#^0c2ecd" href="#^0c2ecd" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Lemma</a> är vi klara.
Definition: Irreducibel sannolikhetsmatris
En sannolikhetsmatris kallas irreducibel om det för varje finns ett sådant att , eller ekvivalent om det från varje tillstånd går att nå varje tillstånd efter ett ändligt antal steg. Definition: Reguljär sannolikhetsmatris
En sannolikhetsmatris kallas reguljär om det finns ett sådant att är positiv, alltså sådant att varje element i är positivt.
En tolkning av vad som menas med en Markovkedja som beskrivs av en reguljär sannolikhetsmatris är att det finns en tidpunkt där kedjan oavsett utgångsläge kan vara i vilket tillstånd som helst. Detta eftersom vi efter att ha applicerat en positiv sannolikhetsmatris på en sannolikhetsvektor får en positiv sannolikhetsvektor, vilket betyder att vi kan vara varsomhelst.Vi ser direkt från dessa definitioner att alla reguljära sannolikhetsmatriser också är irreducibla.Sats
Om är en irreducibel sannolikhetsmatris så gäller Om är en egenvektor med egenvärde där så är samtliga koordinater i nollskilda.
Det finns en positiv sannolikhetsvektor som är en egenvektor med egenvärde .
Om är ett egenvärde där så är där och är algebraisk respektive geometrisk multiplicitet.
De komplexa egenvärdena med belopp är precis de :e enhetsrötterna för något . Bevis: <br>Enligt <a data-tooltip-position="top" aria-label="^d74c6c" data-href="#^d74c6c" href="#^d74c6c" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Sats</a><a data-tooltip-position="top" aria-label="^d74c6c" data-href="#^d74c6c" href="#^d74c6c" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Sats</a> är en egenvektor med egenvärde . är inte nollvektorn, så det finns ett s.a. . Eftersom är irreducibel vet vi att det för alla finns ett så att , vilket ger att , alltså är .
<br>Vi vet enligt <a data-tooltip-position="top" aria-label="^d74c6c" data-href="#^d74c6c" href="#^d74c6c" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Sats</a><a data-tooltip-position="top" aria-label="^d74c6c" data-href="#^d74c6c" href="#^d74c6c" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Sats</a> att det finns en sannolikhetsvektor som är en egenvektor med egenvärde . Ovan såg vi att också är positiv.
Givet ett egetvärde där , om det finns två oberoende egenvektorer kan vi bilda en linjärkombination som också är en egenvektor med egenvärde , där en men inte alla koordinater är , vilket är en motsägelse, alltså är . Om så finns en generaliserad egenvektor s.a. där är en egenvektor, vilket ger att vars norm går mot när går mot eftersom och . Detta är en motsägelse eftersom och därmed även är sannolikhetsmatriser och normerna av kolumnerna därför inte kan gå mot . Därmed är . Sats: Perron-Frobenius sats
Om är en reguljär sannolikhetsmatris så gäller Det finns en positiv egenvektor med egenvärde . där och är algebraisk respektive geometrisk multiplicitet.
Alla egenvärden förutom uppfyller att . finns och är oberoende av . Bevis: <br>Vi vet att är irreducibel eftersom den är reguljär, alltså stämmer påståendet enligt <a data-tooltip-position="top" aria-label="^5ee6c6" data-href="#^5ee6c6" href="#^5ee6c6" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Sats</a><a data-tooltip-position="top" aria-label="^5ee6c6" data-href="#^5ee6c6" href="#^5ee6c6" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Sats</a>.
<br>Följer från samma <a data-tooltip-position="top" aria-label="^5ee6c6" data-href="#^5ee6c6" href="#^5ee6c6" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Sats</a><a data-tooltip-position="top" aria-label="^5ee6c6" data-href="#^5ee6c6" href="#^5ee6c6" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Sats</a>.
Låt vara sannolikhetsvektorn med egenvärde , låt och bilda . Vi får då att , och , alltså är . Både och är positiva, alltså finns ett s.a. är positiv. Den är dock inte en sannolikhetsmatris, så vi bildar som då alltså blir en sannolikhetsmatris. Vi betraktar potenserna av och får att Från detta får vi och eftersom är en sannolikhetsmatris är även alla det, så därför går mot när . Därmed, om är en egenvektor med egenvärde så får vi att ska gå mot när , men det kan endast ske om eller om , eftersom om så går detta mot oändligheten och om men så går detta i en cykel och alltså inte mot något gränsvärde.
Vi får att ]]></description><link>matematik/linjär-algebra/markovkedjor-och-sannolikhetsmatriser.html</link><guid isPermaLink="false">Matematik/Linjär algebra/Markovkedjor och sannolikhetsmatriser.md</guid><pubDate>Thu, 09 Jan 2025 12:34:30 GMT</pubDate></item><item><title><![CDATA[Normala operatorer]]></title><description><![CDATA[<a href=".?query=tag:matematik/linjär-algebra" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik/linjär-algebra">#matematik/linjär-algebra</a> <br>Vi har sett att både <a data-tooltip-position="top" aria-label="Adjungerade avbildningar" data-href="Adjungerade avbildningar" href="matematik/linjär-algebra/adjungerade-avbildningar.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Självadjungerade operatorer</a><a data-tooltip-position="top" aria-label="Adjungerade avbildningar" data-href="Adjungerade avbildningar" href="matematik/linjär-algebra/adjungerade-avbildningar.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Självadjungerade operatorer</a> över och <a data-tooltip-position="top" aria-label="Ortogonala och unitära operatorer" data-href="Ortogonala och unitära operatorer" href="matematik/linjär-algebra/ortogonala-och-unitära-operatorer.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Unitära operatorer</a><a data-tooltip-position="top" aria-label="Ortogonala och unitära operatorer" data-href="Ortogonala och unitära operatorer" href="matematik/linjär-algebra/ortogonala-och-unitära-operatorer.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Unitära operatorer</a> på ett ändligdimensionellt inre produktrum har ortogonala baser av egenvektorer. Vi kan ju då fråga oss vad likheten mellan dessa är, och om vi kan generalisera dem på något sätt. Vi börjar med att relatera existensen av ortogonala egenbaser till självadjungerade respektive unitära operatorer.Sats
Om är linjär och det finns en ortogonal bas till av egenvektorer till så är självadjungerad om och endast om alla egenvärden är reella, och är unitär om och endast om alla egenvärden har belopp .
Bevis:
Med avseende på basen av ortonormala egenvektorer blir matrisen för en diagonalmatris , och är alltså självadjungerad om och endast om , alltså om och endast om alla egenvärden , så . Den är även unitär om och endast om , alltså om och endast om alla egenvärden , så .
Nu bildar vi en generalisering av dessa operatorer. Vi vet ju att självadjungerade operatorer uppfyller medan unitära operatorer uppfyller . Båda dessa uppfyller uppenbarligen att , och det ska visa sig att det är just denna egenskap som är avgörande.Definition: Normal operator
Om är linjär och så kallas normal.
Sats: Normal Har ortonormal egenbas
En operator där är ett komplext ändligdimensionellt inre produktrum är normal om och endast om den har en ortogonal bas av egenvektorer.
Bevis:
Vi visar först . Antag alltså att är normal. Vi använder induktion på . Om så är vi klara eftersom motsvarar en skalering. Annars vet vi att det finns ett egenvärde med normal egenvektor till eftersom . Låt nu . Vi vet då att det finns ett unikt så att , alltså så att . Eftersom är normal vet vi då att
vilket ger att , alltså är , vilket ger att för alla , så är invariant under . Därmed kan vi applicera satsen på restriktionen av till och få en ortonormal bas till av egenvektorer till . När vi lägger till till denna bas får vi en ortonormal bas till .
Vi visar nu . Antag alltså att har en ortonormal bas av egenvektorer med egenvärden . Då får vi för alla att
Detta ger att för alla , alltså är .
]]></description><link>matematik/linjär-algebra/normala-operatorer.html</link><guid isPermaLink="false">Matematik/Linjär algebra/Normala operatorer.md</guid><pubDate>Thu, 09 Jan 2025 11:25:37 GMT</pubDate></item><item><title><![CDATA[Ortogonala och unitära operatorer]]></title><description><![CDATA[<a href=".?query=tag:matematik/linjär-algebra" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik/linjär-algebra">#matematik/linjär-algebra</a> Definition: Isometri
Om och är inre produktrum och är linjär, samt om
för alla , så kallas för en isometri.
Sats: Isometrier är injektiva
Om är en isometri så är injektiv.
Bevis:
För alla så gäller att om så är , alltså är .
Sats: Isometri Inre produkt bevaras
Om är linjär och är ett inre produktrum så är en isometri om och endast om bevarar den inre produkten, alltså
för alla .
Bevis:
Vi visar satsen för fallet då . Om blir beviset likadant men med längre uträkningar. är uppenbar eftersom om den inre produkten bevaras så bevaras normen. Vi behöver alltså bara visa , så antag att är en isometri. Vi utnyttjar bilinjäriteten av den inre produkten i , där vi kan använda att
alltså är för alla så en inre produkt bestäms alltså av normer, alltså bevaras den inre produkten om normen bevaras.
Sats: Isometri Om är linjär och är ett inre produktrum så är en isometri om och endast om .
Bevis:<br>
Vi vet att om och endast om för alla , vilket enligt <a data-tooltip-position="top" aria-label="Inre produktrum > ^311e6a" data-href="Inre produktrum#^311e6a" href="matematik/linjär-algebra/inre-produktrum.html#^311e6a" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Sats</a><a data-tooltip-position="top" aria-label="Inre produktrum > ^311e6a" data-href="Inre produktrum#^311e6a" href="matematik/linjär-algebra/inre-produktrum.html#^311e6a" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Sats</a> gäller om och endast om för alla , vilket gäller om och endast om för alla .
Sats: Isometri Ortonormala baser avbildas på ortonormala baser
Om är linjär och är ett inre produktrum med minst en ortonormal bas så är en isometri om och endast om avbildar alla ortonormala baser på ortonormala baser.
Bevis:
Om är en isometri vet vi att alla ortonormala baser avbildas på ortogonala baser eftersom den inre produkten bevaras. Om istället alla ortonormala baser avbildas på ortonormala baser så vet vi att vi kan välja en ortonormal bas . För alla har vi då att vilket ger att
där vi utnyttjat att eftersom den ortonormala basen avbildas på en ortonormal bas . I satsen ovan har vi alltså kravet att ska ha minst en ortonormal bas. Annars uppfyller ju egentligen alla linjära avbildningar på att de avbildar ortonormala baser på ortonormala baser, eftersom det inte finns några! Som vi sett tidigare finns alltid en ortonormal bas om är ändligdimensionellt, men det kan även finnas i det oändligdimensionella fallet. Definition: Ortogonal och unitär operator
Om operatorn är en inverterbar isometri så kallas ortogonal om och unitär om .
Sats: Ortogonal/unitär En operator är ortogonal/unitär om och endast om är inverterbar och . Bevis:<br>
Vi visar först . Antag alltså att är ortogonal/unitär. Då är en isometri, alltså är enligt <a data-tooltip-position="top" aria-label="^944e36" data-href="#^944e36" href="#^944e36" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Sats</a><a data-tooltip-position="top" aria-label="^944e36" data-href="#^944e36" href="#^944e36" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Sats</a>. är även inverterbar, alltså surjektiv, så är alltså på hela .
Vi visar nu . Antag alltså att är inverterbar och . Vi behöver bara visa att är en isometri, vilket följer direkt eftersom
för alla .
Just att vi säger att ska vara en inverterbar isometri är egentligen bara viktigt för oändligdimensionella vektorrum. På ändligdimensionella vektorrum är ju injektivitet, som vi vet att alla isometrier har, ekvivalent med inverterbarhet. På oändligdimensionella vektorrum däremot kan vi till exempel på bilda avbildningen
som är en isometri men inte inverterbar eftersom den inte är surjektiv.<br>Definition: Ortogonal/unitär matris
Om är en ortogonal/unitär operator och är matrisen för med avseende på en <a data-tooltip-position="top" aria-label="Ortogonalitet och projektioner > ^0ed9e8" data-href="Ortogonalitet och projektioner#^0ed9e8" href="matematik/linjär-algebra/ortogonalitet-och-projektioner.html#^0ed9e8" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Ortonormal bas</a><a data-tooltip-position="top" aria-label="Ortogonalitet och projektioner > ^0ed9e8" data-href="Ortogonalitet och projektioner#^0ed9e8" href="matematik/linjär-algebra/ortogonalitet-och-projektioner.html#^0ed9e8" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Ortonormal bas</a> så kallas för en ortogonal/unitär matris.
Sats: Egenvärden till ortogonala/unitära operatorer har belopp Om är en ortogonal/unitär operator och är ett egenvärde så är .
Bevis:
Låt vara en egenvektor med egenvärde . Då är Sats: Egenvektorer till ortogonala/unitära operatorer är ortogonala
Om är en ortogonal/unitär operator och är olika egenvärden till med respektive egenvektorer så är .
Bevis:
Eftersom bevarar den inre produkten får vi att
men eftersom är , så då måste .
Sats: Unitära operator har ortogonal egenbas
Om är en unitär operator och är ändligdimensionellt så finns en ortogonal bas av egenvektorer till .
Bevis:<br>
Vi använder induktion på . Eftersom finns minst ett egenvärde med egenvektor . Därför är satsen uppenbar om . Annars låter vi och vet då enligt <a data-tooltip-position="top" aria-label="Ortogonalitet och projektioner > ^a63929" data-href="Ortogonalitet och projektioner#^a63929" href="matematik/linjär-algebra/ortogonalitet-och-projektioner.html#^a63929" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Sats</a><a data-tooltip-position="top" aria-label="Ortogonalitet och projektioner > ^a63929" data-href="Ortogonalitet och projektioner#^a63929" href="matematik/linjär-algebra/ortogonalitet-och-projektioner.html#^a63929" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Sats</a> att . Vi vet även att är invariant under eftersom om så är
för alla . Därmed kan vi applicera satsen på det mindre rummet och får alltså en ortogonal bas av egenvektorer till för . Denna bas tillsammans med utgör då en ortogonal bas för eftersom .
<br>Sats: unitär om självadjungerad
Om är <a data-tooltip-position="top" aria-label="Adjungerade avbildningar" data-href="Adjungerade avbildningar" href="matematik/linjär-algebra/adjungerade-avbildningar.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Självadjungerad</a><a data-tooltip-position="top" aria-label="Adjungerade avbildningar" data-href="Adjungerade avbildningar" href="matematik/linjär-algebra/adjungerade-avbildningar.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Självadjungerad</a> och är ändligdimensionellt så är unitär.
Bevis:<br>
Eftersom är självadjungerad finns en ortonormal bas av egenvektorer med respektive reella egenvärden . Vi får då att , så eftersom får vi att den ortonormala basen avbildas på en annan ortonormal bas . Enligt <a data-tooltip-position="top" aria-label="^67f1d3" data-href="#^67f1d3" href="#^67f1d3" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Sats</a><a data-tooltip-position="top" aria-label="^67f1d3" data-href="#^67f1d3" href="#^67f1d3" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Sats</a> räcker detta för att ska vara en isometri, och eftersom är ändligdimensionellt är även inverterbar, alltså unitär.
Av satsen ovan ser vi att om är en antisymmetrisk matris så är konjugatsymmetrisk, alltså är unitär och reell, alltså ortogonal.]]></description><link>matematik/linjär-algebra/ortogonala-och-unitära-operatorer.html</link><guid isPermaLink="false">Matematik/Linjär algebra/Ortogonala och unitära operatorer.md</guid><pubDate>Thu, 09 Jan 2025 10:02:04 GMT</pubDate></item><item><title><![CDATA[Adjungerade avbildningar]]></title><description><![CDATA[<a href=".?query=tag:matematik/linjär-algebra" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik/linjär-algebra">#matematik/linjär-algebra</a> Definition: Adjungerad avbildning
Om och är linjära och
för alla och så kallas för den adjungerade avbildningen till .
Vi kallar för den adjungerade avbildningen eftersom vi enkelt kan visa att om den finns så är den unik.Sats: Adjungerad operator är unik
En linjär avbildning har högst en adjungerad avbildning .
Bevis:
Om är adjungerade avbildningar så har vi att<br>
för alla och , vilket enligt <a data-tooltip-position="top" aria-label="Inre produktrum > ^311e6a" data-href="Inre produktrum#^311e6a" href="matematik/linjär-algebra/inre-produktrum.html#^311e6a" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Sats</a><a data-tooltip-position="top" aria-label="Inre produktrum > ^311e6a" data-href="Inre produktrum#^311e6a" href="matematik/linjär-algebra/inre-produktrum.html#^311e6a" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Sats</a> ger att för alla , alltså är .
<br>Vi ser även direkt vissa egenskaper, som att . Vi har dock fortfarande inte visat existens, så vi vet inte om det alltid finns en adjungerad operator. Det ska visa sig att åtminstone i det ändligdimensionella fallet finns alltid en adjungerad operator. Vi ser det genom att betrakta <a data-tooltip-position="top" aria-label="Ortogonalitet och projektioner > ^0ed9e8" data-href="Ortogonalitet och projektioner#^0ed9e8" href="matematik/linjär-algebra/ortogonalitet-och-projektioner.html#^0ed9e8" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Ortonormala baser</a><a data-tooltip-position="top" aria-label="Ortogonalitet och projektioner > ^0ed9e8" data-href="Ortogonalitet och projektioner#^0ed9e8" href="matematik/linjär-algebra/ortogonalitet-och-projektioner.html#^0ed9e8" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Ortonormala baser</a>.Sats: Adjungerade operatorn är konjugattransponatet i ON-bas
Om och är ändligdimensionella inre produktrum och är linjär, samt är matrisen för m.a.p ortonormala baser för och , så är konjugattransponatet matrisen för m.a.p samma ortonormala baser.
Bevis:<br>
Låt och vara ortonormala baser för respektive . Då får vi att matrisen för m.a.p de ortonormala baserna enligt <a data-tooltip-position="top" aria-label="Ortogonalitet och projektioner > ^299a88" data-href="Ortogonalitet och projektioner#^299a88" href="matematik/linjär-algebra/ortogonalitet-och-projektioner.html#^299a88" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Sats</a><a data-tooltip-position="top" aria-label="Ortogonalitet och projektioner > ^299a88" data-href="Ortogonalitet och projektioner#^299a88" href="matematik/linjär-algebra/ortogonalitet-och-projektioner.html#^299a88" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Sats</a> ges av
och på samma sätt ges då matrisen för den adjungerade operatorn av
alltså är .
Förutom att vi vill garantera existens av ortonormala baser ser vi att eftersom vi tar transponatet av är det viktigt att åtminstone är ändligdimensionellt; annars kan vi i transponatet få kolumner med ett oändligt antal nollskilda element.Definition: Självadjungerad
Om är linjär och så kallas självadjungerad.
Definintion: Konjugattransponat
Om är en reell eller komplex matris så låter vi beteckna konjugattransponatet, alltså
Om kallar vi för konjugatsymmetrisk eller Hermitesk.
Sats: Självadjungerade operatorer har endast reella egenvärden
Samtliga egenvärden till en självadjungerad operator är reella.
Bevis:
Låt vara en egenvektor till med egenvärde . Då är
alltså är , alltså är .
Sats: Spektralsatsen
Om är ett ändligdimensionellt inre produktrum och operatorn är självadjungerad så finns en ortogonal bas av egenvektorer till .
Bevis:<br>
Vi använder induktion på . Om så är satsen uppenbar eftersom motsvarar en skalering. Antag nu att . Vi vet då att det finns minst ett egenvärde till oavsett om eller , eftersom om så kan vi se matrisen för som en komplex matris vars karaktäristiska polynom vi då vet har minst en komplex rot, men enligt <a data-tooltip-position="top" aria-label="^1e86e0" data-href="#^1e86e0" href="#^1e86e0" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Sats</a><a data-tooltip-position="top" aria-label="^1e86e0" data-href="#^1e86e0" href="#^1e86e0" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Sats</a> är den roten reell, alltså även ett egenvärde om . Låt därmed vara en egenvektor med egenvärde . Låt sedan , vilket ger att . Vi vet att är invariant under , och vi får att även är det eftersom om så är Därmed kan vi applicera satsen på restriktionen av till och få en ortogonal bas till av egenvektorer till . När vi lägger till till denna bas får vi en ortogonal bas till eftersom och är ortogonal mot alla vektorer i .
]]></description><link>matematik/linjär-algebra/adjungerade-avbildningar.html</link><guid isPermaLink="false">Matematik/Linjär algebra/Adjungerade avbildningar.md</guid><pubDate>Thu, 09 Jan 2025 09:54:05 GMT</pubDate></item><item><title><![CDATA[Duala rum]]></title><description><![CDATA[<a href=".?query=tag:matematik/linjär-algebra" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik/linjär-algebra">#matematik/linjär-algebra</a> Definition: Dualt rum
Om är ett vektorrum så är det duala rummet vektorrummet
En linjär avbildning till kallas för en linjär form.
Exempel
Om och är kolumnvektorer så kan ses som alla radvektorer av storlek , eftersom en radvektor multiplicerat från vänster med en kolumnvektor blir ett tal.
För att generalisera exemplet går vi till inre produktrum.Definition: Om är ett inre produktrum och så är den linjära formen som ges av att
för alla .
Definition: Dual bas
Om är ändligdimensionellt med bas så är den duala basen en bas för som ges av där
för alla .
Vi ser att om är oändligdimensionellt så blir den "duala basen" linjärt oberoende men inte en bas eftersom en linjär form kan vara nollskild för oändligt många basvektorer medan alla linjära former som spänns upp av den "duala basen" endast är nollskilda för ändligt många basvektorer.Vi märker även att den duala basen ger koordinater eftersom<br>
Slutligen ser vi även att om är en <a data-tooltip-position="top" aria-label="Ortogonalitet och projektioner > ^0ed9e8" data-href="Ortogonalitet och projektioner#^0ed9e8" href="matematik/linjär-algebra/ortogonalitet-och-projektioner.html#^0ed9e8" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Ortonormal bas</a><a data-tooltip-position="top" aria-label="Ortogonalitet och projektioner > ^0ed9e8" data-href="Ortogonalitet och projektioner#^0ed9e8" href="matematik/linjär-algebra/ortogonalitet-och-projektioner.html#^0ed9e8" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Ortonormal bas</a> så följer det direkt från definitionen att är en dual bas.]]></description><link>matematik/linjär-algebra/duala-rum.html</link><guid isPermaLink="false">Matematik/Linjär algebra/Duala rum.md</guid><pubDate>Thu, 09 Jan 2025 09:23:18 GMT</pubDate></item><item><title><![CDATA[Konvergens och Cauchyföljder]]></title><description><![CDATA[<a href=".?query=tag:matematik/linjär-algebra" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik/linjär-algebra">#matematik/linjär-algebra</a> <br>För att kunna tala om oändliga summor av vektorer måste vi definiera någon slags konvergens. Vi kan göra det enkelt på <a data-href="Inre produktrum" href="matematik/linjär-algebra/inre-produktrum.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Inre produktrum</a><a data-href="Inre produktrum" href="matematik/linjär-algebra/inre-produktrum.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Inre produktrum</a>.Definition: Konvergens
Låt vara ett inre produktrum och vara en följd av vektorer i . Följden kallas konvergent om det finns en vektor så att det för alla finns ett så att för alla . Vi skriver då Vi märker dock att det är svårt att avgöra om en godtycklig följd konvergerar eftersom vi inte nödvändigtvis vet vad den konvergerar mot. Därför inför vi även så kallade Cauchyföljder.Definition: Cauchyföljd
Låt vara ett inre produktrum och vara en följd av vektorer i . Följden kallas för en Cauchyföljd om det för alla finns ett så att för alla .
Sats: Konvergent Cauchyföljd
Om är en konvergent följd av vektorer i ett inre produktrum så är följden en Cauchyföljd.
Bevis:<br>
Låt . <a data-tooltip-position="top" aria-label="Norm och vinklar > ^3d5fd7" data-href="Norm och vinklar#^3d5fd7" href="matematik/linjär-algebra/norm-och-vinklar.html#^3d5fd7" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Triangelolikheten</a><a data-tooltip-position="top" aria-label="Norm och vinklar > ^3d5fd7" data-href="Norm och vinklar#^3d5fd7" href="matematik/linjär-algebra/norm-och-vinklar.html#^3d5fd7" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Triangelolikheten</a> ger då att
och konvergensen ger att vi kan få och godtyckligt små, alltså är följden en Cauchyföljd.
Omvändningen är däremot inte alltid sann. Till exempel finns rationella Cauchyföljder som konvergerar mot irrationella tal.Definition: Fullständigt metriskt rum
Ett metriskt rum kallas fullständigt om alla Cauchyföljder konvergerar.
Definition: Låt vara vektorrummet av alla rationella talföljder. Låt
vara delrummet av alla rationella Cauchyföljder, och låt<br>
Då definierar vi som <a data-tooltip-position="top" aria-label="Kvotrum" data-href="Kvotrum" href="matematik/linjär-algebra/kvotrum.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Kvotrummet</a><a data-tooltip-position="top" aria-label="Kvotrum" data-href="Kvotrum" href="matematik/linjär-algebra/kvotrum.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Kvotrummet</a> .
På det sättet blir ett fullständigt metriskt rum.]]></description><link>matematik/linjär-algebra/konvergens-och-cauchyföljder.html</link><guid isPermaLink="false">Matematik/Linjär algebra/Konvergens och Cauchyföljder.md</guid><pubDate>Thu, 09 Jan 2025 09:11:34 GMT</pubDate></item><item><title><![CDATA[Singulärvärdesuppdelning]]></title><description><![CDATA[<a href=".?query=tag:matematik/linjär-algebra" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik/linjär-algebra">#matematik/linjär-algebra</a> <br>Med <a data-tooltip-position="top" aria-label="Egenvärden och egenvektorer" data-href="Egenvärden och egenvektorer" href="matematik/linjär-algebra/egenvärden-och-egenvektorer.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Diagonalisering</a><a data-tooltip-position="top" aria-label="Egenvärden och egenvektorer" data-href="Egenvärden och egenvektorer" href="matematik/linjär-algebra/egenvärden-och-egenvektorer.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Diagonalisering</a> såg vi att vi genom att välja rätt bas ofta kan uttrycka en <a data-tooltip-position="top" aria-label="Linjära avbildningar" data-href="Linjära avbildningar" href="matematik/linjär-algebra/linjära-avbildningar.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Operator</a><a data-tooltip-position="top" aria-label="Linjära avbildningar" data-href="Linjära avbildningar" href="matematik/linjär-algebra/linjära-avbildningar.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Operator</a> med en väldigt enkel matris - en diagonalmatris - och med <a data-href="Jordans Normalform" href="matematik/linjär-algebra/jordans-normalform.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Jordans Normalform</a><a data-href="Jordans Normalform" href="matematik/linjär-algebra/jordans-normalform.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Jordans Normalform</a> såg vi att vi alltid kan uttrycka en operator med en hyfsat enkel matris. Nu vill vi göra samma sak för generella avbildningar där vi alltså nu har friheten att välja baser både för och för .När vi får välja baserna helt fritt har vi i någon mening för mycket frihet eftersom vi alltid kan få matrisen för på blockformen<br> där är en identitetsmatris och nollorna är nollmatriser så att matrisen blir av rätt storlek. Då förlorar vi ju dock i princip all information om vad avbildningen gör, så det är inte speciellt användbart. Vi sätter därför begränsningen att baserna ska vara <a data-tooltip-position="top" aria-label="Ortogonalitet och projektioner" data-href="Ortogonalitet och projektioner" href="matematik/linjär-algebra/ortogonalitet-och-projektioner.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Ortonormala</a><a data-tooltip-position="top" aria-label="Ortogonalitet och projektioner" data-href="Ortogonalitet och projektioner" href="matematik/linjär-algebra/ortogonalitet-och-projektioner.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Ortonormala</a>.Sats: Singulärvärdesuppdelning
Om är linjär och och är ändligdimensionella inre produktrum så finns ortonormala baser för och så att matrisen för blir på blockformen
där är en diagonalmatris med positiva reella tal längs diagonalen, och där nollorna är nollmatriser så att matrisen blir av rätt storlek.
Bevis:<br>
Det räcker att visa satsen för fallet då är en isomorfi, eftersom om är ändligdimensionellt vet vi annars att och kan betrakta restriktionen av till med målrum , som då är en isomorfi.
Eftersom och är ändligdimensionella vet vi enligt <a data-tooltip-position="top" aria-label="Adjungerade avbildningar > ^f59eca" data-href="Adjungerade avbildningar#^f59eca" href="matematik/linjär-algebra/adjungerade-avbildningar.html#^f59eca" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Sats</a><a data-tooltip-position="top" aria-label="Adjungerade avbildningar > ^f59eca" data-href="Adjungerade avbildningar#^f59eca" href="matematik/linjär-algebra/adjungerade-avbildningar.html#^f59eca" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Sats</a> att den adjungerade operatorn finns. Vi vet då enligt <a data-tooltip-position="top" aria-label="Adjungerade avbildningar > ^92616d" data-href="Adjungerade avbildningar#^92616d" href="matematik/linjär-algebra/adjungerade-avbildningar.html#^92616d" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Sats</a><a data-tooltip-position="top" aria-label="Adjungerade avbildningar > ^92616d" data-href="Adjungerade avbildningar#^92616d" href="matematik/linjär-algebra/adjungerade-avbildningar.html#^92616d" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Sats</a> att både och är diagonaliserbara eftersom de är självadjungerade. Vi låter därmed och vara egenrummen till egenvärde för respektive .
Vi ser att om är en egenvektor med egenvärde till så är
vilket ger att är reellt och positivt eftersom . Vi ser även att
alltså är en egenvektor med egenvärde till , eftersom är injektiv och därmed . För varje egenvärde kan vi därmed välja en ortonormal bas till och då få att
alltså är en ortogonal bas för eftersom ty är en isomorfi och därmed inte kan vara större än spannet av . Vi ser att om vi låter för alla så blir en ortonormal bas för . Vi får då alltså att
för alla . Vi ser även att alla är ortogonala mot varandra eftersom om och är egenvektorer till så är
alltså måste om (och på samma sätt för ). Därmed utgör alla dessa ortogonala baser för och tillsammans ortonormala baser för respektive . Och med m.a.p dessa baser får vi matrisen på den efterfrågade formen, eftersom matrisen för på alltså är Definition: Singulärvärden och singulärvektorer
Låt vara linjär och och vara ändligdimensionella inre produktrum. Om är ett positivt reellt tal, och är enhetsvektorer, samt
så kallas för ett singulärvärde, för högersingulär och för vänstersingulär.
Anmärkning
Även räknas ibland som ett singulärvärde, och alla och kallas då högersingulära respektive vänstersingulära. Vi får dock inte de entydiga paren av en vänstersingulär och högersingulär vektor som vi får med positiva singulärvärden.
Definition: Singulärvärdesuppdelning
Om är en ändlig matris med element i eller så är singulärvärdesuppdelningen av matriser sådana att
där är ortogonala/unitära och är på blockformen
där är en diagonalmatris med positiva reella i fallande ordning nedåt längs diagonalen.
Vi ser alltså att och här är basbytesmatriser till baserna där matrisen är på formen av , där alltså diagonalelementen är singulärvärdena ordnade i fallande ordning. Just att de ska vara i fallande ordning är så att blir unik. Varken eller blir unika eftersom en del kolumner i och fortfarande kan permuteras om de tillhör samma singulärvärde.]]></description><link>matematik/linjär-algebra/singulärvärdesuppdelning.html</link><guid isPermaLink="false">Matematik/Linjär algebra/Singulärvärdesuppdelning.md</guid><pubDate>Wed, 08 Jan 2025 20:11:45 GMT</pubDate></item><item><title><![CDATA[Kvotrum]]></title><description><![CDATA[<a href=".?query=tag:matematik/linjär-algebra" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik/linjär-algebra">#matematik/linjär-algebra</a> <br>Säg att vi har en <a data-tooltip-position="top" aria-label="Linjära avbildningar" data-href="Linjära avbildningar" href="matematik/linjär-algebra/linjära-avbildningar.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Linjär avbildning</a><a data-tooltip-position="top" aria-label="Linjära avbildningar" data-href="Linjära avbildningar" href="matematik/linjär-algebra/linjära-avbildningar.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Linjär avbildning</a> . Då finns ju något nollrum , och vi noterar att om vi tar något så är
för alla . Det vore ju då rimligt att betrakta alla vektorer som kan skrivas på formen för något som samma vektor eftersom ändå behandlar dem som det. Detta leder oss till att först definiera sidoklasser, som vi låter vara mängder av vektorer som vi vill betrakta som samma. Just nollrummet var ett motiverande exempel, men vi vill såklart definiera detta för godtyckligt delrum. Definition: Sidoklass
För alla och delrum definierar vi sidoklassen enligt Hittills är sidoklasser alltså bara mängder av vektorer. Notera även att sidoklasser ofta inte är delrum. Hursomhelst vill vi ju betrakta dessa mängder som vektorer, så då måste vi ju definiera addition och skalärmultiplikation. Innan vi kan göra det hittar vi några egenskaper för sidoklasserna. Först noterar vi att vi kan få samma sidoklass från olika val av . Sats: Olika vektorer, samma sidoklass
För alla och delrum så är om och endast om
Bevis:
Om så är , alltså är för något . Om istället så är Vi är nu redo att definiera addition och skalärmultiplikation av våra sidoklasser.Definition: Sidoklasser som vektorer
För alla och delrum så är
och Vi måste bara verifiera att dessa operationer är väldefinierade, vilket följer från förra satsen.Sats: Addition och skalärmultiplikation av sidoklasser är väldefinierat
Addition och skalärmultiplikation av sidoklasser är väldefinierat, det vill säga om och så är .
Bevis:
Vi har att och , vilket ger att Vi kan nu alltså bilda ett vektorrum av våra sidoklasser. Vi kallar vektorrummet för kvotrummet av och .Definition: Kvotrum
Kvotrummet av ett vektorrum och delrum är vektorrummet av alla sidoklasser, d.v.s <br>Med kvotrum kan vi alltså se det som att vi från ett vektorrum tar bort informationen som ett delrum innehåller. Detta liknar <a data-href="Direkta summor" href="matematik/linjär-algebra/direkta-summor.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Direkta summor</a><a data-href="Direkta summor" href="matematik/linjär-algebra/direkta-summor.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Direkta summor</a>, där vi istället lägger till informationen som ett vektorrum innehåller. På det sättet kan alltså direkt summa och kvot ses som motsatta operationer.Sats: Om är ett delrum så är
Bevis:
Välj så att . Vi kan definiera enligt
för alla och . Den är linjär, surjektiv och injektiv eftersom Sats: Om är ändligdimensionellt så är
Bevis:
Eftersom får vi att För att gå tillbaka till vårt ursprungliga exempel med en linjär avbildning med nollrum , kan vi alltså bilda rummet vars vektorer alltså är mängder av vektorer som ger samma värde för. Hur agerar avgörs alltså av hur den agerar på sina sidoklasser. Detta leder oss till en rätt rimlig sats.Sats: Isomorfisatsen
Om är linjär så är
Bevis:
Vi definierar enligt . Detta är väldefinierat eftersom om så är , alltså är linjär eftersom är injektiv eftersom
och är surjektiv per definition, alltså är en isomorfi.
Från isomorfisatsen följer även den lite svagare dimensionssatsen.Sats: Dimensionssatsen
Om är linjär och är ändligdimensionellt så är
Bevis:
Vi får från isomorfisatsen att ]]></description><link>matematik/linjär-algebra/kvotrum.html</link><guid isPermaLink="false">Matematik/Linjär algebra/Kvotrum.md</guid><pubDate>Wed, 08 Jan 2025 13:32:44 GMT</pubDate></item><item><title><![CDATA[Kroppar]]></title><description><![CDATA[<a href=".?query=tag:matematik/linjär-algebra" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik/linjär-algebra">#matematik/linjär-algebra</a> Definition: Kropp
En kropp är en mängd tillsammans med två binära operationer och som har åtminstone två element och , med egenskaperna Associativitet: och Kommutativitet: och Identiteter: och Inverser: finns ett s.a. , och om så finns ett så att .
Distributivitet: för alla .
Definition: Delkropp
En delkropp av en kropp är en delmängd där , , , för alla , och för alla .
Sats: Kropp är vektorrum över delkropp
Om är en kropp och är en delkropp, så är ett vektorrum över .
Bevis:
Addition av tal i samt multiplikation med tal i är väldefinierat och uppfyller samma regler som de för ett vektorrum.
Definition: Isomorfi av kroppar
Om är kroppar så är en isomorfi mellan och en bijektion som respekterar addition, multiplikation och enheter, det vill säga , , och .
Notera att det från denna definition följer att en isomorfi även respekterar inverser, så att och .Definition: För alla heltal så är restklasserna i vid division med med addition och multiplikation som ges av Sats: kropp om primtal
Om är ett primtal så är en kropp med element.
Bevis:
Heltalen uppfyller redan alla egenskaper förutom existensen av en multiplikativ invers. Det följer att även heltalen modulo uppfyller dessa krav. Kvar är att visa att även multiplikativa inverser finns för nollskilda . Vi noterar att om så är , och eftersom så är alltså , så . Därmed är multiplikation med injektivt, och eftersom är ändlig är alltså multiplikation med även surjektivt, alltså finns en multiplikativ invers.
Sats: Minsta delkropp är eller för något primtal Om är en kropp där så finns en minsta delkropp som är snittet av alla delkropp i . Då är isomorf med antingen eller .
Bevis:
Vi vet att och därmed även . Om detta ger oändligt många element så finns alla heltal och därmed alla rationella tal i , och då är . Annars finns ett minsta s.a. . Det måste vara ett primtal eftersom om så är vilket ger att antingen eller , men det motsäger att är minst.
Definition: Primkropp
Den minsta delkroppen av en kropp kallas för primkroppen i .
Sats: En ändlig kropp har element
Om är en ändlig kropp så har exakt stycken element för något primtal och heltal .
Bevis:
Primkroppen i har ett primtal element, och är ett vektorrum över och är därför isomorf med för något , som har stycken element.
Definition: Karakteristik
Om primkroppen till en kropp är så har karakteristik . Om primkroppen är så har karakteristik .
Sats: Om en kropp har karakteristik så är för alla .
Bevis:
I binomialutvecklingen blir varje term förutom den första och sista delbar med eftersom och är delbart med medan varken eller är det om .
Från detta följer både Fermats lilla sats, som säger att , och från det även Eulers sats, som säger att , vilket fås från att helt enkelt dividera med i Fermats lilla sats, om .Sats: Ändliga kroppar kan representeras av matriser
Alla ändliga kroppar går att representera med en mängd matriser och med multiplikation och addition definierat som vanligt för matriser.
Bevis:
Låt vara en ändlig kropp med primkroppen . Det finns då en bas för som vektorrum över . För alla finns då en matris m.a.p basen som motsvarar multiplikation med , som är linjärt. Då beter sig dessa matriser precis som elementen i eftersom matrisen för blir och matrisen för blir . Mängden av alla är därmed en kropp isomorf med .
Definition: Irreducibelt polynom
Ett polynom kallas irreducibelt om det inte kan skrivas som en produkt av två polynom i av lägre grad.
<br>Sats: Konstruktion av kropp
Om är en matris i och <a data-tooltip-position="top" aria-label="Minimalpolynom" data-href="Minimalpolynom" href="matematik/linjär-algebra/minimalpolynom.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Minimalpolynomet</a><a data-tooltip-position="top" aria-label="Minimalpolynom" data-href="Minimalpolynom" href="matematik/linjär-algebra/minimalpolynom.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Minimalpolynomet</a> till är irreducibelt så är en kropp med addition och multiplikation av matriser som operationer.
Bevis:
Vi vet redan att är ett vektorrum över . Låt . Då är en bas för . Vi ser även att multiplikation i är kommutativt eftersom och kommuterar för alla . Kvar att visa är då att multiplikativa inverser finns. Antag att ett nollskilt element i inte är inverterbart. Elementet kan skrivas som där eftersom om så är elementet en multipel av identitetsmatrisen, som är inverterbar. Med polynomdivision får vi då att där inte är nollpolynomet eftersom är irreducibelt. Vi vet då att , alltså är , alltså är inverterbar eftersom det är en multipel av identitetsmatrisen. Därmed får vi alltså är inverterbar för alla , och därmed är en kropp.
]]></description><link>matematik/linjär-algebra/kroppar.html</link><guid isPermaLink="false">Matematik/Linjär algebra/Kroppar.md</guid><pubDate>Wed, 08 Jan 2025 10:37:30 GMT</pubDate></item><item><title><![CDATA[Linjär algebra]]></title><description><![CDATA[
<a data-href="Linjära avbildningar" href="matematik/linjär-algebra/linjära-avbildningar.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Linjära avbildningar</a><a data-href="Linjära avbildningar" href="matematik/linjär-algebra/linjära-avbildningar.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Linjära avbildningar</a>
<br><a data-href="Direkta summor" href="matematik/linjär-algebra/direkta-summor.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Direkta summor</a><a data-href="Direkta summor" href="matematik/linjär-algebra/direkta-summor.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Direkta summor</a>
<br><a data-href="Kvotrum" href="matematik/linjär-algebra/kvotrum.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Kvotrum</a><a data-href="Kvotrum" href="matematik/linjär-algebra/kvotrum.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Kvotrum</a> <br><a data-href="Egenvärden och egenvektorer" href="matematik/linjär-algebra/egenvärden-och-egenvektorer.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Egenvärden och egenvektorer</a><a data-href="Egenvärden och egenvektorer" href="matematik/linjär-algebra/egenvärden-och-egenvektorer.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Egenvärden och egenvektorer</a>
<br><a data-href="Karakteristiska polynom" href="matematik/linjär-algebra/karakteristiska-polynom.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Karakteristiska polynom</a><a data-href="Karakteristiska polynom" href="matematik/linjär-algebra/karakteristiska-polynom.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Karakteristiska polynom</a>
<br><a data-href="Minimalpolynom" href="matematik/linjär-algebra/minimalpolynom.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Minimalpolynom</a><a data-href="Minimalpolynom" href="matematik/linjär-algebra/minimalpolynom.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Minimalpolynom</a>
<br><a data-href="Samtidig diagonalisering" href="matematik/linjär-algebra/samtidig-diagonalisering.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Samtidig diagonalisering</a><a data-href="Samtidig diagonalisering" href="matematik/linjär-algebra/samtidig-diagonalisering.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Samtidig diagonalisering</a>
<br><a data-href="Nilpotenta operatorer" href="matematik/linjär-algebra/nilpotenta-operatorer.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Nilpotenta operatorer</a><a data-href="Nilpotenta operatorer" href="matematik/linjär-algebra/nilpotenta-operatorer.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Nilpotenta operatorer</a>
<br><a data-href="Jordans Normalform" href="matematik/linjär-algebra/jordans-normalform.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Jordans Normalform</a><a data-href="Jordans Normalform" href="matematik/linjär-algebra/jordans-normalform.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Jordans Normalform</a> <br><a data-href="Inre produktrum" href="matematik/linjär-algebra/inre-produktrum.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Inre produktrum</a><a data-href="Inre produktrum" href="matematik/linjär-algebra/inre-produktrum.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Inre produktrum</a>
<br><a data-href="Norm och vinklar" href="matematik/linjär-algebra/norm-och-vinklar.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Norm och vinklar</a><a data-href="Norm och vinklar" href="matematik/linjär-algebra/norm-och-vinklar.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Norm och vinklar</a>
<br><a data-href="Ortogonalitet och projektioner" href="matematik/linjär-algebra/ortogonalitet-och-projektioner.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Ortogonalitet och projektioner</a><a data-href="Ortogonalitet och projektioner" href="matematik/linjär-algebra/ortogonalitet-och-projektioner.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Ortogonalitet och projektioner</a>
<br><a data-href="Konvergens och Cauchyföljder" href="matematik/linjär-algebra/konvergens-och-cauchyföljder.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Konvergens och Cauchyföljder</a><a data-href="Konvergens och Cauchyföljder" href="matematik/linjär-algebra/konvergens-och-cauchyföljder.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Konvergens och Cauchyföljder</a>
<br><a data-href="Hilbertrum" href="matematik/linjär-algebra/hilbertrum.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Hilbertrum</a><a data-href="Hilbertrum" href="matematik/linjär-algebra/hilbertrum.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Hilbertrum</a>
<br><a data-href="Duala rum" href="matematik/linjär-algebra/duala-rum.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Duala rum</a><a data-href="Duala rum" href="matematik/linjär-algebra/duala-rum.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Duala rum</a>
<br><a data-href="Adjungerade avbildningar" href="matematik/linjär-algebra/adjungerade-avbildningar.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Adjungerade avbildningar</a><a data-href="Adjungerade avbildningar" href="matematik/linjär-algebra/adjungerade-avbildningar.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Adjungerade avbildningar</a>
<br><a data-href="Ortogonala och unitära operatorer" href="matematik/linjär-algebra/ortogonala-och-unitära-operatorer.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Ortogonala och unitära operatorer</a><a data-href="Ortogonala och unitära operatorer" href="matematik/linjär-algebra/ortogonala-och-unitära-operatorer.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Ortogonala och unitära operatorer</a>
<br><a data-href="Normala operatorer" href="matematik/linjär-algebra/normala-operatorer.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Normala operatorer</a><a data-href="Normala operatorer" href="matematik/linjär-algebra/normala-operatorer.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Normala operatorer</a> <br><a data-href="Duala avbildningar" href="matematik/linjär-algebra/duala-avbildningar.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Duala avbildningar</a><a data-href="Duala avbildningar" href="matematik/linjär-algebra/duala-avbildningar.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Duala avbildningar</a>
<br><a data-href="Singulärvärdesuppdelning" href="matematik/linjär-algebra/singulärvärdesuppdelning.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Singulärvärdesuppdelning</a><a data-href="Singulärvärdesuppdelning" href="matematik/linjär-algebra/singulärvärdesuppdelning.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Singulärvärdesuppdelning</a>
<br><a data-href="Moore-Penrose-inversen" href="matematik/linjär-algebra/moore-penrose-inversen.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Moore-Penrose-inversen</a><a data-href="Moore-Penrose-inversen" href="matematik/linjär-algebra/moore-penrose-inversen.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Moore-Penrose-inversen</a> <br><a data-href="Markovkedjor och sannolikhetsmatriser" href="matematik/linjär-algebra/markovkedjor-och-sannolikhetsmatriser.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Markovkedjor och sannolikhetsmatriser</a><a data-href="Markovkedjor och sannolikhetsmatriser" href="matematik/linjär-algebra/markovkedjor-och-sannolikhetsmatriser.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Markovkedjor och sannolikhetsmatriser</a> <br><a data-href="Multilinjära avbildningar" href="matematik/linjär-algebra/multilinjära-avbildningar.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Multilinjära avbildningar</a><a data-href="Multilinjära avbildningar" href="matematik/linjär-algebra/multilinjära-avbildningar.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Multilinjära avbildningar</a>
<br><a data-href="Tensorer" href="matematik/linjär-algebra/tensorer.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Tensorer</a><a data-href="Tensorer" href="matematik/linjär-algebra/tensorer.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Tensorer</a>
<br><a data-href="Symmetriska och alternerande tensorer" href="matematik/linjär-algebra/symmetriska-och-alternerande-tensorer.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Symmetriska och alternerande tensorer</a><a data-href="Symmetriska och alternerande tensorer" href="matematik/linjär-algebra/symmetriska-och-alternerande-tensorer.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Symmetriska och alternerande tensorer</a>
<br><a data-href="Alternerande och skevsymmetriska avbildningar" href="matematik/linjär-algebra/alternerande-och-skevsymmetriska-avbildningar.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Alternerande och skevsymmetriska avbildningar</a><a data-href="Alternerande och skevsymmetriska avbildningar" href="matematik/linjär-algebra/alternerande-och-skevsymmetriska-avbildningar.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Alternerande och skevsymmetriska avbildningar</a>
<br><a data-href="Yttre potenser" href="matematik/linjär-algebra/yttre-potenser.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Yttre potenser</a><a data-href="Yttre potenser" href="matematik/linjär-algebra/yttre-potenser.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Yttre potenser</a>
<br><a data-href="Yttre algebran" href="matematik/linjär-algebra/yttre-algebran.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Yttre algebran</a><a data-href="Yttre algebran" href="matematik/linjär-algebra/yttre-algebran.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Yttre algebran</a> <br><a data-href="Kroppar" href="matematik/linjär-algebra/kroppar.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Kroppar</a><a data-href="Kroppar" href="matematik/linjär-algebra/kroppar.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Kroppar</a>
]]></description><link>matematik/linjär-algebra.html</link><guid isPermaLink="false">Matematik/Linjär algebra.md</guid><pubDate>Wed, 08 Jan 2025 10:37:02 GMT</pubDate></item><item><title><![CDATA[Jordans Normalform]]></title><description><![CDATA[<a href=".?query=tag:matematik/linjär-algebra" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik/linjär-algebra">#matematik/linjär-algebra</a> <br>Tyvärr går ju inte alla <a data-tooltip-position="top" aria-label="Linjära avbildningar" data-href="Linjära avbildningar" href="matematik/linjär-algebra/linjära-avbildningar.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Operatorer</a><a data-tooltip-position="top" aria-label="Linjära avbildningar" data-href="Linjära avbildningar" href="matematik/linjär-algebra/linjära-avbildningar.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Operatorer</a> att <a data-tooltip-position="top" aria-label="Egenvärden och egenvektorer" data-href="Egenvärden och egenvektorer" href="matematik/linjär-algebra/egenvärden-och-egenvektorer.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Diagonalisera</a><a data-tooltip-position="top" aria-label="Egenvärden och egenvektorer" data-href="Egenvärden och egenvektorer" href="matematik/linjär-algebra/egenvärden-och-egenvektorer.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Diagonalisera</a>, men vi kan ändå alltid göra någonting som är så bra som möjligt, åtminstone om vår kropp är algebraiskt sluten, till exempel om kroppen är . När vi diagonaliserar en operator vill vi givet egenvärdena till hitta en bas bestående av vektorer i egenrummen . Principen med Jordans normalform är att betrakta så kallade generaliserade egenrum som är på formen där är den algebraiska multipliciteten för . Och det visar sig att det alltid (om kroppen är algebraiskt sluten) går att hitta en bas bestående av vektorer från dessa generaliserade egenrum.<br>Beviset för att detta alltid fungerar samt metoden för att ta fram en sådan bas är ganska komplexa. Vi kommer överföra beviset till <a data-href="Nilpotenta operatorer" href="matematik/linjär-algebra/nilpotenta-operatorer.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Nilpotenta operatorer</a><a data-href="Nilpotenta operatorer" href="matematik/linjär-algebra/nilpotenta-operatorer.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Nilpotenta operatorer</a>; läs gärna igenom den sidan!<br>Sats: Uppdelning i generaliserade egenrum
Om är linjär, är ändligdimensionellt och är algebraiskt slutet, samt om är de unika egenvärdena till med respektive algebraiska multipliciteter , så är den <a data-tooltip-position="top" aria-label="Direkta summor" data-href="Direkta summor" href="matematik/linjär-algebra/direkta-summor.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Inre direkta summan</a><a data-tooltip-position="top" aria-label="Direkta summor" data-href="Direkta summor" href="matematik/linjär-algebra/direkta-summor.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Inre direkta summan</a> av delrum med respektive dimensioner sådana att alla är invarianta under och sådana att alla är nilpotenta på .
Bevis:<br>
Att ska vara algebraiskt slutet är så att vi dels vet att det finns egenvärden, och dels så att vi vet att karaktäristiska polynom kan faktoriseras till linjära faktorer.
För alla , definiera . Vi använder <a data-tooltip-position="top" aria-label="Nilpotenta operatorer > ^844b4f" data-href="Nilpotenta operatorer#^844b4f" href="matematik/linjär-algebra/nilpotenta-operatorer.html#^844b4f" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">denna sats</a><a data-tooltip-position="top" aria-label="Nilpotenta operatorer > ^844b4f" data-href="Nilpotenta operatorer#^844b4f" href="matematik/linjär-algebra/nilpotenta-operatorer.html#^844b4f" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">denna sats</a> på för att få delrum och s.a. är nilpotent på och inverterbar på . Eftersom och är invarianta under så är de även invarianta under för alla , därmed under och även alla . Därmed är determinanten av lika med produkten av determinanten av på respektive , för alla . Vi vet även att är det enda egenvärdet till på eftersom på annars inte hade varit nilpotent, och vi vet även att inte är ett egenvärde till på eftersom är inverterbar på . Detta ger att det karaktäristiska polynomet till på är eftersom alla karaktäristiska polynom kan faktoriseras till linjära faktorer, alltså är . Vi vet även att är disjunkt med spannet av alla andra , eftersom om ett nollskilt är i och i spannet av alla andra så finns s.a. och
Den sista ekvationen gäller eftersom om är i spannet av alla för så är alltså en summa av en vektor från varje och eftersom alla är invarianta under alla kommer varje döda termen från . Men eftersom även är invariant under alla betyder ovanstående produkt att det finns något nollskilt och s.a. , men det betyder att är ett egenvärde till på , vilket är en motsägelse. Därmed vet vi att är en direkt summa, alltså är I denna sats är alltså alla de generaliserade egenrummen, så vi ser här att det mycket riktigt alltid finns en bas av generaliserade egenvektorer. Det sista steget i Jordans normalform är att välja denna bas på ett sätt som gör matrisen allra enklast. Detta görs ofta genom att för varje generaliserat egenrum hitta så kallade jordankedjor av basvektorer på formen för något .<br>Beviset kommer <a data-tooltip-position="top" aria-label="https://www.ma.rhul.ac.uk/~uvah099/Maths/JNFfinal.pdf" rel="noopener nofollow" class="external-link is-unresolved" href="https://www.ma.rhul.ac.uk/~uvah099/Maths/JNFfinal.pdf" target="_self">härifrån</a>.Sats: Kedjeuppdelning med nilpotent operator
Om är linjär och nilpotent så finns en bas till på formen
för något och några , sådana att för alla .
Bevis:
Vi använder induktion på . Om så är satsen uppenbarligen sann (tomma mängden är bas). Den är också uppenbarligen sann om , eftersom godtycklig bas till då fungerar. Annars, om och , så vet vi att eftersom om så är inte nilpotent. Vårt induktionsantagande är därmed att satsen är sann för restriktionen av till . Då finns alltså ett samt och s.a.
är en bas för , där för alla . För alla kan vi då välja s.a. . Vi vet då att innehåller de linjärt oberoende vektorerna , alltså kan vi utvidga dessa till en bas av genom tillägg av vektorer . Vi vill nu visa att
är en bas för . För att visa linjärt oberoende, antag att
Genom att applicera på båda led får vi
alltså är för alla och eftersom alla vektorer i denna summa är linjärt oberoende. Kvar får vi då<br>
vilket ger att resten av konstanterna är eftersom vi sa att dessa utgör en bas för . Alltså är vår mängd linjärt oberoende. Med <a data-tooltip-position="top" aria-label="Kvotrum > ^a470a8" data-href="Kvotrum#^a470a8" href="matematik/linjär-algebra/kvotrum.html#^a470a8" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Dimensionssatsen</a><a data-tooltip-position="top" aria-label="Kvotrum > ^a470a8" data-href="Kvotrum#^a470a8" href="matematik/linjär-algebra/kvotrum.html#^a470a8" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Dimensionssatsen</a> visar vi enkelt att de även spänner upp hela ; eftersom vi vet att och att får vi att vilket är antalet vektorer i vår mängd, alltså är det en bas.
Om vi applicerar dessa två satser får vi alltså matrisen på Jordans normalform, alltså på diagonalblockform där varje block har ett egenvärde längs hela diagonalen och ettor över varje diagonalelement.Från beviset framgår även en rekursiv algoritm för att hitta en bas på formen från satsen:Metod: Hitta Jordans normalform för nilpotenta operatorer Om , välj en godtycklig bas för . Annars, använd algoritmen på restriktionen av till för att få en bas på formen Hitta sedan s.a. . Utvidga sedan till en bas av genom tillägg av . Då är en bas för på den efterfrågade formen.
Vi kan nu använda dessa satser och metoden ovan för att hitta Jordans normalform i den allmänna fallet.<br>Metod: Hitta Jordans normalform
Hitta egenvärdena till med algebraiska multipliciteter . För alla : Hitta en bas till det generaliserade egenrummet . Använd sedan <a data-tooltip-position="top" aria-label="^5c37c9" data-href="#^5c37c9" href="#^5c37c9" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">metoden för nilpotenta operatorer</a><a data-tooltip-position="top" aria-label="^5c37c9" data-href="#^5c37c9" href="#^5c37c9" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">metoden för nilpotenta operatorer</a> på operatorn och rummet för att ta fram en Jordanbas till . Samtliga Jordanbaser till utgör tillsammans en Jordanbas för .
]]></description><link>matematik/linjär-algebra/jordans-normalform.html</link><guid isPermaLink="false">Matematik/Linjär algebra/Jordans Normalform.md</guid><pubDate>Thu, 02 Jan 2025 21:33:32 GMT</pubDate></item><item><title><![CDATA[Minimalpolynom]]></title><description><![CDATA[<a href=".?query=tag:matematik/linjär-algebra" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik/linjär-algebra">#matematik/linjär-algebra</a> <br>Definition: Minimalpolynom
Givet en <a data-tooltip-position="top" aria-label="Linjära avbildningar" data-href="Linjära avbildningar" href="matematik/linjär-algebra/linjära-avbildningar.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Linjär operator</a><a data-tooltip-position="top" aria-label="Linjära avbildningar" data-href="Linjära avbildningar" href="matematik/linjär-algebra/linjära-avbildningar.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Linjär operator</a> där är ändligdimensionellt så är minimalpolynomet för polynomet av lägsta grad och med ledande koefficient som uppfyller att Sats: Unikt minimalpolynom finns alltid Om är linjär och är ändligdimensionellt så har ett unikt minimalpolynom .
Bevis:
Vi kan bilda en linjär avbildning så att
och vi vet då att eftersom är oändligdimensionellt medan inte är det. Det finns en lägsta grad som förekommer i , och endast ett polynom av den graden har ledande koefficient eftersom om och båda är av den lägsta graden och har ledande koefficient så har polynomet lägre grad än den lägsta graden och måste då vara nollpolynomet, alltså är .
Vi kan definiera allt på precis samma sätt för matriser istället, så alla matriser har alltså ett minimalpolynom s.a. .Sats: Egenvärden är rötter till minimalpolynomet
Om är linjär med minimalpolynom och är en egenvektor till med egenvärde så är
Bevis:
Vi vet att vilket ger att eftersom .
Sats: Minimalpolynomet delar alla polynom i Om är linjär med minimalpolynom och är ett polynom s.a. så är en faktor i .
Bevis:
Med polynomdivision vet vi att
för polynom där eller . Vi får då att
alltså är vilket ger att eftersom har lägre grad än minimalpolynomet.
<br>Vi vill nu visa Cayley-Hamiltons sats, som säger att om är det karakteristiska polynomet till en matris så är . Det är ett icketrivialt bevis, så vi delar upp det i några lemman. Beviset kommer <a data-tooltip-position="top" aria-label="https://math.stackexchange.com/a/4331887/1180648" rel="noopener nofollow" class="external-link is-unresolved" href="https://math.stackexchange.com/a/4331887/1180648" target="_self">härifrån</a>.<br>Lemma:
Om med <a data-tooltip-position="top" aria-label="Karakteristiska polynom" data-href="Karakteristiska polynom" href="matematik/linjär-algebra/karakteristiska-polynom.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Karakteristiskt polynom</a><a data-tooltip-position="top" aria-label="Karakteristiska polynom" data-href="Karakteristiska polynom" href="matematik/linjär-algebra/karakteristiska-polynom.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Karakteristiskt polynom</a> och det finns en vektor s.a. är en bas till , så är .
Bevis:
Det finns då alltså med s.a.
I ovanstående bas blir matrisen för vilket ger det karaktäristiska polynomet (upp till ett teckenbyte)
Detta verifieras enkelt genom att skriva ut . Vi har alltså att enligt första ekvationen, vilket ger att för alla , alltså avbildar alla basvektorer på , alltså är .
Lemma:
Om är en blockmatris s.a. och så är .
Bevis:
Eftersom determinanten av en sådan övertriangulär blockmatris är produkten av determinanterna av diagonalblocken och får vi att , vilket ger att Cayley-Hamiltons sats
Om med karakteristiskt polynom så är .
Bevis:<br>
Vi använder induktion på . Om så är satsen uppenbart sann. Nu om så kan vi välja ett nollskilt och hitta det minsta s.a. är linjärt beroende. Vi vet att eftersom . Om så är vi klara enligt <a data-tooltip-position="top" aria-label="^bfedaa" data-href="#^bfedaa" href="#^bfedaa" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Lemma</a><a data-tooltip-position="top" aria-label="^bfedaa" data-href="#^bfedaa" href="#^bfedaa" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Lemma</a>. Annars kan vi utvidga till en bas av , och då blir m.a.p denna bas blockdiagonal på formen där är av storlek eftersom vi visste att de första basvektorerna avbildas på varandra, och den :e basvektorn avbildas på en linjärkombination av de första. Därmed är av storlek vilket är mindre än eftersom , så enligt vårt induktionsantagande är då alltså och vilket enligt <a data-tooltip-position="top" aria-label="^eb4e35" data-href="#^eb4e35" href="#^eb4e35" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Lemma</a><a data-tooltip-position="top" aria-label="^eb4e35" data-href="#^eb4e35" href="#^eb4e35" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Lemma</a> medför att . ]]></description><link>matematik/linjär-algebra/minimalpolynom.html</link><guid isPermaLink="false">Matematik/Linjär algebra/Minimalpolynom.md</guid><pubDate>Tue, 31 Dec 2024 09:55:47 GMT</pubDate></item><item><title><![CDATA[Norm och vinklar]]></title><description><![CDATA[<a href=".?query=tag:matematik/linjär-algebra" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik/linjär-algebra">#matematik/linjär-algebra</a> <br>Den <a data-tooltip-position="top" aria-label="Inre produktrum" data-href="Inre produktrum" href="matematik/linjär-algebra/inre-produktrum.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Inre produkten</a><a data-tooltip-position="top" aria-label="Inre produktrum" data-href="Inre produktrum" href="matematik/linjär-algebra/inre-produktrum.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Inre produkten</a> ger oss ett mått på både längd och vinklar. Vi börjar med längd.Definition: Norm
Om där är ett inre produktrum så ges normen av av Här ser vi en anledning att definiera inre produkten som positivt definit: roten är alltid definierad och endast nollvektorn har norm , vilket är geometriskt intuitivt.Men för att normen ska ge en metrik (alltså för att ett inre produktrum ska vara ett s.k. metriskt rum) måste den även uppfylla triangelolikheten (triangelolikheten är med i definitionen av ett metriskt rum), och det behöver vi först Cauchy-Schwarz olikhet för.Sats: Cauchy-Schwarz olikhet
Om där är ett inre produktrum så är
med likhet om och endast om och är linjärt beroende.
Bevis:
Om någon av och är noll så är satsen uppenbar. Antag därför att . Vi får då att
vilket ger att
Om och är linjärt beroende så är för något , alltså är Triangelolikheten
Om där är ett inre produktrum så är
Bevis:
Vi får att
enligt Cauchy-Schwarz olikhet.
Vi har alltså nu visat att inre produktrum är metriska rum. Om kroppen är definierar vi även vinklar.Definition: Vinkel
Om är ett inre produktrum på , samt och så är vinkeln mellan och vinkeln som uppfyller ]]></description><link>matematik/linjär-algebra/norm-och-vinklar.html</link><guid isPermaLink="false">Matematik/Linjär algebra/Norm och vinklar.md</guid><pubDate>Mon, 30 Dec 2024 22:33:49 GMT</pubDate></item><item><title><![CDATA[Direkta summor]]></title><description><![CDATA[<a href=".?query=tag:matematik/linjär-algebra" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik/linjär-algebra">#matematik/linjär-algebra</a> Ett sätt att kombinera två vektorrum och är genom en direkt summa. Vi kan se direkta summor som att bilda ett nytt vektorrum vars vektorer är par av vektorer från respektive , där vi utför addition och skalärmultiplikation komponentvis.Definition: Yttre direkt summa
Den yttre direkta summan av två vektorrum och är ett vektorrum s.a.
där för alla , och . Om och är delrum till något gemensamt vektorrum så är addition där och definierat. Vi inför då även summan av delrummen.Definition: Summa av delrum
Summan av två delrum och av ett gemensamt vektorrum är ett delrum s.a. Om nu och är delrum sådana att varje vektor kan skrivas på ett unikt sätt som för något och så kan vi se det som att varje vektor i innehåller informationen av en vektor i och en vektor i . Detta liknar den yttre direkta summan väldigt mycket, och vi kallar då summan för en inre direkt summa eller bara direkt summa och skriver även för att beteckna den inre direkta summan.Definition: Inre direkt summa
Om och är delrum av ett gemensamt vektorrum och om varje vektor kan skrivas på ett unikt sätt som för något och så kallar vi summan för en inre direkt summa och skriver för att indikera att summan är direkt.
Anmärkning: kan beteckna antingen inre eller yttre direkt summa
När skrivs är det alltså inte alltid viktigt om det är en yttre eller inre direkt summa som avses, eftersom dessa i någon mening innehåller precis samma information, och därför kan ses som samma sak. Om däremot och inte är delrum av något gemensamt vektorrum så avses alltid den yttre direkta summan.
Med definitionen vi givit nu är det inte uppenbart hur vi avgör om är en direkt summa eller inte. Som tur är finns ett lätt sätt att kontrollera det.Sats: direkt Om och är delrum av ett gemensamt vektorrum så är en direkt summa om och endast om .
Bevis:
Vi visar först . Antag alltså att är en direkt summa, och antag att . Då är , alltså är som en summa av en vektor i och en i . Eftersom alltid uppfyller detta och denna summa ska vara unik måste alltså , alltså är .
Nu visar vi . Antag alltså att , och antag att för några och . Då är , vilket implicerar att eftersom de är additiva inverser av varandra. Detta ger att , alltså är och vilket ger att alla är unikt bestämda som en summa .
Nu tillbaka till världen där vi betraktar den inre och yttre direkta summan som samma sak. I följande sats används den yttre direkta summan i beviset, men satsen gäller alltså för både yttre on inre direkta summor.Sats: Om och är ändligdimensionella så är
Bevis:
Givet baser och för respektive får vi att
är en bas för eftersom varje vektor ]]></description><link>matematik/linjär-algebra/direkta-summor.html</link><guid isPermaLink="false">Matematik/Linjär algebra/Direkta summor.md</guid><pubDate>Mon, 30 Dec 2024 17:43:43 GMT</pubDate></item><item><title><![CDATA[Moore-Penrose-inversen]]></title><description><![CDATA[<a href=".?query=tag:matematik/linjär-algebra" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik/linjär-algebra">#matematik/linjär-algebra</a> <br>Med <a data-href="Singulärvärdesuppdelning" href="matematik/linjär-algebra/singulärvärdesuppdelning.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Singulärvärdesuppdelning</a><a data-href="Singulärvärdesuppdelning" href="matematik/linjär-algebra/singulärvärdesuppdelning.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Singulärvärdesuppdelning</a> kan vi introducera en slags pseudoinvers av matriser. Vi vill såklart att om en matris är inverterbar så ska pseudoinversen vara inversen, och annars i någon mening approximera en invers.Definition: Moore-Penrose-invers
Om är en ändlig matris med tal i eller och är singulärvärdesuppdelningen av så är Moore-Penrose-inversen av matrisen
där är fast varje nollskilt element har ersatts med .
]]></description><link>matematik/linjär-algebra/moore-penrose-inversen.html</link><guid isPermaLink="false">Matematik/Linjär algebra/Moore-Penrose-inversen.md</guid><pubDate>Mon, 30 Dec 2024 16:43:55 GMT</pubDate></item><item><title><![CDATA[Duala avbildningar]]></title><description><![CDATA[<a href=".?query=tag:matematik/linjär-algebra" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik/linjär-algebra">#matematik/linjär-algebra</a> Definition: Dual avbildning
Om är linjär så är den duala avbildningen en linjär avbildning som ges av <br>Sats: Transponatet beskriver den duala avbildningen
Om är linjär och är matrisen för m.a.p godtyckliga baser och för respektive , och om det finns <a data-tooltip-position="top" aria-label="Duala rum > ^b96c17" data-href="Duala rum#^b96c17" href="matematik/linjär-algebra/duala-rum.html#^b96c17" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Duala baser</a><a data-tooltip-position="top" aria-label="Duala rum > ^b96c17" data-href="Duala rum#^b96c17" href="matematik/linjär-algebra/duala-rum.html#^b96c17" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Duala baser</a> och , så är matrisen för den duala avbildningen m.a.p de duala baserna.
Bevis:
Låt och låt vara matrisen för m.a.p de duala baserna. Vi vet då att
alltså är .
]]></description><link>matematik/linjär-algebra/duala-avbildningar.html</link><guid isPermaLink="false">Matematik/Linjär algebra/Duala avbildningar.md</guid><pubDate>Mon, 30 Dec 2024 13:03:26 GMT</pubDate></item><item><title><![CDATA[Nilpotenta operatorer]]></title><description><![CDATA[<a href=".?query=tag:matematik/linjär-algebra" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik/linjär-algebra">#matematik/linjär-algebra</a> <br>(Inspirerat av <a data-tooltip-position="top" aria-label="https://download.tuxfamily.org/openmathdep/algebra_linear/Finite_Vector_Spaces-Halmos.pdf#page=119" rel="noopener nofollow" class="external-link is-unresolved" href="https://download.tuxfamily.org/openmathdep/algebra_linear/Finite_Vector_Spaces-Halmos.pdf#page=119" target="_self">denna bok</a>)<br>
Att en <a data-tooltip-position="top" aria-label="Linjära avbildningar" data-href="Linjära avbildningar" href="matematik/linjär-algebra/linjära-avbildningar.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Linjär operator</a><a data-tooltip-position="top" aria-label="Linjära avbildningar" data-href="Linjära avbildningar" href="matematik/linjär-algebra/linjära-avbildningar.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Linjär operator</a> i någon mening är noll är ju intuitivt motsatsen till att den är inverterbar. Vi vet ju dock att bara för att en operator inte är nolloperatorn behöver den inte vara inverterbar. Till exempel, om som <a data-tooltip-position="top" aria-label="Direkta summor" data-href="Direkta summor" href="matematik/linjär-algebra/direkta-summor.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Inre direkt summa</a><a data-tooltip-position="top" aria-label="Direkta summor" data-href="Direkta summor" href="matematik/linjär-algebra/direkta-summor.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Inre direkt summa</a> så kan ju vara inverterbar på och noll på .Då kan vi nu fråga oss om alla operatorer går att dela upp i en inverterbar och en nolloperator på det sättet. Och vi kan ganska enkelt komma fram till att nej, det går inte alltid. Vi kan ju till exempel avbilda en basvektor på en annan basvektor , som i sin tur avbildas på . För att få med det här slags fallet måste vi alltså bredda vår bild av vad som menas med att en operator "i någon mening" är noll. Vi inför därför begreppet nilpotent operator, som är en operator s.a. någon potens av är nolloperatorn, det vill säga att bara vi applicerar tillräckligt många gånger får vi alltid noll. I vårt senaste exempel skulle alltså på vara nilpotent.Definition: Nilpotent operator
Om är linjär och för något så kallas nilpotent.
Uppenbarligen är nilpotenta operatorer inte inverterbara eftersom då också hade varit inverterbar. Och precis som innan kan vi tyvärr fortfarande inte säga att en godtycklig operator antingen är inverterbar eller nilpotent, eftersom om så kan vara inverterbar på och nilpotent på . Men då kan vi nu precis som innan fråga oss om alla operator går att dela upp i en inverterbar och en nilpotent operator på det sättet. Och om är ändligdimensionellt så visar det sig att ja, det går faktiskt alltid! Sats: Operatorer kan delas upp unikt i en inverterbar och en nilpotent operator
Om är linjär och är ändligdimensionellt så finns unika delrum så att , samt så att restriktionen av till är inverterbar, och så att restriktionen av till är nilpotent.
Bevis:
Vi börjar med att betrakta nollrummen . Uppenbarligen är eftersom om så är för alla . Vi märker sedan att om för något så är som vi ser snabbt eftersom<br>
alltså är vilket ger att . Eftersom är ändligdimensionellt så kan inte nollrummen fortsätta växa för evigt, alltså finns ett minsta s.a. . Vi vet att är invariant under eftersom . Vi låter nu och vet då att , alltså är invariant under . Vårt mål nu är att visa att samt att är inverterbar på samt nilpotent på .
Om så är alltså och så finns ett s.a. . Detta ger att , vilket ger att eftersom per vår definition av så är . Vi har alltså att , alltså är en direkt summa. Från <a data-tooltip-position="top" aria-label="Kvotrum > ^a470a8" data-href="Kvotrum#^a470a8" href="matematik/linjär-algebra/kvotrum.html#^a470a8" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Dimensionssatsen</a><a data-tooltip-position="top" aria-label="Kvotrum > ^a470a8" data-href="Kvotrum#^a470a8" href="matematik/linjär-algebra/kvotrum.html#^a470a8" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Dimensionssatsen</a> samt <a data-tooltip-position="top" aria-label="Direkta summor > ^42a317" data-href="Direkta summor#^42a317" href="matematik/linjär-algebra/direkta-summor.html#^42a317" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">denna sats</a><a data-tooltip-position="top" aria-label="Direkta summor > ^42a317" data-href="Direkta summor#^42a317" href="matematik/linjär-algebra/direkta-summor.html#^42a317" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">denna sats</a> följer då även att
vilket ger att . Från definitionen av och följer att på är nilpotent med exponent . Slutligen om och så är , alltså är , men , så , alltså är inverterbar på .
Slutligen visar vi att denna uppdelning är unik. Antag därför att är delrum till sådana att och sådana att på är inverterbar och på är nilpotent. Vi vet då att för alla , alltså är , och vi vet även att för något , alltså är . Detta ger att och .
]]></description><link>matematik/linjär-algebra/nilpotenta-operatorer.html</link><guid isPermaLink="false">Matematik/Linjär algebra/Nilpotenta operatorer.md</guid><pubDate>Fri, 27 Dec 2024 12:51:26 GMT</pubDate></item><item><title><![CDATA[Yttre potenser]]></title><description><![CDATA[<a href=".?query=tag:matematik/linjär-algebra" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik/linjär-algebra">#matematik/linjär-algebra</a> <br>Yttre potenser är <a data-href="Tensorer" href="matematik/linjär-algebra/tensorer.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Tensorer</a><a data-href="Tensorer" href="matematik/linjär-algebra/tensorer.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Tensorer</a> med en liten twist. Med tensorer studerar vi alla <a data-href="Multilinjära avbildningar" href="matematik/linjär-algebra/multilinjära-avbildningar.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Multilinjära avbildningar</a><a data-href="Multilinjära avbildningar" href="matematik/linjär-algebra/multilinjära-avbildningar.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Multilinjära avbildningar</a>, medan med yttre potenser studerar vi endast <a data-tooltip-position="top" aria-label="Alternerande och skevsymmetriska avbildningar" data-href="Alternerande och skevsymmetriska avbildningar" href="matematik/linjär-algebra/alternerande-och-skevsymmetriska-avbildningar.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Alternerande avbildningar</a><a data-tooltip-position="top" aria-label="Alternerande och skevsymmetriska avbildningar" data-href="Alternerande och skevsymmetriska avbildningar" href="matematik/linjär-algebra/alternerande-och-skevsymmetriska-avbildningar.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Alternerande avbildningar</a>. Dessa liknar <a data-tooltip-position="top" aria-label="Symmetriska och alternerande tensorer" data-href="Symmetriska och alternerande tensorer" href="matematik/linjär-algebra/symmetriska-och-alternerande-tensorer.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Alternerande tensorer</a><a data-tooltip-position="top" aria-label="Symmetriska och alternerande tensorer" data-href="Symmetriska och alternerande tensorer" href="matematik/linjär-algebra/symmetriska-och-alternerande-tensorer.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Alternerande tensorer</a> och de kan egentligen definieras utifrån varandra. Men vi ger ändå yttre potenser en egen definition utifrån dess universella egenskap, som liknar tensorproduktens universella egenskap väldigt mycket.Definition: Yttre potensen En yttre potensen av grad av ett vektorrum är ett vektorrum tillsammans med en alternerande -multilinjär avbildning , så att det för alla vektorrum och alla alternerande -multilinjära avbildningar finns en unik linjär avbildning sådan att , det vill sägaför alla .
Vi får en del egenskaper som är mer eller mindre samma som för tensorer, så vi upprepar inte bevisen i de fallen.Sats: spänner upp .
Om och är vektorrum och är en yttre potens av grad av så är<br>
Bevis: se <a data-tooltip-position="top" aria-label="Tensorer > ^460107" data-href="Tensorer#^460107" href="matematik/linjär-algebra/tensorer.html#^460107" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Tensorer</a><a data-tooltip-position="top" aria-label="Tensorer > ^460107" data-href="Tensorer#^460107" href="matematik/linjär-algebra/tensorer.html#^460107" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Tensorer</a>.
<br>Sats: Yttre potensen är unik upp till unik isomorfi
Om samt är möjliga yttre potenser av grad av så finns en unik isomorfi mellan och .
Bevis: se <a data-tooltip-position="top" aria-label="Tensorer > ^b6ed60" data-href="Tensorer#^b6ed60" href="matematik/linjär-algebra/tensorer.html#^b6ed60" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Tensorer</a><a data-tooltip-position="top" aria-label="Tensorer > ^b6ed60" data-href="Tensorer#^b6ed60" href="matematik/linjär-algebra/tensorer.html#^b6ed60" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Tensorer</a>.
Nu kan vi precis som med tensorer införa en bas till för att dels visa existens och dels för att göra livet lättare. Beviset liknar återigen motsvarigheten för tensorer en del, men vi skriver ut det ändå.Sats: Yttre potens med bas
Låt vara en bas för , låt vara ett vektorrum och låt vara -multilinjär och alternerande. Då är en yttre potens av grad av om och endast om
är en bas för .
Bevis:
Vi börjar med att visa . Antag alltså att är en yttre potens av grad av . Givet finns då tal och så att
alltså spänner alla upp . Däremot är dessa inte linjärt oberoende eftersom är alternerande. Vi vet att vi kan sortera index med ett ändligt antal platsbyten, alltså skiljer alla permuteringar varandra av som mest ett teckenbyte. Därmed spänner upp alla , vilket alltså spänner upp . För att visa linjärt oberoende utnyttjar vi att en alternerande -multilinjär avbildning bestäms entydigt av alla för alla . Vi kan därmed välja och så att alla är linjärt oberoende. Det ger att alla är linjärt oberoende, vilket medför att alla är linjärt oberoende, alltså är en bas för .
Vi visar nu . Antag alltså att är en bas för . Givet en alternerande -multilinjär avbildning vet vi då att den motsvarande linjära avbildningen unikt måste bestämmas av att
för alla . Då gäller att
för alla , alltså är och är unikt bestämd, så mycket riktigt är en yttre potens av grad av .
Från detta följer att om är ändligdimensionellt så är
eftersom det är så många olika uppsättningar index som finns.]]></description><link>matematik/linjär-algebra/yttre-potenser.html</link><guid isPermaLink="false">Matematik/Linjär algebra/Yttre potenser.md</guid><pubDate>Fri, 27 Dec 2024 12:51:26 GMT</pubDate></item><item><title><![CDATA[Hilbertrum]]></title><description><![CDATA[<a href=".?query=tag:matematik/linjär-algebra" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik/linjär-algebra">#matematik/linjär-algebra</a> <br>Definition: Hilbertrum
Ett Hilbertrum är ett <a data-tooltip-position="top" aria-label="Konvergens och Cauchyföljder > ^e291a4" data-href="Konvergens och Cauchyföljder#^e291a4" href="matematik/linjär-algebra/konvergens-och-cauchyföljder.html#^e291a4" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">fullständigt</a><a data-tooltip-position="top" aria-label="Konvergens och Cauchyföljder > ^e291a4" data-href="Konvergens och Cauchyföljder#^e291a4" href="matematik/linjär-algebra/konvergens-och-cauchyföljder.html#^e291a4" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">fullständigt</a> inre produktrum.
Till exempel är alla och hilbertrum. Vi tar även upp två andra viktiga.Definition: Om kroppen eller så definierar vi Hilbertrummet
med inre produkt Sats: är väldefinierad är väldefinierad, alltså ett Hilbertrum.
Bevis:
För godtyckliga så är
och vi får att , alltså är summan och inre produkten väldefinierad. Kvar är att visa att alla Cauchyföljder konvergerar. Låt vara en Cauchyföljd av följder från . Bilda sedan för alla följden . Denna följd är en Cauchyföljd av tal från eftersom följden av följder är en Cauchyföljd. Eftersom vi vet att är fullständigt ( eller ) så vet vi att varje sådan talföljd konvergerar mot något . Bilda därmed följden . Vi får då att eftersom varje term i går att få godtyckligt litet, så vi kan tvinga serien till att konvergera till ett godtyckligt litet tal.
Definition: Funktionsrummet Börja med rummet med inre produkt . Låt sedan
och låt
Bilda sedan kvoten med . Detta är dock inte en inre produkt så låt sedan även
och bilda slutligen funktionsrummet .
Denna definition kräver dessa jobbiga kvoter eftersom, för det första, när vi tar vektorrummet av kontinuerliga funktioner så finns följder av funktioner vars gränsvärde inte är en kontinuerlig funktion; till exempel kan man tänka sig en följd av funktioner där lutningen i någon punkt ökar och går mot , så att gränsvärdesfunktionen har ett hopp i den punkten. För det andra, när vi även tagit med dessa gränsvärdesfunktioner så har vi inte längre en inre produkt eftersom det finns gränsvärdesfunktioner som är nollskild på en nollmängd och därmed har norm men inte är noll, vilket bryter mot kravet att inre produkten är positivt definit. Därmed kvotar vi till sist bort dessa funktioner som bör betraktas som noll.Definition: Ortogonal bas i Hilbertrum
En ortogonal bas i ett Hilbertrum är ortogonal mängd vektorer så att det för alla finns tal så att serien
konvergerar mot .
<br>Precis som i fallet med <a data-tooltip-position="top" aria-label="Ortogonalitet och projektioner > ^299a88" data-href="Ortogonalitet och projektioner#^299a88" href="matematik/linjär-algebra/ortogonalitet-och-projektioner.html#^299a88" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">"vanliga" ortogonala baser</a><a data-tooltip-position="top" aria-label="Ortogonalitet och projektioner > ^299a88" data-href="Ortogonalitet och projektioner#^299a88" href="matematik/linjär-algebra/ortogonalitet-och-projektioner.html#^299a88" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">"vanliga" ortogonala baser</a> kan vi få fram talen enligt
]]></description><link>matematik/linjär-algebra/hilbertrum.html</link><guid isPermaLink="false">Matematik/Linjär algebra/Hilbertrum.md</guid><pubDate>Sat, 21 Dec 2024 21:57:13 GMT</pubDate></item><item><title><![CDATA[Samtidig diagonalisering]]></title><description><![CDATA[<a href=".?query=tag:matematik/linjär-algebra" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik/linjär-algebra">#matematik/linjär-algebra</a> <br>Två <a data-tooltip-position="top" aria-label="Egenvärden och egenvektorer" data-href="Egenvärden och egenvektorer" href="matematik/linjär-algebra/egenvärden-och-egenvektorer.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Diagonaliserbara</a><a data-tooltip-position="top" aria-label="Egenvärden och egenvektorer" data-href="Egenvärden och egenvektorer" href="matematik/linjär-algebra/egenvärden-och-egenvektorer.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Diagonaliserbara</a> <a data-tooltip-position="top" aria-label="Linjära avbildningar" data-href="Linjära avbildningar" href="matematik/linjär-algebra/linjära-avbildningar.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Linjära operatorer</a><a data-tooltip-position="top" aria-label="Linjära avbildningar" data-href="Linjära avbildningar" href="matematik/linjär-algebra/linjära-avbildningar.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Linjära operatorer</a> kan såklart ha samma egenvektor men med olika egenvärden för den egenvektorn. och behöver alltså inte vara lika men kan fortfarande ha en gemensam egenbas, och blir då diagonalmatriser m.a.p samma bas, vilket såklart kan underlätta för beräkningar.Definition: Samtidig diagonalisering
Om är linjära och diagonaliserbara samt har en gemensam egenbas så kallas de samtidigt diagonaliserbara.
Med denna definition låter det svårt att avgöra huruvida två operatorer är samtidigt diagonaliserbara, men som tur är finns ett enkelt kriterium.Sats: Samtidigt diagonaliserbara diagonaliserbara och kommuterar
Om är linjära så är och samtidigt diagonaliserbara om och endast om och är diagonaliserbara och .
Bevis:
Om och är samtidigt diagonaliserbara med egenbas så är
för alla , alltså är .
Om istället vet vi först att det finns egenrum till så att
och att om så är
Vi vet att alla baser till som är unioner av baser till varje är en egenbas till , så om varje restriktion av till är diagonaliserbar kan vi för varje välja en bas till av egenvektorer till , vilket skapar en gemensam egenbas till och . Betrakta därför restriktionen av till . Vi vet då att varje kan skrivas som
där alla är egenvektorer till unika egenvärden. Detta ger att
för alla , eftersom alla för dödas av och bara skaleras av alla . Därmed är alla eftersom och är invariant under alla vilket ger att på är diagonaliserbar.
]]></description><link>matematik/linjär-algebra/samtidig-diagonalisering.html</link><guid isPermaLink="false">Matematik/Linjär algebra/Samtidig diagonalisering.md</guid><pubDate>Thu, 19 Dec 2024 22:45:51 GMT</pubDate></item><item><title><![CDATA[Egenvärden och egenvektorer]]></title><description><![CDATA[<a href=".?query=tag:matematik/linjär-algebra" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik/linjär-algebra">#matematik/linjär-algebra</a> <br>Säg att vi har en <a data-tooltip-position="top" aria-label="Linjära avbildningar" data-href="Linjära avbildningar" href="matematik/linjär-algebra/linjära-avbildningar.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Linjär operator</a><a data-tooltip-position="top" aria-label="Linjära avbildningar" data-href="Linjära avbildningar" href="matematik/linjär-algebra/linjära-avbildningar.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Linjär operator</a> . Då är det ju möjligt att för vissa vektorer bara motsvarar en skalering, så att
för något och . Det visar sig att sådana vektorer är väldigt användbara, så ger vi därför namnet egenvektor och skalären namnet egenvärde.Definition: Egenvärde, egenvektor
Om är linjär och det finns och så att och
så kallas en egenvektor med motsvarande egenvärde om Vi märker att om vi kan hitta en bas till som enbart består av egenvektorer till så blir matrisen till med avseende på den basen en diagonalmatris, alltså en matris där endast diagonalelementen kan vara nollskilda.Definition: Diagonaliserbarhet
Om är linjär och det finns en bas till som enbart består av egenvektorer till så kallas diagonaliserbar.
]]></description><link>matematik/linjär-algebra/egenvärden-och-egenvektorer.html</link><guid isPermaLink="false">Matematik/Linjär algebra/Egenvärden och egenvektorer.md</guid><pubDate>Thu, 19 Dec 2024 15:52:20 GMT</pubDate></item><item><title><![CDATA[Karakteristiska polynom]]></title><description><![CDATA[<a href=".?query=tag:matematik/linjär-algebra" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik/linjär-algebra">#matematik/linjär-algebra</a> <br>Om vi kan hitta en bas till som enbart består av <a data-tooltip-position="top" aria-label="Egenvärden och egenvektorer" data-href="Egenvärden och egenvektorer" href="matematik/linjär-algebra/egenvärden-och-egenvektorer.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Egenvektorer</a><a data-tooltip-position="top" aria-label="Egenvärden och egenvektorer" data-href="Egenvärden och egenvektorer" href="matematik/linjär-algebra/egenvärden-och-egenvektorer.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Egenvektorer</a> till någon <a data-tooltip-position="top" aria-label="Linjära avbildningar" data-href="Linjära avbildningar" href="matematik/linjär-algebra/linjära-avbildningar.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Linjär operator</a><a data-tooltip-position="top" aria-label="Linjära avbildningar" data-href="Linjära avbildningar" href="matematik/linjär-algebra/linjära-avbildningar.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Linjär operator</a> så kan vi skriva som en diagonalmatris. Och diagonalmatriser är ju väldigt trevliga att jobba med eftersom de är enkla att multiplicera med. Vi skulle ju då vilja ha en metod för att hitta egenvärden, egenvektorer och slutligen en bas av egenvektorer. Detta leder oss till det karakteristiska polynomet av en matris.Definition: Karakteristiskt polynom
Det karakteristiska polynomet av en matris är polynomet
Det karakteristiska polynomet av en linjär operator är det karakteristiska polynomet för där är godtycklig bas för .
Om är en kvadratisk matris eller linjär operator så betecknar det karaktäristiska polynomet för , om inte annat sagts.
Vi noterar för det första att vi inte definierar detta i det oändligdimensionella fallet eftersom polynom måste ha ett ändligt antal termer. Vi behöver även visa att det karakteristiska polynomet för en linjär operator är väldefinierat, så att det inte beror av basen .Sats: Karakteristiska polynomet är oberoende av bas
Om är linjär och är baser för så är det karakteristiska polynomet för samma som för .
Bevis:
Vi vet att för någon basbytesmatris . Därmed är Nu ser vi sedan att vi med det karakteristiska polynomet kan hitta egenvärdena till en linjär operator .Sats: Egenvärdena är rötterna till karakteristiska polynomet
Om med matris m.a.p någon bas och är det karakteristiska polynomet till så är egenvärdena till precis rötterna till .
Bevis:
Vi har att för något om och endast om , om och endast om det finns nollskilda lösningar till ]]></description><link>matematik/linjär-algebra/karakteristiska-polynom.html</link><guid isPermaLink="false">Matematik/Linjär algebra/Karakteristiska polynom.md</guid><pubDate>Wed, 18 Dec 2024 09:17:55 GMT</pubDate></item><item><title><![CDATA[Linjära avbildningar]]></title><description><![CDATA[<a href=".?query=tag:matematik/linjär-algebra" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik/linjär-algebra">#matematik/linjär-algebra</a> Definition: Linjär avbildning
En avbildning kallas linjär om den respekterar addition och skalärmultiplikation, d.v.s om
eller ekvivalent om
för alla och .
Om kallas en linjär operator eller bara operator.
]]></description><link>matematik/linjär-algebra/linjära-avbildningar.html</link><guid isPermaLink="false">Matematik/Linjär algebra/Linjära avbildningar.md</guid><pubDate>Tue, 17 Dec 2024 13:16:23 GMT</pubDate></item><item><title><![CDATA[Multilinjära avbildningar]]></title><description><![CDATA[<a href=".?query=tag:matematik/linjär-algebra" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik/linjär-algebra">#matematik/linjär-algebra</a> <br>Definition: Multilinjär avbildning
En avbildning där och är vektorrum kallas -multilinjär eller bara multilinjär om är <a data-tooltip-position="top" aria-label="Linjära avbildningar" data-href="Linjära avbildningar" href="matematik/linjär-algebra/linjära-avbildningar.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Linjär</a><a data-tooltip-position="top" aria-label="Linjära avbildningar" data-href="Linjära avbildningar" href="matematik/linjär-algebra/linjära-avbildningar.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Linjär</a> i varje argument, alltså om som funktion av är linjär. En -multilinjär funktion kallas bilinjär.
Exempel
Multiplikation i en kropp är -multilinjärt, alltså bilinjärt, eftersom<br>
Allmänt brukar avbildningar som vi mer eller mindre kallar för produkter vara bilinjära, som matrismultiplikation eller <a data-tooltip-position="top" aria-label="Tensorer" data-href="Tensorer" href="matematik/linjär-algebra/tensorer.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Tensorprodukt</a><a data-tooltip-position="top" aria-label="Tensorer" data-href="Tensorer" href="matematik/linjär-algebra/tensorer.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Tensorprodukt</a>.
]]></description><link>matematik/linjär-algebra/multilinjära-avbildningar.html</link><guid isPermaLink="false">Matematik/Linjär algebra/Multilinjära avbildningar.md</guid><pubDate>Tue, 17 Dec 2024 11:22:01 GMT</pubDate></item><item><title><![CDATA[Yttre algebran]]></title><description><![CDATA[<a href=".?query=tag:matematik/linjär-algebra" class="tag is-unresolved" target="_self" rel="noopener nofollow" data-href="#matematik/linjär-algebra">#matematik/linjär-algebra</a> <br>Vi kan införa en multiplikation av <a data-href="Yttre potenser" href="matematik/linjär-algebra/yttre-potenser.html" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Yttre potenser</a><a data-href="Yttre potenser" href="matematik/linjär-algebra/yttre-potenser.html" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Yttre potenser</a> av olika grader enligt följande.Definition: Multiplikation av yttre potenser
Vi definierar multiplikation enligt När vi nu alltså räknar med yttre potenser av olika storlek är det rimligt att samla alla dessa i ett gemensamt vektorrum som med denna bilinjära multiplikation alltså blir en algebra som vi kallar för den yttre algebran av .Definition: Yttre algebran
Den yttre algebran av ett vektorrum med bas är ett en algebra med vektorrum som ges av<br>
och multiplikation som ges av <a data-tooltip-position="top" aria-label="^281be4" data-href="#^281be4" href="#^281be4" class="original-internal-link" target="_self" rel="noopener nofollow" style="display: none;">Definition: Multiplikation av yttre potenser</a><a data-tooltip-position="top" aria-label="^281be4" data-href="#^281be4" href="#^281be4" class="internal-link mathLink-internal-link" target="_self" rel="noopener nofollow">Definition: Multiplikation av yttre potenser</a>.
Vi noterar här att om är ändligdimensionellt så är , samt att om , där betecknar vektorrummet med bara nollvektorn. Vi får även att
]]></description><link>matematik/linjär-algebra/yttre-algebran.html</link><guid isPermaLink="false">Matematik/Linjär algebra/Yttre algebran.md</guid><pubDate>Mon, 16 Dec 2024 18:17:39 GMT</pubDate></item></channel></rss>